{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vjtYChCxpJo"
   },
   "source": [
    "# MIT 6.036 Spring 2019: Homework 6\n",
    "\n",
    "This homework does not include provided Python code. Instead, we encourage you to write your own code to help you answer some of these problems, and/or test and debug the code components we do ask for. All of the problems should be simple enough that hand calculation should be possible, but it may be convenient to write some short programs to explore the neural networks, particularly for problem 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework builds on the material in the notes on neural networks up through and including section 6 on loss functions.\n",
    "\n",
    "In particular, in this homework we consider neural networks with multiple layers. Each layer has multiple inputs and outputs, and can be broken down into two parts:\n",
    "\n",
    "- A __linear__ module that implements a linear transformation: $z_j = ( \\sum_{i=1} ^ {m} x_iw_{i,j} ) + w_{0j}$ specified by a weight matrix $W$ and a bias vector $W_0$. The output is $[z_1, \\ldots, z_n]^T$. \n",
    "- An __activation__ module that applies an activation function to the outputs of the linear module for some activation function $f$, such as Tanh or ReLU in the hidden layers or Softmax (see below) at the output layer. We write the output as: $[f(z_1), \\ldots, f(z_m)]$, although technically, for some activation functions such as softmax, each output will depend on all the $z_i$, not just one.\n",
    "\n",
    "We will use the following notation for quantities in a network:\n",
    "\n",
    "- Inputs to the network are $x_1, \\ldots, x_d$.\n",
    "- Number of layers is $L$\n",
    "- There are $m^l$ inputs to layer $l$\n",
    "- There are $n^l = m^{l+1}$ outouts from layer $l$\n",
    "- The weight matrix for layer $l$ is $W^l$, an $m^l \\times n^l$ matrix, and the bias vector (offset) is $W_0^l$ an $n^l \\times 1$ vector\n",
    "- The outputs of the linear module for layer $l$ are known as __pre-activation__ values and denoted $z^l$\n",
    "- The activation function at layer $l$ is $f^l(\\cdot)$ \n",
    "- Layer $l$ activations are $a^l = [f^l(z_1^l), \\ldots, f^l(z_{n^l}^l)]^T$\n",
    "- The output of the network is the values $a^L = [f^L(z_1^L), \\ldots, f^L(z_{n^L}^L)]^T$\n",
    "- Loss function $Loss(a,y)$ measures the loss of output values $a$ when the target is $y$\n",
    "\n",
    "# 1) Loss functions and output activations: classification\n",
    "When doing classification, it's natural to think of the output values as being discrete: +1 and -1. But it is generally difficult to use optimization-based methods without somehow thinking of the outputs as being continuous (even though you will have to discretize when it's time to make a prediction).\n",
    "\n",
    "\n",
    "## 1.1) Hinge loss, linear activation\n",
    "When we looked at the SVM objective for classification, we did this:\n",
    "\n",
    "- Defined the output space to be $\\mathbb{R}$\n",
    "- Developed the hinge loss function\n",
    "$$ Loss(a,y) = L_h(ya) = \\begin{cases}\n",
    "        0, & \\text{if } ya \\gt 1 \\\\\n",
    "        1-ya, & \\text{otherwise } \n",
    "       \\end{cases}\n",
    "$$\n",
    "where $a$ is the continuous output (we're using $a$ here to be consistent with the neural network terminology of _activation_) and $y$ is the desired/target output\n",
    "- Tried to find parameters $\\theta$ of our model to minimize loss summed over the training data\n",
    "\n",
    "Consider a single \"neuron\" with a linear activation function; that is, where $a_1^L = \\sum_k w_{k,1}^L x_k + w_{0,1}^L$. In this case, we have $L=1$ and $f^L(z)=z$.\n",
    "\n",
    "## 1.1.A) \n",
    "Write a short program to compute the gradient of the loss function with respect to the weight vector (not the bias): $\\nabla_{w^L}Loss(a_1^l,y)$ when $Loss(a,y) = L_h(ya)$.\n",
    "- `x` is a column vector\n",
    "- `y` is a number, a label\n",
    "- `a` is a number, an activation\n",
    "\n",
    "It should return a column vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss_grad(x, y, a):\n",
    "    return np.where(y*a > 1, 0, -y*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Log loss, sigmoidal activation\n",
    "Another way to make the output for a classifier continuous is to make it be in the range $(0,1)$, which admits the interpretation of being the predicted __probability__ that the example is positive. A convenient way to make the activation of a unit be in the range $(0,1)$ is to use a sigmoid function:\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}}$$.\n",
    "\n",
    "The figure below shows a sigmoid activation function on the left, with the rectified linear (ReLU) activation function on the right for comparison.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://introml_oll.odl.mit.edu/cat-soop/_static/6.036/homework/hw06/sig_relu_v1.png\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "## 1.2.A) \n",
    "What is an expression for the derivative of the sigmoid with respect to $z$, expressed as a function of $z$, its input?\n",
    "\n",
    "$\\frac{d\\sigma}{dz} = -1(1 + e^{-z})^{-2} (-e^{-z}) = \\frac{e^{-z}}{(1 + e^{-z})^2}$\n",
    "\n",
    "\n",
    "## 1.2.B) \n",
    "What is an expression for the derivative of the sigmoid with respect to $z$, but this time expressed as a function of \n",
    "$o = \\sigma(z)$, its output? Hint: Think about the expression $1 - \\frac{1}{1+e^{-z}}$.\n",
    "\n",
    "$1 - \\frac{1}{1+e^{-z}} = \\frac{e^{-z}}{1 + e^{-z}} = 1 - \\sigma(z)$\n",
    "\n",
    "$\\frac{d\\sigma}{dz} = (1 - \\sigma(z))\\sigma(z) = \\left(\\ \\frac{e^{-z}}{1 + e^{-z}} \\right)\\ \\left(\\ \\frac{1}{1 + e^{-z}} \\right)\\ = \\frac{e^{-z}}{(1 + e^{-z})^2}$\n",
    "\n",
    "$\\frac{d\\sigma}{dz} = \\frac{e^{-z}}{(1 + e^{-z})^2} = \\left(\\ \\frac{e^{-z}}{1 + e^{-z}} \\right)\\ \\left(\\ \\frac{1}{1 + e^{-z}} \\right)\\ = (1 - \\sigma(z))\\sigma(z) = (1 - o)o$\n",
    "\n",
    "__In this model, we will consider positive points to have label +1, and negative points to have label 0.__\n",
    "\n",
    "We need a loss function that works well when we are predicting probabilities. A good choice is to ask what probability is assigned to the correct label. We will interpret the value outputted by our classifier as the probability that the example is positive. So, if the output value is $a$ and the true label is $+1$, then the probability assigned to the true label is $a$; on the other hand, if the true label is $0$, then the probability assigned to the true label is $1âˆ’a$. Because we actually will be interested in the probability of the predictions on the whole data set, we'd want to choose weights to __maximize__\n",
    "\n",
    "$$\\prod_t P(a^{(t)}, y^{(t)})$$ \n",
    "\n",
    "where $P(a^{(t)}, y^{(t)})$ is the probability that the network predicts the correct label for data point $(t)$.\n",
    "\n",
    "Using a notational trick (which turns an if expression into a product) that might seem unmotivated now, but will be useful later, we can write the probability $P(a, y)$ as\n",
    "\n",
    "$$ P(a, y) = a^y(1-a)^{(1-y)} $$\n",
    "\n",
    "## 1.2.C) \n",
    "What is the value of $P(a,y)$ when $y=0$?\n",
    "\n",
    "## 1.2.D) \n",
    "What is the value of $P(a,y)$ when $y=1$?\n",
    "\n",
    "## 1.2.E) \n",
    "Find a simplified expression for $\\log (P(a, y))$ that does not use exponentiation. Note that we refer to the natural logarithm $\\ln$ as $\\log$ throughout this assignment, consistent with the lecture notes.\n",
    "\n",
    "In fact, because log is a monotonic function, the same weights that maximize the product of the probabilities will minimize the _negative log likelihood_ (\"likelihood\" is the same as probability; we just use that name here because the phrase is an idiom in machine learning, abbreviated NLL):\n",
    "\n",
    "$$ Loss(a,y) = NLL(a, y) = -y\\log a - (1-y) \\log(1-a) $$\n",
    "\n",
    "Our objective function (over our $n$ data points) will then be\n",
    "\n",
    "$$ \\sum NLL(a^{(t)}, y^{(t)}) = -\\sum_{t=1} ^ n \\left[\\ y^{(t)}\\log a^{(t)} + (1-y^{(t)}) \\log(1-a^{(t)}) \\right]\\ $$ \n",
    "\n",
    "Remember that $a^{(t)}$ is our model's output for training example $t$, and $y^{(t)}$ is the true label (+1 or 0).\n",
    "Now, we can think about a single unit with a sigmoidal activation function, trained to minimize NLL. So, $a_1^L = \\sigma( \\sum_k w_{k,1}^L x_k + w_{0,1}^L )$. In this case, we have $L=1$\n",
    "## 1.2.F) \n",
    "Write a formula for the gradient of the NLL with respect to the first weight, $\\nabla_{w_{1,1}^L} NLL(a_1^L, y)$ for a single training example. Hint: consider using the chain rule; the final answer (expression) is very short.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    NLL(a_1^L, y) &= -y\\log a - (1-y) \\log(1-a) \\\\\n",
    "    &= -y\\log( \\sigma ( \\sum_k w_{k,1}^L x_k + w_{0,1}^L ) ) - (1-y) \\log( 1 - \\sigma ( \\sum_k w_{k,1}^L x_k + w_{0,1}^L ) ) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$z_1^L = \\sigma ( \\sum_k w_{k,1}^L x_k + w_{0,1}^L )$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial}{\\partial w_{1,1}^L} NLL(a_1^L, y) &= -y \\frac{1}{\\sigma(z_1^L)} \\sigma(z_1^L) ( 1- \\sigma(z_1^L) )x_1 - (1 - y) \\frac{1}{1- \\sigma(z_1^L)} (-1) \\sigma(z_1^L) (1 - \\sigma(z_1^L)) x_1 \\\\\n",
    "    &= -y (1 - \\sigma(z_1^L)) x_1 + (1 -y) \\sigma(z_1^L) x_1 \\\\\n",
    "    &= x_1 (-y + y \\sigma(z_1^L) - y \\sigma(z_1^L) + \\sigma(z_1^L)) \\\\\n",
    "    &= x_1 (a_1^L - y)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## 1.2.G)\n",
    "Write a formula for the gradient of the NLL with respect to the full weight vector, $\\nabla_{W^L} NLL(a_1^L, y)$, for a single training example.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\nabla_{W^L} NLL(a_1^L, y) &= \\frac{\\partial Loss}{\\partial a} \\ \\frac{\\partial a}{\\partial z} \\ \\frac{\\partial z}{\\partial w} \\\\\n",
    "    &= \\left(\\ \\frac{-y}{a} + \\frac{(1-y)}{1-a} \\right)\\ \\left(\\ (1-a)a \\right)\\ \\left(\\ x \\right)\\ \\\\\n",
    "    &= \\left(\\ (-y)(1-a) + a(1-y) \\right)\\ \\left(\\ x \\right)\\ \\\\\n",
    "    &= ( -y +ya + a - ya )x \\\\\n",
    "    &= (a-y)x \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multiclass classification\n",
    "What if we needed to classify homework problems into three categories: enlightening, boring, impossible? We can do this by using a \"one-hot\" encoding on the output, and using three output units with what is called a \"softmax\" (SM) activation module. It's not a typical activation module, since it takes in all $n_L$ pre-activation values $z_j^L$ in $\\mathbb{R}$ and returns $n_L$ output values $a_j^L \\in [0,1]$ such that $\\sum_j a_j^L = 1$. This can be interpreted as representing a probability distribution over the possible categories.\n",
    "\n",
    "The individual entries are computed as\n",
    "\n",
    "$$ a_j = \\frac{e^{z_j}}{\\sum_{k=1} ^ {n^L} e^{z_k}}$$\n",
    " \n",
    "We'll describe the relationship of the vector $a$ on the vector $z$ as \n",
    "\n",
    "$$a=SM(z)$$\n",
    "\n",
    "The network below shows a one-layer network with a linear module followed by a softmax activation module.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://introml_oll.odl.mit.edu/cat-soop/_static/6.036/homework/hw06/softmax.png\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "## 2.A)\n",
    "What probability distribution over the categories is represented by $z^L = [-1, 0, 1]^T$?\n",
    "\n",
    "Now, we need a loss function $Loss(a,y)$ where $a$ is a discrete probability distribution and $y$ is a one-hot vector encoding of a single output value. It makes sense to use negative log likelihood as a loss function for the same reasons as before. So, we'll just extend our definition of NLL from earlier:\n",
    "\n",
    "$$ NLL(a, y) = - \\sum_{j=1} ^ {n^L} y_j\\ln a_j^L $$\n",
    "\n",
    "\n",
    "Note that the above expression is for multi-classes (number of class $\\gt$ 2). For two-classes, the expression reduce to what you saw after Problem 1.2.E.\n",
    "\n",
    "## 2.B)\n",
    "If $a = [0.3, 0.5, 0.2]^T$ and $y = [0, 0, 1]^T$, what is $NLL(a,y)$?\n",
    "\n",
    "Now, we can think about a single layer with a softmax activation module, trained to minimize NLL. The pre-activation values (the output of the linear module) are:\n",
    "\n",
    "$$ z_j^L = \\sum_k w_{k,j}^L x_k + w_{0,j}^L $$\n",
    "\n",
    "and $a^L = SM(z^L)$.\n",
    "To do gradient descent, we need to know $ \\frac{\\partial}{\\partial w_{k,j}^L}NLL(a^L, y) $. We'll reveal the secret (that you might guess from Problem 1) that it has an awesome form! (Please consider deriving this, for fun and satisfaction!)\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial w_{k,j}^L}NLL(a^L, y) = x_k(a_j^L - y_j) $$\n",
    "\n",
    "And of course, it's easy to compute the whole matrix of these derivatives, $\\nabla_{w^L} NLL(a^L, y)$, in one quick matrix computation.\n",
    "\n",
    "## 2.C)\n",
    "Suppose we have two input units and three possible output values, and the weight matrix $W^L$ is\n",
    "\n",
    "$$\n",
    "W^L = \\begin{bmatrix}\n",
    "    1 & -1 & 2 \\\\\n",
    "    -1 & 2 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "or in Python form: `w = np.array([[1, -1, -2], [-1, 2, 1]])`.\n",
    "\n",
    "Assume the biases are zero, the input $x = [1,1]^T$ (e.g., `x = np.array([[1, 1]]).T)`, and the target output $y=[0,1,0]^T$ (e.g., `y = np.array([[0, 1, 0]]).T)`. What is the matrix $\\nabla_{w^L} NLL(a^L, y)$? Hint: You might want to solve using Python and numpy, or using colab for calculation.\n",
    "\n",
    "## 2.D) \n",
    "What is the predicted probability that $x$ is in class 1, before any gradient updates? (Assume we have classes 0, 1, and 2.)\n",
    "\n",
    "## 2.E)\n",
    "Using step size 0.5, what is $W^L$ after one gradient update step?\n",
    "\n",
    "## 2.F)\n",
    "What is the predicted probability that $x$ is in class 1, given the new weight matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Q0278eUVx91Z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(z):\n",
    "    return np.exp(z) / np.sum(np.exp(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhKmop0MBh9R"
   },
   "source": [
    " **Problem 2A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "g0093Y5sBitt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09 ],\n",
       "       [0.245],\n",
       "       [0.665]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.array([[-1, 0, 1]]).T\n",
    "# your code here\n",
    "# 2A\n",
    "a = softmax(z)\n",
    "np.round(a, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZigPSFHBOWy"
   },
   "source": [
    "**Problem 2.C-F**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "McPjfdEPBSNs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.245 -0.335  0.09 ]\n",
      " [ 0.245 -0.335  0.09 ]]\n",
      "[[0.66524096]]\n",
      "[[ 0.878 -0.833 -2.045]\n",
      " [-1.122  2.167  0.955]]\n",
      "[[0.77245284]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[1, -1, -2], [-1, 2, 1]])\n",
    "x = np.array([[1], [1]])\n",
    "y = np.array([[0, 1, 0]]).T\n",
    "# your code here\n",
    "# 2C\n",
    "z = w.T@x\n",
    "# 3x1\n",
    "a = softmax(z)\n",
    "# 2x1 1x3 = 2x3\n",
    "nll_gradient = x @ ((a - y).T)\n",
    "print(np.round(nll_gradient, 3))\n",
    "\n",
    "# 2D\n",
    "print(a[1:2, :])\n",
    "w = w - 0.5*g\n",
    "\n",
    "# 2E\n",
    "print(np.round(w, 3))\n",
    "a = softmax(w.T@x)\n",
    "\n",
    "# 2F\n",
    "print(a[1:2, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nefgHcxHgLe"
   },
   "source": [
    "# 3) Neural Networks\n",
    "In this problem we will analyze a simple neural network to understand its classification properties. You might find the colab file useful. However, we encourage you to go through all the calculation by hand once, which should be a good practice.\n",
    "\n",
    "Consider the neural network given in the figure below, with ReLU activation functions ($f^1$ in the figure) on all hidden neurons, and softmax activation ($f^2$ in the figure) for the output layer, resulting in softmax outputs ($a_1^2$  and $a_2^2$  in the figure).\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://introml_oll.odl.mit.edu/cat-soop/_static/6.036/homework/hw06/nnet.png\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "Given an input $x = [x_1, x_2]^T$, the hidden units in the network are activated in stages as described by the following equations:\n",
    "\n",
    "$$\n",
    "z_1^1 = x_1w_{1,1}^1 + x_2w_{2,1}^1 + w_{0,1}^1 \\qquad a_1^1 = \\max \\{ z_1^1, 0\\} \\\\\n",
    "z_2^1 = x_1w_{1,2}^1 + x_2w_{2,2}^1 + w_{0,2}^1 \\qquad a_2^1 = \\max \\{ z_2^1, 0\\} \\\\\n",
    "z_3^1 = x_1w_{1,3}^1 + x_2w_{2,3}^1 + w_{0,3}^1 \\qquad a_3^1 = \\max \\{ z_3^1, 0\\} \\\\\n",
    "z_4^1 = x_1w_{1,4}^1 + x_2w_{2,4}^1 + w_{0,4}^1 \\qquad a_4^1 = \\max \\{ z_4^1, 0\\} \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_1^2 = a_1^1w_{1,1}^2 + a_2^1w_{2,1}^2 + a_3^1w_{3,1}^2 + a_4^1w_{4,1}^2 + w_{0,1}^2 \\\\\n",
    "z_2^2 = a_1^1w_{1,2}^2 + a_2^1w_{2,2}^2 + a_3^1w_{3,2}^2 + a_4^1w_{4,2}^2 + w_{0,2}^2 \\\\\n",
    "$$\n",
    "\n",
    "The final output of the network is obtained by applying the _softmax_ function to the last hidden layer,\n",
    "\n",
    "$$\n",
    "a_1^2 = \\frac{e^{z_1^2}}{e^{z_1^2} + e^{z_2^2}} \\\\\n",
    "a_2^2 = \\frac{e^{z_2^2}}{e^{z_1^2} + e^{z_2^2}} \\\\\n",
    "$$\n",
    "\n",
    "In this problem, we will consider the following setting of parameters:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "    w_{1,1}^1 & w_{1,2}^1 & w_{1,3}^1 & w_{1,4}^1 \\\\\n",
    "    w_{2,1}^1 & w_{2,2}^1 & w_{2,3}^1 & w_{2,4}^1 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    1 & 0 & -1 & 0 \\\\\n",
    "    0 & 1 & 0 & -1 \\\\\n",
    "\\end{bmatrix}\n",
    ",\\qquad\n",
    "\\begin{bmatrix}\n",
    "    w_{0,1}^1 \\\\\n",
    "    w_{0,2}^1 \\\\\n",
    "    w_{0,3}^1 \\\\\n",
    "    w_{0,4}^1 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    -1 \\\\\n",
    "    -1 \\\\\n",
    "    -1 \\\\\n",
    "    -1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "    w_{1,1}^2 & w_{1,2}^2 \\\\\n",
    "    w_{2,1}^2 & w_{2,2}^2 \\\\\n",
    "    w_{3,1}^2 & w_{3,2}^2 \\\\\n",
    "    w_{4,1}^2 & w_{4,2}^2 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    1 & -1 \\\\\n",
    "    1 & -1 \\\\\n",
    "    1 & -1 \\\\\n",
    "    1 & -1 \\\\\n",
    "\\end{bmatrix}\n",
    ",\\qquad\n",
    "\\begin{bmatrix}\n",
    "    w_{0,1}^2 \\\\\n",
    "    w_{0,2}^2 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "# 3.1) Output\n",
    "Consider the input $x_1 = 3, x_2 = 14$\n",
    "\n",
    "## 3.1.A) \n",
    "What are the outputs of the hidden units, $(f^1(z_1^1),f^1(z_2^1), f^1(z_3^1), f^1(z_4^1))$?\n",
    "\n",
    "## 3.1.B)\n",
    "What is the final output $(a_1^2, a_2^2)$ of the network?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 13, 0, 0]]\n",
      "[[0.9999999999993086, 6.914400106935422e-13]]\n"
     ]
    }
   ],
   "source": [
    "# layer 1 weights\n",
    "w_1 = np.array([[1, 0, -1, 0], [0, 1, 0, -1]])\n",
    "w_1_bias = np.array([[-1, -1, -1, -1]]).T\n",
    "# layer 2 weights\n",
    "w_2 = np.array([[1, -1], [1, -1], [1, -1], [1, -1]])\n",
    "w_2_bias = np.array([[0, 2]]).T\n",
    "\n",
    "# your code here\n",
    "def relu(z):\n",
    "    return np.where(z > 0, z, 0)\n",
    "\n",
    "# 3.1.A\n",
    "x = np.array([[3, 14]]).T\n",
    "z_1 = (w_1.T @ x) + w_1_bias\n",
    "a_1 = relu(z_1)\n",
    "print(a_1.reshape(1, -1).tolist())\n",
    "z_2 = (w_2.T @ a_1) + w_2_bias\n",
    "a_2 = softmax(z_2)\n",
    "\n",
    "# 3.1.B\n",
    "print(a_2.reshape(1, -1).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2) Unit decision boundaries\n",
    "Let's characterize the _decision boundaries_ in $x$-space, corresponding to the four hidden units. These are the regions where the input to the units $z_1^1, z_2^1, z_3^1, z_4^1$ are exactly zero.\n",
    "\n",
    "Hint: You should draw a diagram of the decision boundaries for each unit in the $x$-space and label the sides of the boundaries with 0 and + to indicate whether the unit's output would be exactly 0 or positive, respectively. (The diagram should be a 2D plot with $x_1$ and $x_2$ on each axis, with lines for $z_1^1 = 0, z_2^1 = 0, z_3^1 = 0, z_4^1 = 0$\n",
    "\n",
    "## 3.2.A)\n",
    "What is the shape of the decision boundary for a single unit?\n",
    "\n",
    "Find link to graph [here](https://www.desmos.com/calculator/kdildkysvt).\n",
    "\n",
    "## 3.2.B)\n",
    "Enter a 2 x 4 matrix where each column represents a (different) input vector $[x_1, x_2]^T$ each of which is on the decision boundary for the first unit, that is, for which $z_1^1 = 0$ (There are multiple possible answers.)\n",
    "\n",
    "Any points on the rightmost vertical line representing $z_1^l = 0 in the graph linked above.$\n",
    "\n",
    "## 3.2.C)\n",
    "Consider the following input vectors: $x^{(1)} = [0.5, 0.5]^T, x^{(2)} = [0, 2]^T, x^{(3)} = [-3, 0.5]^T$. Enter a matrix where each column represents the outputs of the hidden units $(f(z_1^1), \\cdots, f(z_4^1))$ for each of the input vectors. You can use your diagram of decision boundaries.\n",
    "\n",
    "You can read these off the diagram of decision boundaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3) Network outputs\n",
    "In our network above, the output layer with two softmax units is used to classify into one of two classes. For class 1, the first unit's output should be larger than the other unit's output, and for class 2, the second unit's output should be larger. This generalizes nicely to $k$ classes by using $k$ output units.\n",
    "\n",
    "(We have previously examined addressing two-class classification problems using a single output unit with a sigmoid activation; this is another way to address them.)\n",
    "\n",
    "Let's characterize the region in $x$-space where this network's output indicates the first class (that is, $a_1^2$ is larger) or indicates the second class (that is, $a_2^2$ is larger). Your diagram from the previous part will be useful here.\n",
    "\n",
    "What is the output value of the neural network in each of the following cases? Write your answer for $a_i^2$ as expressions, you can use powers of $e$, for example, `e**2 + 1`; the exponents can be negative, `e**(-2) + 1`.\n",
    "\n",
    "Case 1) For $f(z_1^1) + f(z_2^1) + f(z_3^1) + f(z_4^1) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "id": "5Qtv3VR2AA_m",
    "outputId": "798b1f01-fbe2-4fc4-9482-f979de216f98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.05288295]\n",
      " [6.48663207]\n",
      " [6.7519581 ]\n",
      " [7.58553317]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "T  = np.matrix([[0.0 , 0.1 , 0.9 , 0.0],\n",
    "[0.9 , 0.1 , 0.0 , 0.0],\n",
    "[0.0 , 0.0 , 0.1 , 0.9],\n",
    "[0.9 , 0.0 , 0.0 , 0.1]])\n",
    "g = 0.9\n",
    "r = np.matrix([0, 1., 0., 2.]).reshape(4, 1)\n",
    "\n",
    "print(np.linalg.solve(np.eye(4) - g * T, r))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
