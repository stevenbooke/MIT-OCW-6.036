{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNJnLrgd6aLV"
   },
   "source": [
    "# MIT 6.036 Spring 2019: Homework 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Tj65sGzr8O7e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BL-awiRyGp84"
   },
   "source": [
    "# Setup\n",
    "Run the next code block to download and import the code for this lab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fsHM3W44G2iJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-20 10:47:57--  https://introml_oll.odl.mit.edu/cat-soop/_static/6.036/homework/hw12/code_and_data_for_hw12.zip\n",
      "Resolving introml_oll.odl.mit.edu (introml_oll.odl.mit.edu)... 3.226.240.108\n",
      "Connecting to introml_oll.odl.mit.edu (introml_oll.odl.mit.edu)|3.226.240.108|:443... connected.\n",
      "WARNING: cannot verify introml_oll.odl.mit.edu's certificate, issued by ‘CN=InCommon RSA Server CA,OU=InCommon,O=Internet2,L=Ann Arbor,ST=MI,C=US’:\n",
      "  Unable to locally verify the issuer's authority.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6963985 (6.6M) [application/zip]\n",
      "Saving to: ‘code_and_data_for_hw12.zip’\n",
      "\n",
      "code_and_data_for_h 100%[===================>]   6.64M  5.78MB/s    in 1.1s    \n",
      "\n",
      "2023-04-20 10:47:59 (5.78 MB/s) - ‘code_and_data_for_hw12.zip’ saved [6963985/6963985]\n",
      "\n",
      "Archive:  code_and_data_for_hw12.zip\n",
      "  inflating: hw12_code_and_data/code_for_hw12.py  \n",
      "  inflating: hw12_code_and_data/movies.csv  \n",
      "  inflating: hw12_code_and_data/ratings.csv  \n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate https://introml_oll.odl.mit.edu/cat-soop/_static/6.036/homework/hw12/code_and_data_for_hw12.zip\n",
    "!unzip code_and_data_for_hw12.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Matrix factorization by rank\n",
    "Consider the following data set:\n",
    "\n",
    "$$\n",
    "Y =\n",
    "\\begin{bmatrix}\n",
    "    4 & 3 & 1 \\\\\n",
    "    1 & 3 & -2 \\\\\n",
    "    5 & 2 & 3 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We would like to factorize $Y=UV^T$ such that $U$ and $V$ are both $3 x k$ matrices.\n",
    "\n",
    "Hint: See if any columns are linear combinations of others. Try subtracting the first two columns.\n",
    "\n",
    "### 1A)\n",
    "Can this matrix be factored using $k=1$? Provide $U$, $V^T$ matrices if it can (such that $Y=UV^T$), or enter 'None' if it cannot be done.\n",
    "\n",
    "### 1B)\n",
    "Can this matrix be factored using $k=2$? Provide $U$, $V^T$ matrices if it can (such that $Y=UV^T$), or enter 'None' if it cannot be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  3,  1],\n",
       "       [ 1,  3, -2],\n",
       "       [ 5,  2,  3]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.array([[4, 3, 1], [1, 3, -2], [5, 2, 3]])\n",
    "U = np.array([[4, 3], [1, 3], [5, 2]])\n",
    "V = np.array([[1, 0, 1], [0, 1, -1]]).T\n",
    "U@V.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "724STOOC9skZ"
   },
   "source": [
    "# Some preliminary code\n",
    "\n",
    "Here are some useful functions which will be used in the following sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rjLcTfzu936E"
   },
   "outputs": [],
   "source": [
    "def pred(data, x):\n",
    "    (a, i, r) = data\n",
    "    (u, b_u, v, b_v) = x\n",
    "    return np.dot(u[a].T, v[i]) + b_u[a] + b_v[i]\n",
    "  \n",
    "# X : n x k\n",
    "# Y : n\n",
    "def ridge_analytic(X, Y, lam):\n",
    "    (n, k) = X.shape\n",
    "    xm = np.mean(X, axis = 0, keepdims = True)   # 1 x n\n",
    "    ym = np.mean(Y)                              # 1 x 1\n",
    "    Z = X - xm                                   # d x n\n",
    "    T = Y - ym                                   # 1 x n\n",
    "    th = np.linalg.solve(np.dot(Z.T, Z) + lam * np.identity(k), np.dot(Z.T, T))\n",
    "    # th_0 account for the centering\n",
    "    th_0 = (ym - np.dot(xm, th))                 # 1 x 1\n",
    "    return th.reshape((k,1)), float(th_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFTBSXas8Tdu"
   },
   "source": [
    "### Lab reference\n",
    "Note that in practice, it is impossible to have all users rate all movies; companies like Netflix recommend movies to its users who have rated only a subset of movies.\n",
    "\n",
    "To make this concrete, consider an incomplete version of the data from the previous question:\n",
    "\n",
    "$$Y = \\begin{bmatrix}\n",
    "? & 1 & ? & 1 & 5 & 1 & 5 \\\\\n",
    "1 & 5 & 1 & ? & ? & 5 & 1 \\\\\n",
    "5 & 5 & 5 & 5 & ? & ? & 5 \\\\\n",
    "1 & ? & 1 & 1 & 1 & 1 & ? \n",
    "\\end{bmatrix}$$\n",
    " \n",
    "Now our goal is to find $U$, $V^T$ that minimizes $J(U,V)$ given the incomplete matrix $Y$ above. To do so, we will use the __alternating least squares__ method, where we\n",
    "\n",
    "1. Fix (or initialize) $V$ and find the optimal $U$ that minimizes $J(U)$ given this $V$.\n",
    "2. Fix $U$ to the value obtained from 1, and find the optimal $V$ that minimizes $J(V)$ given this $U$.\n",
    "3. Repeat from 1, with $V$ fixed to value obtained from 2.\n",
    "\n",
    "To implement the alternating least squares method with our incomplete matrix $Y$, we will define some notation that will make the gradient $\\nabla_{u^{(a)}} J(U)$ easier to write in matrix form, and similarly for $\\nabla_{v^{(i)}} J(V)$.\n",
    "\n",
    "Let $I_a$ be the set of all movies that were rated by person $a$. To be precise, $I_a = \\{i | (a, i) \\in D\\}$. Also, define $B_a$ to be the matrix of relevant data: $B_a$ is constructed by taking $V$ and removing every row $i$ that is not in $I_a$. In other words, $B_a$ is the matrix of movie features for just the movies that were rated by person $a$. Letting $l_a$ be the number of elements in $I_a$, observe that $B_a$ has dimensions $l_a$ by $k$.\n",
    "\n",
    "### End Lab reference\n",
    "\n",
    "## 2) Some movies are more equal than others\n",
    "\n",
    "In Lab 12, we formulated recommender systems without offsets:  there was no equivalent of $\\theta_0$ in the regression problem we solved.  But offsets are very useful in this problem to account for some reviewers that are just generally grumpy (or enthusiastic) or some movies that are just generally awful (or great). For this section we will work to extend the results from the lab to include offsets.\n",
    "\n",
    "Rather than trying to predict what rating Amy will give a movie based on the properties of that movie, it might be more effective to predict how much \"more highly than usual\" Amy will rate the movie.  To do this, we introduce a vector $b_u$ to characterize offsets for users and a vector $b_v$ to characterize offsets for movies.  Now, the objective can be written as\n",
    "$$J(U, V) = \\frac{1}{2}\\sum_{(a, i) \\in D} (Y_{ai} - {u^{(a)}}\\cdot v^{(i)} - b_u^{(a)} - b_v^{(i)})^2 + \\frac{\\lambda}{2} \\sum_{a = 1}^n \\lVert u^{(a)} \\rVert^2 + \\frac{\\lambda}{2} \\sum_{i = 1}^m  \\lVert v^{(i)} \\rVert^2 $$\n",
    "\n",
    "We will stick with just one half of the problem for now:  finding $U$ and $b_u$ given a fixed $V$ and $b_v$.  We'll concentrate on computing a new estimate for $u^{(a)}$ and $b_u^{(a)}$.  Continuing the notation from lab, the regression problem that we have to solve becomes:\n",
    "$$-B_a^T (Z_a - B_a u^{(a)} - b_v - b_u^{(a)}) + \\lambda u^{(a)} = 0$$\n",
    "where $b_v$ is the $l_a$ by 1 vector of offsets for the movies and $b_u^{(a)}$ is the offset for user $a$.  This is a linear regression problem in which the targets can be viewed as $Z_a - b_v$, and where there is an offset $b_u^{(a)}$ that is not regularized.  This is, then, the standard ridge regression set-up.  We have seen how to solve this problem via gradient descent before, but it can also be solved analytically by essentially turning it into a regular linear regression problem. Using this approach to computing the optimal $u^{(a)}$ requires first centering the data, then doing linear regression, then doing a little bit of work to recover the offset $b_u^{(a)}$.  In the code, we have supplied a procedure (`ridge_analytic`) for doing exactly this.\n",
    "\n",
    "Let's see how it works out in our running example.  For illustrative purposes, we'll add two more movies:\n",
    "\n",
    "8) 6.036 lecture videos <br />\n",
    "9) The Zzzz files <br />\n",
    "\n",
    "Our new preference data is:\n",
    "$$Y = \\begin{bmatrix}\n",
    "? & 1 & ? & 1 & 5 & 1 & 5 & 5 & 1\\\\\n",
    "1 & 5 & 1 & ? & ? & 5 & 1 & 5 & 1\\\\\n",
    "5 & 5 & 5 & 5 & ? & ? & 5 & ? & 1\\\\\n",
    "1 & ? & 1 & 1 & 1 & 1 & ? & 5 & ?\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "We will assume the following value for $b_v$:\n",
    "$$b_v = \\begin{bmatrix}\n",
    "3 & 3 & 3 & 3 & 3 & 3 & 3 & 5 & 1\n",
    "\\end{bmatrix}^T $$\n",
    "And let\n",
    "$$V = \\begin{bmatrix}\n",
    "10 & 1 & 10 & 1 & 10 & 1 & 10 & 5 & 5\\\\\n",
    "1 & 10 & 1 & 10 & 1 & 10 & 1 & 5 & 5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "We can now compute the optimal $u^{(a)}$ and $b_u^{(a)}$ for $a=0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "K4c7Wb4F9QxM"
   },
   "outputs": [],
   "source": [
    "Z = np.array([[1], [1], [5], [1], [5], [5], [1]])\n",
    "b_v = np.array([[3], [3], [3], [3], [3], [5], [1]])\n",
    "B = np.array([[1, 10], [1, 10], [10, 1], [1, 10], [10, 1], [5, 5], [5, 5]])\n",
    "# Solution with offsets, using ridge_analytic provided in code file\n",
    "u_a, b_u_a = ridge_analytic(B, (Z - b_v), 1)\n",
    "\n",
    "#u_a, b_u_a\n",
    "#(array([[ 0.22024566],\n",
    "#        [-0.22193986]]),\n",
    "# array([[ 0.00762389]]))\n",
    "\n",
    "# Solution using previous model, with no offsets\n",
    "u_a_no_b = np.dot(np.linalg.inv(np.dot(B.T, B) + 1 * np.identity(2)), np.dot(B.T, Z))\n",
    "\n",
    "#u_a_no_b\n",
    "#array([[ 0.50148126],\n",
    "#       [ 0.0562376 ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gkpCpPiJi3B"
   },
   "source": [
    "The code above computes $u^{(a)}$ and $b_u^{(a)}$for Amy. Use them for the next few parts. You can assume that Llama movies always satisfy $v^{(i)} = \\begin{bmatrix} 10 \\\\ 1 \\\\ \\end{bmatrix}$ and Robot movies satisfy $v^{(i)} = \\begin{bmatrix} 1 \\\\ 10 \\\\ \\end{bmatrix}$\n",
    "\n",
    "### 2A)\n",
    "How will Amy feel about a brand new Llama movie (not in the existing movie data) that gets bad ratings from almost everyone ($b_v^{(i)} = 1$), in the two models?\n",
    "\n",
    "### 2B)\n",
    "What about a brand new Robot movie (not in the existing movie data) that gets good ratings from almost everyone ($b_v^{(i)} = 5$), in the two models?\n",
    "\n",
    "### 2C)\n",
    "What about a brand new Robot movie (not in the existing movie data) that gets good ratings from almost everyone ($b_v^{(i)} = 3$), in the two models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "O30gG-qq-Z4O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.07]] [[3.99]]\n",
      "[[1.06]] [[-3.99]]\n",
      "[[1.06]] [[-1.99]]\n"
     ]
    }
   ],
   "source": [
    "Z = np.array([[1], [1], [5], [1], [5], [5], [1]])\n",
    "b_v = np.array([[1], [1], [1], [1], [1], [1], [1]])\n",
    "B = np.array([[1, 10], [1, 10], [10, 1], [1, 10], [10, 1], [5, 5], [5, 5]])\n",
    "# Solution with offsets, using ridge_analytic provided in code file\n",
    "u_a, b_u_a = ridge_analytic(B, (Z - b_v), 1)\n",
    "\n",
    "# Solution using previous model, with no offsets\n",
    "u_a_no_b = np.dot(np.linalg.inv(np.dot(B.T, B) + 1 * np.identity(2)), np.dot(B.T, Z))\n",
    "\n",
    "v_llama = np.array([[10], [1]])\n",
    "v_robot = np.array([[1], [10]])\n",
    "\n",
    "print(np.round(np.dot(u_a_no_b.T, v_llama), 2), np.round(np.dot(u_a.T, v_llama) + b_u_a, 2))\n",
    "\n",
    "b_v = np.array([[5], [5], [5], [5], [5], [5], [5]])\n",
    "u_a, b_u_a = ridge_analytic(B, (Z - b_v), 1)\n",
    "print(np.round(np.dot(u_a_no_b.T, v_robot), 2), np.round(np.dot(u_a.T, v_robot) + b_u_a, 2))\n",
    "\n",
    "b_v = np.array([[3], [3], [3], [3], [3], [3], [3]])\n",
    "u_a, b_u_a = ridge_analytic(B, (Z - b_v), 1)\n",
    "print(np.round(np.dot(u_a_no_b.T, v_robot), 2), np.round(np.dot(u_a.T, v_robot) + b_u_a, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keW70T5N-iJj"
   },
   "source": [
    "# 3) Implementing recommender systems\n",
    "\n",
    "Now we'll look in detail at two implementations of matrix factorization.  One is the alternating least squares algorithm, and the other is the stochastic gradient descent algorithm.\n",
    "\n",
    "We will assume that the input data is made up of `(a, i, r)` triples, where `a` is a user index. `i` is an item index and `r` is a rating.  Here is a small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kmC7xNyD_GcG"
   },
   "outputs": [],
   "source": [
    "# Data is a list of (a, i, r) triples\n",
    "ratings_small = \\\n",
    "[(0, 0, 5), (0, 1, 3), (0, 3, 1),\n",
    " (1, 0, 4), (1, 3, 1),\n",
    " (2, 0, 1), (2, 1, 1), (2, 3, 5),\n",
    " (3, 0, 1), (3, 3, 4),\n",
    " (4, 1, 1), (4, 2, 5), (4, 3, 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoDdcZmp_O00"
   },
   "source": [
    "We will assume that predictions are made using user and item offsets, that is,\n",
    "$$y = {u^{(a)}}\\cdot v^{(i)} + b_u^{(a)} + b_v^{(i)}$$\n",
    "In this expression $b_u$ is a vector of user offsets and $b_v$ is a vector of item offsets.\n",
    "\n",
    "### 3.1) Alternating Least Squares (ALS)\n",
    "Below is a function that provides the \"outer loop\" of the alternating least squares algorithm.\n",
    "\n",
    "* Define n and m from the data.\n",
    "* Initialize a list of lists `us_from_v` where `us_from_v[i]` contains the indices and ratings of users who rated item `i`.\n",
    "* Similarly, initialize a list of lists `vs_from_u` where `vs_from_u[a]` contains the indices and ratings of items rated by user `a`.\n",
    "* Initialize the set of parameters `x` (note that the u,v entries are set randomly while the user and item offsets are set to 0) where \n",
    "    * `x[0] = u`, a list of column vectors (initialized randomly) such that `u[a]` corresponds to $u^{(a)}$ as defined above.\n",
    "    * `x[1] = b_u`, a column vector (initialized with 0s) equal to $b_u$ as defined above.\n",
    "    * `x[2] = v`, a list of column vectors (initialized randomly) such that `v[i]` corresponds to $v^{(i)}$ as defined above.\n",
    "    * `x[3] = b_v`, a column vector (initialized with 0s) equal to $b_v$ as defined above.  \n",
    "* Then we alternate minimizations.\n",
    "* And report the results: the error between predicted scores and a held-out set of actual scores on the same users and items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "qCsmTRm3_nA9"
   },
   "outputs": [],
   "source": [
    "# The ALS outer loop\n",
    "def mf_als(data_train, data_validate, k=2, lam=0.02, max_iter=100, verbose=False):\n",
    "    # size of the problem\n",
    "    n = max(d[0] for d in data_train)+1 # users\n",
    "    m = max(d[1] for d in data_train)+1 # items\n",
    "    # which entries are set in each row and column\n",
    "    us_from_v = [[] for i in range(m)]\n",
    "    vs_from_u = [[] for a in range(n)]\n",
    "    for (a, i, r) in data_train:\n",
    "        # item i was rated by users a with rating r\n",
    "        us_from_v[i].append((a, r))\n",
    "        # user a rated items i with rating r\n",
    "        vs_from_u[a].append((i, r))\n",
    "    # Initial guess at u, b_u, v, b_v\n",
    "    # Note that u and v are lists of column vectors (columns of U, V).\n",
    "    x = ([np.random.normal(1/k, size=(k,1)) for a in range(n)],\n",
    "          np.zeros(n),\n",
    "          [np.random.normal(1/k, size=(k,1)) for i in range(m)],\n",
    "          np.zeros(m))\n",
    "    # Alternation, modifies the contents of x\n",
    "    start_time = time.time()\n",
    "    for i in range(max_iter):\n",
    "        update_U(data_train, vs_from_u, x, k, lam)\n",
    "        update_V(data_train, us_from_v, x, k, lam)\n",
    "        if verbose:\n",
    "            print('train rmse', rmse(data_train, x), 'validate rmse', data_validate and rmse(data_validate, x))\n",
    "        if data_validate == None: # code is slower, print out progress\n",
    "            print(\"Iteration {} finished. Total Elapsed Time: {:.2f}\".format(i + 1, time.time() - start_time))\n",
    "    # The root mean square errors measured on validate set\n",
    "    if data_validate != None:\n",
    "        print('ALS result for k =', k, ': rmse train =', rmse(data_train, x), '; rmse validate =', rmse(data_validate, x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbO6NZnkAqdM"
   },
   "source": [
    "The function `ridge_analytic(X,Y,lam)` is defined at the top of the file. Furthermore, the above code computes RMSE via the following function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pnCAMsfIAXay"
   },
   "outputs": [],
   "source": [
    "# Compute the root mean square error\n",
    "def rmse(data, x):\n",
    "    error = 0.\n",
    "    for datum in data:\n",
    "        error += (datum[-1] - pred(datum, x))**2\n",
    "    return np.sqrt(error/len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3IBSKVLYyb9"
   },
   "source": [
    "Here is an example of `vs_from_u` for the small data set given above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxrGk0QwZTP1"
   },
   "source": [
    "```\n",
    "vs_from_u = \\\n",
    "[[(0, 5), (1, 3), (3, 1)],\n",
    " [(0, 4), (3, 1)],\n",
    " [(0, 1), (1, 1), (3, 5)],\n",
    " [(0, 1), (3, 4)],\n",
    " [(1, 1), (2, 5), (3, 4)]]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gt-zq_R7AYmP"
   },
   "source": [
    "Now, the only part that's missing are the `update_U` and `update_V`\n",
    "procedures.  These are very similar, so we'll just do `update_U`,\n",
    "where we hold the $v^{(i)}$ constant and solve for the $u^{(a)}$.  We\n",
    "have seen above that each of the steps is solving a ridge regression\n",
    "problem, that is, finding a set of coefficients for a linear function,\n",
    "$(\\theta, \\theta_0)$, so as to minimize the mean sum of squared errors\n",
    "on data given by $(X,Y)$ (with regularization on the magnitude of\n",
    "$\\theta$).\n",
    "\n",
    "We have given you a function `ridge_analytic(X,Y,lam)` that solves the ridge regression problem and returns `(th, th0)` as usual, where `th` is a column vector and `th0` is a float."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2jNQBw7BZ6G"
   },
   "source": [
    "### 3.1)\n",
    "Now, write the function `update_U(data, vs_from_u, x, k, lam)`\n",
    "\n",
    "* `data` is a list of `(a, i, r)` triples\n",
    "* `vs_from_u` is a list of lists as defined above\n",
    "* `x` is a list of parameters as defined above\n",
    "* `k` is an integer indicating the length of the individual u and v vectors\n",
    "* `lam` is the regularization parameter\n",
    "\n",
    "The function should update the entries in `x` corresponding to the `u` vectors and the `b_u` entries.  It should also return `x`, so the Tutor can check it.\n",
    "Note that if there are no ratings from a particular user, we don't want to update that user's entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Pzy7mxNVBbKF"
   },
   "outputs": [],
   "source": [
    "def update_U(data, vs_from_u, x, k, lam):\n",
    "    (u, b_u, v, b_v) = x\n",
    "    # Your code here\n",
    "    # user a rated items i with rating r\n",
    "    # vs_from_u[a].append((i, r))\n",
    "    n = len(vs_from_u)\n",
    "    for a in range(n):\n",
    "        # equivalent lab notation in comments\n",
    "        if not vs_from_u[a]:\n",
    "            continue\n",
    "        items_user_a_rated = vs_from_u[a]\n",
    "        # B_a, l_a x k\n",
    "        v_i = np.hstack([v[i] for i, _ in items_user_a_rated]).T\n",
    "        # Z_a, l_a x 1\n",
    "        y_ai = np.array([r for _, r in items_user_a_rated]).reshape((-1, 1))\n",
    "        b_v_i = b_v[[i for i, _ in items_user_a_rated]].reshape((-1, 1))\n",
    "        u_a, b_u_a = ridge_analytic(v_i, (y_ai - b_v_i), lam)\n",
    "        x[0][a] = u_a\n",
    "        x[1][a] = b_u_a\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SK2Mmh6sBjj-"
   },
   "source": [
    "Here is a function to help test your code. It uses `ratings_small`, defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "z8yh1XDeCBKf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def update_U_test():\n",
    "    '''\n",
    "    This is a test function provided to help you debug your implementation\n",
    "    '''\n",
    "    k = 2\n",
    "    lam = 0.01\n",
    "    \n",
    "    vs_from_u = \\\n",
    "      [[(0, 5), (1, 3), (3, 1)],\n",
    "       [(0, 4), (3, 1)],\n",
    "       [(0, 1), (1, 1), (3, 5)],\n",
    "       [(0, 1), (3, 4)],\n",
    "       [(1, 1), (2, 5), (3, 4)]]\n",
    "  \n",
    "    np.random.seed(0)\n",
    "    \n",
    "    first = []\n",
    "    for i in range(5):\n",
    "        first.append(np.random.rand(2, 1))\n",
    "    second = np.zeros((5,))\n",
    "    third = []\n",
    "    for i in range(5):\n",
    "        third.append(np.random.rand(2, 1))\n",
    "    fourth = np.zeros((5,))\n",
    "    x0 = (first, second, third, fourth)\n",
    "    \n",
    "    x_result = update_U(ratings_small, vs_from_u, x0, k, lam)\n",
    "    \n",
    "    assert np.all(np.isclose(x_result[0], np.array([[[4.048442188078757], [-2.5000082235465526]],\n",
    "                                                [[3.2715388359271054], [-1.2879317400952521]],\n",
    "                                                [[-6.237522315961142], [-2.9639103597721355]],\n",
    "                                                [[-3.2715388359271054], [1.2879317400952521]],\n",
    "                                                [[-4.87111151185168], [-1.761023196019822]] ])))\n",
    "    assert np.all(np.isclose(x_result[1].reshape(-1,), \n",
    "                           np.array([[3.043665230868208], [2.048616799474877], [7.462166369240114], [2.951383200525123], [5.487071919883842]]).reshape(-1,)))\n",
    "    assert np.all(np.isclose(x_result[2], np.array([[[0.7917250380826646], [0.5288949197529045]],\n",
    "                                       [[0.5680445610939323], [0.925596638292661]],\n",
    "                                       [[0.07103605819788694], [0.08712929970154071]],\n",
    "                                       [[0.02021839744032572], [0.832619845547938]],\n",
    "                                       [[0.7781567509498505], [0.8700121482468192]] ])))\n",
    "    assert np.all(np.isclose(x_result[3].reshape(-1,), np.array([[0.0], [0.0], [0.0], [0.0], [0.0]]).reshape(-1,)))\n",
    "    print(\"Test passed!\")\n",
    "    \n",
    "update_U_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFRxfWPHDRR_"
   },
   "source": [
    "**NOTE: ** The following **does not** need to be written: it can be filled in by clicking View Answer in the update_U problem. It will be useful in part 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "tBXOkPRlDsPv"
   },
   "outputs": [],
   "source": [
    "def update_V(data, us_from_v, x, k, lam):\n",
    "    (u, b_u, v, b_v) = x\n",
    "    # Your code here\n",
    "    # item i was rated by users a with rating r\n",
    "    # us_from_v[i].append((a, r))\n",
    "    m = len(us_from_v)\n",
    "    for i in range(m):\n",
    "        # equivalent lab notation in comments\n",
    "        if not us_from_v[i]:\n",
    "            continue\n",
    "        item_i_ratings = us_from_v[i]\n",
    "        # B_a, l_a x k\n",
    "        u_a = np.hstack([u[a] for a, _ in item_i_ratings]).T\n",
    "        # Z_a, l_a x 1\n",
    "        y_ai = np.array([r for _, r in item_i_ratings]).reshape((-1, 1))\n",
    "        b_u_a = b_u[[a for a, _ in item_i_ratings]].reshape((-1, 1))\n",
    "        v_i, b_v_i = ridge_analytic(u_a, (y_ai - b_u_a), lam)\n",
    "        x[2][i] = v_i\n",
    "        x[3][i] = b_v_i\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WXU4LYCCSAU"
   },
   "source": [
    "### 3.2) Stochastic Gradient Descent (SGD)\n",
    "Alternatively, we can use Stochastic Gradient Descent directly on the objective function\n",
    "$$J(U, V) = \\frac{1}{2}\\sum_{(a, i) \\in D} (Y_{ai} - {u^{(a)}}\\cdot v^{(i)} - b_u^{(a)} - b_v^{(i)})^2 + \\frac{\\lambda}{2} \\sum_{a = 1}^n \\lVert u^{(a)} \\rVert^2 + \\frac{\\lambda}{2} \\sum_{i = 1}^m  \\lVert v^{(i)} \\rVert^2 $$\n",
    "Note, however, that this is not strictly a sum over the data.  The regularization terms are not inside the first summation and they cannot simply be moved inside, since we would end up penalizing the parameters unevenly depending on how many ratings there were for particular items by particular users.\n",
    "\n",
    "We can, however, define vectors of values $\\lambda_u^{(a)}$ and $\\lambda_v^{(i)}$ so that this is equivalent to the original objective:\n",
    "$$J(U, V) = \\frac{1}{2}\\sum_{(a, i) \\in D} \\left[(Y_{ai} - {u^{(a)}}\\cdot v^{(i)} - b_u^{(a)} - b_v^{(i)})^2 + \\lambda_u^{(a)}\\lVert u^{(a)} \\rVert^2 + \\lambda_v^{(i)} \\lVert v^{(i)} \\rVert^2\\right] $$\n",
    "\n",
    "In order to make this work out, we must have had:\n",
    "$$\\frac{\\lambda}{2} \\sum_{a = 1}^n \\lVert u^{(a)} \\rVert^2 = \\frac{1}{2}\\sum_{(a, i) \\in D} \\lambda_u^{(a)}\\lVert u^{(a)} \\rVert^2 $$\n",
    "matching the terms for each $a$ separately, we must have:\n",
    "$$ \\frac{\\lambda}{2} \\lVert u^{(a)} \\rVert^2 = \\sum_{i : (a, i) \\in D} \\frac{\\lambda_u^{(a)}}{2} \\lVert u^{(a)} \\rVert^2 = \\lvert \\{i : (a, i) \\in D\\} \\rvert \\frac{\\lambda_u^{(a)}}{2} \\lVert u^{(a)} \\rVert^2 $$\n",
    "Then, we must have:\n",
    "$$ \\lambda_u^{(a)} = \\frac{\\lambda}{\\lvert \\{i : (a, i) \\in D\\} \\rvert}$$\n",
    "Similarly,\n",
    "$$ \\lambda_v^{(i)} = \\frac{\\lambda}{\\lvert \\{a : (a, i) \\in D\\} \\rvert}$$\n",
    "\n",
    "\n",
    "This is now strictly a sum over the data. Note that the offset terms are not regularized.\n",
    "\n",
    "The \"outer loop\" of this approach is given here:\n",
    "\n",
    "* Define n and m from the data.\n",
    "* Define vectors $\\lambda_u$ and $\\lambda_v$.\n",
    "* Initialize the set of parameters (created and initialized as in ALS).  \n",
    "* Loop picking a random data entry `(a,i,r)` and taking a step down the gradient during each iteration.\n",
    "* And report the results: the error between predicted scores and a held-out set of actual scores on the same users and items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "NwwpI-deCaQZ"
   },
   "outputs": [],
   "source": [
    "# The SGD outer loop\n",
    "def mf_sgd(data_train, data_validate, step_size_fn, k=2, lam=0.02, max_iter=100, verbose=False):\n",
    "    # size of the problem\n",
    "    ndata = len(data_train)\n",
    "    n = max(d[0] for d in data_train)+1\n",
    "    m = max(d[1] for d in data_train)+1\n",
    "    # Distribute the lambda among the users and items\n",
    "    lam_uv = lam/counts(data_train,0), lam/counts(data_train,1)\n",
    "    # Initial guess at u, b_u, v, b_v (also b)\n",
    "    x = ([np.random.normal(1/k, size=(k,1)) for j in range(n)],\n",
    "         np.zeros(n),\n",
    "         [np.random.normal(1/k, size=(k,1)) for j in range(m)],\n",
    "         np.zeros(m))\n",
    "    di = int(max_iter/10.)\n",
    "    for i in range(max_iter):\n",
    "        if i%di == 0 and verbose:\n",
    "            print('i=', i, 'train rmse=', rmse(data_train, x),\n",
    "                  'validate rmse', data_validate and rmse(data_validate, x))\n",
    "        step = step_size_fn(i)\n",
    "        j = np.random.randint(ndata)            # pick data item\n",
    "        sgd_step(data_train[j], x, lam_uv, step) # modify x\n",
    "    print('SGD result for k =', k, ': rmse train =', rmse(data_train, x), '; rmse validate =', rmse(data_validate, x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dm9qTaclChU4"
   },
   "source": [
    "### 3.2)\n",
    "The missing procedure is `sgd_step` which takes a gradient step using a\n",
    "noisy estimate of the gradient based on one point.  This function will\n",
    "update the relevant components of `x` corresponding to `u[a]`,\n",
    "`b_u[a]`, `v[i]` and `b_v[i]`.  Your function should modify the entries in `x`\n",
    "and also return `x` so that the Tutor can check it.\n",
    "\n",
    "You will need to derive the gradient for each of the variable\n",
    "components of `x`, including the offsets.  Refer back to section 1 for guidance, but\n",
    "remember the offsets and remember that the offset terms are not regularized.\n",
    "\n",
    "__WARNING: In numpy `x += y` and `x -= y` can produce different results from `x = x+y` and `x = x-y`.  For consistency in checking, please don't use `x += y` or `x -= y`! \n",
    "<a href=\"https://stackoverflow.com/questions/31987713/numpy-array-difference-between-a-x-vs-a-a-x\">Read here</a>__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "trIPrEQKCm-e"
   },
   "outputs": [],
   "source": [
    "def sgd_step(data, x, lam, step):\n",
    "    (a, i, r) = data\n",
    "    (u, b_u, v, b_v) = x\n",
    "    (lam_u, lam_v) = lam\n",
    "    print('lam_u', lam_u, 'lam_v', lam_v)\n",
    "    # Your code here\n",
    "    u_a = u[a]\n",
    "    b_u_a = b_u[a]\n",
    "    lam_u_a = lam_u[a]\n",
    "    v_i = v[i]\n",
    "    b_v_i = b_v[i]\n",
    "    lam_v_i = lam_v[i]\n",
    "    pred = np.dot(u_a.T, v_i) + b_u_a + b_v_i\n",
    "    d_u_a = (pred - r) * v_i + lam_u_a * u_a\n",
    "    d_b_u_a = pred - r\n",
    "    d_v_i = (pred - r) * u_a + lam_v_i * v_i\n",
    "    d_b_v_i = pred - r\n",
    "    x[0][a] = u_a - step*d_u_a\n",
    "    x[1][a] = b_u_a - step*d_b_u_a\n",
    "    x[2][i] = v_i - step*d_v_i\n",
    "    x[3][i] = b_v_i - step*d_b_v_i\n",
    "    return x\n",
    "\n",
    "# def sgd_step(data, x, lam, step):\n",
    "#     (a, i, r) = data\n",
    "#     (u, b_u, v, b_v) = x\n",
    "#     (lam_u, lam_v) = lam\n",
    "#     print('lam_u', lam_u, 'lam_v', lam_v)\n",
    "#     print('lam_u.shape', lam_u.shape, 'lam_v.shape', lam_v.shape)\n",
    "#     # Your code here\n",
    "#     u_a = u[a]\n",
    "#     b_u_a = b_u[[a]].reshape((1, 1))\n",
    "#     lam_u_a = lam_u[[a]].reshape((1, 1))\n",
    "#     v_i = v[i]\n",
    "#     b_v_i = b_v[[i]].reshape((1, 1))\n",
    "#     lam_v_i = lam_v[[i]].reshape((1, 1))\n",
    "#     print('u_a', u_a, 'b_u_a', b_u_a, 'lam_u_a', lam_u_a, 'v_i', v_i, 'b_v_i', b_v_i, 'lam_v_i', lam_v_i)\n",
    "#     print('u_a.shape', u_a.shape, 'b_u_a.shape', b_u_a.shape, 'lam_u_a.shape', lam_u_a.shape, 'v_i.shape', v_i.shape, 'b_v_i.shape', b_v_i.shape, 'lam_v_i.shape', lam_v_i.shape)\n",
    "#     d_u_a = (np.dot(u_a.T, v_i) + b_u_a + b_v_i - r) * v_i + lam_u_a * u_a\n",
    "#     d_b_u_a = np.dot(u_a.T, v_i) + b_u_a + b_v_i - r\n",
    "#     print('d_u_a', d_u_a, 'd_b_u_a', d_b_u_a)\n",
    "#     print('d_u_a.shape', d_u_a.shape, 'd_b_u_a.shape', d_b_u_a.shape)\n",
    "#     d_v_i = (np.dot(u_a.T, v_i) + b_u_a + b_v_i - r) * u_a + lam_v_i * v_i\n",
    "#     d_b_v_i = np.dot(u_a.T, v_i) + b_u_a + b_v_i - r\n",
    "#     print('d_v_i', d_v_i, 'd_b_v_i', d_b_v_i)\n",
    "#     print('d_v_i.shape', d_v_i.shape, 'd_b_v_i.shape', d_b_v_i.shape)\n",
    "#     x[0][a] = u_a - step*d_u_a\n",
    "#     x[1][a] = b_u_a - step*d_b_u_a\n",
    "#     x[2][i] = v_i - step*d_v_i\n",
    "#     x[3][i] = b_v_i - step*d_b_v_i\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsQHchl2nyrt"
   },
   "source": [
    "Here is a function to help test your code. It uses `ratings_small`, defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "UdG9v_WdoELe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def sgd_step_test():\n",
    "    '''\n",
    "    This is a test function provided to help you debug your implementation\n",
    "    '''\n",
    "    step = 0.025\n",
    "    lam =(np.array([ 0.00333333,  0.005,  0.00333333,  0.005,  0.00333333]), np.array([ 0.0025,  0.00333333,  0.01,  0.002]))\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    first = []\n",
    "    for i in range(5):\n",
    "        first.append(np.random.rand(2, 1))\n",
    "    second = np.zeros((5,))\n",
    "    third = []\n",
    "    for i in range(5):\n",
    "        third.append(np.random.rand(2, 1))\n",
    "    fourth = np.zeros((5,))\n",
    "    x0 = (first, second, third, fourth)\n",
    "    \n",
    "    x_result = sgd_step(ratings_small[3], x0, lam, step)\n",
    "    \n",
    "    assert np.all(np.isclose(x_result[0], np.array([[[0.5488135039273248], [0.7151893663724195]],\n",
    "                                                [[0.6667107015911342], [0.5875840438721468]],\n",
    "                                                [[0.4236547993389047], [0.6458941130666561]],\n",
    "                                                [[0.4375872112626925], [0.8917730007820798]],\n",
    "                                                [[0.9636627605010293], [0.3834415188257777]]])))\n",
    "    assert np.all(np.isclose(x_result[1].reshape(-1,), np.array([[0.0], [0.08086477989447478], [0.0], [0.0], [0.0]]).reshape(-1,)))\n",
    "    assert np.all(np.isclose(x_result[2], np.array([[[0.8404178830022684], [0.5729237224816648]],\n",
    "                                                [[0.5680445610939323], [0.925596638292661]],\n",
    "                                                [[0.07103605819788694], [0.08712929970154071]],\n",
    "                                                [[0.02021839744032572], [0.832619845547938]],\n",
    "                                                [[0.7781567509498505], [0.8700121482468192]]])))\n",
    "    assert np.all(np.isclose(x_result[3].reshape(-1,), np.array([[0.08086477989447478], [0.0], [0.0], [0.0], [0.0]]).reshape(-1,)))\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "sgd_step_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8kZ8OrXHPHL"
   },
   "source": [
    "# 4) MovieLens\n",
    "\n",
    "Now we're going to start working with some real-world data. There's a\n",
    "commonly used <a\n",
    "href=\"https://grouplens.org/datasets/movielens/\">dataset</a> of\n",
    "movie ratings, called the MovieLens Dataset.\n",
    "\n",
    "Below, we have included the following utility functions:\n",
    "\n",
    "* `load_ratings_data_small(path_data='ratings.csv')` returns\n",
    "  a list of ratings triples (a, i, r) for a subset of the data, to be\n",
    "  used in parameter tuning.\n",
    "\n",
    "* `load_ratings_data(path_data='ratings.csv')` returns the full list\n",
    "  of ratings triples.\n",
    "\n",
    "* `load_movies(path_movies='movies.csv')` returns two dictionaries.\n",
    "  The first maps movie indices to title strings and the second maps\n",
    "  movie indices to a list of genre strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ua8q7jsAK-xG"
   },
   "outputs": [],
   "source": [
    "def load_ratings_data_small(path_data='hw12_code_and_data/ratings.csv'):\n",
    "    \"\"\"\n",
    "    Returns two lists of triples (a, i, r) (training, validate)\n",
    "    \"\"\"\n",
    "    # we want to \"randomly\" sample but make it deterministic\n",
    "    def user_hash(uid):\n",
    "        return 71 * uid % 401\n",
    "    def user_movie_hash(uid, iid):\n",
    "        return (17 * uid + 43 * iid) % 61\n",
    "    data_train = []\n",
    "    data_validate = []\n",
    "    with open(path_data) as f_data:\n",
    "        for line in f_data:\n",
    "            (uid, iid, rating, timestamp) = line.strip().split(\",\")\n",
    "            h1 = user_hash(int(uid))\n",
    "            if h1 <= 40:\n",
    "                h2 = user_movie_hash(int(uid), int(iid))\n",
    "                if h2 <= 12:\n",
    "                    data_validate.append([int(uid), int(iid), float(rating)])\n",
    "                else:\n",
    "                    data_train.append([int(uid), int(iid), float(rating)])\n",
    "    print('Loading from', path_data,\n",
    "          'users_train', len(set(x[0] for x in data_train)),\n",
    "          'items_train', len(set(x[1] for x in data_train)),\n",
    "          'users_validate', len(set(x[0] for x in data_validate)),\n",
    "          'items_validate', len(set(x[1] for x in data_validate)))\n",
    "    return data_train, data_validate\n",
    "\n",
    "def load_ratings_data(path_data='hw12_code_and_data/ratings.csv'):\n",
    "    \"\"\"\n",
    "    Returns a list of triples (a, i, r)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(path_data) as f_data:\n",
    "        for line in f_data:\n",
    "            (uid, iid, rating, timestamp) = line.strip().split(\",\")\n",
    "            data.append([int(uid), int(iid), float(rating)])\n",
    "\n",
    "    print('Loading from', path_data,\n",
    "          'users', len(set(x[0] for x in data)),\n",
    "          'items', len(set(x[1] for x in data)))\n",
    "    return data\n",
    "\n",
    "def load_movies(path_movies='hw12_code_and_data/movies.csv'):\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping item_id to item_name and another dictionary\n",
    "    mapping item_id to a list of genres\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    genreMap = {}\n",
    "    with open(path_movies, encoding = \"utf8\") as f_data:\n",
    "        for line in f_data:\n",
    "            parts = line.strip().split(\",\")\n",
    "            item_id = int(parts[0])\n",
    "            item_name = \",\".join(parts[1:-1]) # file is poorly formatted\n",
    "            item_genres = parts[-1].split(\"|\")\n",
    "            data[item_id] = item_name\n",
    "            genreMap[item_id] = item_genres\n",
    "    return data, genreMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuXHXoNNLC4K"
   },
   "source": [
    "You will need to use the above functions to answer the questions below.\n",
    "\n",
    "**NOTE: Our checkers will succeed on 95% of submissions stemming from\n",
    "correct implementations.  However, it is possible (but unlikely) for\n",
    "your implementation to be correct but your answer to be rejected due\n",
    "to getting an unlucky initialization.  If you are fairly sure that\n",
    "your code is correct, you should try re-training a new model and\n",
    "submitting answers from the new model.**\n",
    "\n",
    "If you haven't already, complete the implementations of `update_U`,\n",
    "`update_V`, and `sgd_step` (if you implemented it) above based on your solutions\n",
    "to the previous questions; the definition of `update_V` can be found via View Answer in\n",
    "the `update_U` problem.  We will limit ourselves to running ALS in\n",
    "this problem since it requires less parameter tuning than SGD does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tng9UdUHf81"
   },
   "source": [
    "## 4.1) Recommendations\n",
    "\n",
    "In the following, we will want to be able to save models locally so as to avoid running als (which takes some time) too many times. In order to do this, we include the following helper functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "b_iEFmSpIamo"
   },
   "outputs": [],
   "source": [
    "# After retrieving the output x from mf_als, you can use this function to save the output so\n",
    "# you don't have to re-train your model\n",
    "def save_model(x):\n",
    "    pickle.dump(x, open(\"ALSmodel\", \"wb\"))\n",
    "\n",
    "# After training and saving your model once, you can use this function to retrieve the previous model\n",
    "def load_model():\n",
    "    x = pickle.load(open(\"ALSmodel\", \"rb\"))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INs64XAdIc-2"
   },
   "source": [
    "Now, compute a model with ALS and save it to disk, by executing the following code:\n",
    "\n",
    "<b> Note: If you are running into errors when running `mf_als`, make sure that your code for `update_U` did not update a user's entries if that user had no ratings. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "mkfGyqHmHsyJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from hw12_code_and_data/ratings.csv users 13366 items 2000\n",
      "Iteration 1 finished. Total Elapsed Time: 3.10\n",
      "Iteration 2 finished. Total Elapsed Time: 6.13\n",
      "Iteration 3 finished. Total Elapsed Time: 9.24\n",
      "Iteration 4 finished. Total Elapsed Time: 12.30\n",
      "Iteration 5 finished. Total Elapsed Time: 15.35\n",
      "Iteration 6 finished. Total Elapsed Time: 18.35\n",
      "Iteration 7 finished. Total Elapsed Time: 21.38\n",
      "Iteration 8 finished. Total Elapsed Time: 24.39\n",
      "Iteration 9 finished. Total Elapsed Time: 27.42\n",
      "Iteration 10 finished. Total Elapsed Time: 30.45\n",
      "Iteration 11 finished. Total Elapsed Time: 33.49\n",
      "Iteration 12 finished. Total Elapsed Time: 36.48\n",
      "Iteration 13 finished. Total Elapsed Time: 39.54\n",
      "Iteration 14 finished. Total Elapsed Time: 42.55\n",
      "Iteration 15 finished. Total Elapsed Time: 45.59\n",
      "Iteration 16 finished. Total Elapsed Time: 48.59\n",
      "Iteration 17 finished. Total Elapsed Time: 51.61\n",
      "Iteration 18 finished. Total Elapsed Time: 54.62\n",
      "Iteration 19 finished. Total Elapsed Time: 57.68\n",
      "Iteration 20 finished. Total Elapsed Time: 60.71\n"
     ]
    }
   ],
   "source": [
    "data = load_ratings_data()\n",
    "movies_dict, genres_dict = load_movies()\n",
    "model = mf_als(data, None, k=10, lam=1, max_iter=20)\n",
    "save_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaSm5pbAHvK5"
   },
   "source": [
    "This takes two minutes or so on a reasonably fast laptop. As alluded to before, <b>you only have to run `mf_als` once</b> -- henceforth, you can load the saved model via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "5coq_y-DIosy"
   },
   "outputs": [],
   "source": [
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8zWX7LvIrHn"
   },
   "source": [
    "Note that `load_model()` returns a tuple $(u, b_u, v, b_v)$ corresponding to the matrix factorization learned using the alternating least squares algorithm from above.\n",
    "\n",
    "We will assume this model for the rest of the questions below.\n",
    "\n",
    "We have introduced a \"synthetic\" user into the ratings file with id = 270894. Remember that we can access user data in `data`, which is a list of $(a, i, r)$ tuples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQ6zZ5qrIzSr"
   },
   "source": [
    "### 4.1A)\n",
    "Write a piece of code to get relevant movie ratings in `data`, then look for the movies in `genres_dict`. Based on the movies that they've rated 5.0, what is this user's favorite genre? A list of all possible genres is given by the `genres` list below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "XZ1ez7IQI2H6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Animation', 74)\n"
     ]
    }
   ],
   "source": [
    "genres = ['Western', 'Comedy', 'Children', 'Crime', 'Musical',\n",
    "          'Adventure', 'Drama', 'Horror', 'War', 'Documentary',\n",
    "          'Romance', 'Animation', 'Film-Noir', 'Sci-Fi', 'Mystery',\n",
    "          'Fantasy', 'IMAX', 'Action', 'Thriller']\n",
    "\n",
    "# Your code to find the user's favorite genre here:\n",
    "user_id = 270894\n",
    "users_genre_preference = {}\n",
    "for a, i, r in data:\n",
    "    if a != user_id or r != 5.0:\n",
    "        continue\n",
    "    movie_genres = genres_dict[i]\n",
    "    for genre in movie_genres:\n",
    "        users_genre_preference[genre] = users_genre_preference.get(genre, 0) + 1\n",
    "print(max(users_genre_preference.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SJbluRnI813"
   },
   "source": [
    "### 4.1B)\n",
    "Write a piece of code to find the top 50 movies in `movies_dict` that are predicted for this user.  How many movies in this list match their favorite genre? Remember to remove movies the user has already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "oCPUnW6HJGKj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(270894, 78499, 5.569226831622767), (270894, 2761, 5.35335081092243), (270894, 588, 5.339685230759691), (270894, 6377, 5.332275791924108), (270894, 4886, 5.296043424778638), (270894, 152081, 5.274982107114708), (270894, 720, 5.271315885886009), (270894, 68954, 5.262040271288446), (270894, 745, 5.200077200356188), (270894, 5444, 5.182299512683893), (270894, 1223, 5.18002614608879), (270894, 551, 5.126914094909444), (270894, 5618, 5.046585355720123), (270894, 1033, 4.959269004715457), (270894, 2085, 4.956644834568388), (270894, 616, 4.953235599395153), (270894, 2394, 4.949630999424179), (270894, 106696, 4.921827375048215), (270894, 115617, 4.876607191857276), (270894, 783, 4.868274036695403), (270894, 1023, 4.845080741061674), (270894, 79091, 4.838588311642232), (270894, 103141, 4.782489337676313), (270894, 2089, 4.7712221505534425), (270894, 1566, 4.758214283714145), (270894, 1688, 4.7324014912214345)]\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "#Your code to find how many movies match their favorite genre here\n",
    "def pred(a, i, model):\n",
    "    (u, b_u, v, b_v) = model\n",
    "    return np.dot(u[a].T, v[i]) + b_u[a] + b_v[i]\n",
    "\n",
    "user_id = 270894\n",
    "all_movies = set([i for i in movies_dict])\n",
    "movies_seen_by_user = set([i for a, i, _ in data if a == user_id])\n",
    "movies_not_seen_by_user = all_movies.difference(movies_seen_by_user)\n",
    "predictions = [(user_id, i, pred(user_id, i, model)[0, 0]) for i in movies_not_seen_by_user]\n",
    "predictions.sort(key=lambda x: x[2], reverse=True)\n",
    "animation_predicted_top_50 = list(filter(lambda x: 'Animation' in genres_dict[x[1]], predictions[:50]))\n",
    "print(animation_predicted_top_50)\n",
    "print(len(animation_predicted_top_50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLGXJQMkJKVg"
   },
   "source": [
    "## 4.2) Similarity\n",
    "\n",
    "Typically, movie similarities are estimated based on similarity of\n",
    "genre or cast, or by human-generated recommendations.  It turns out that\n",
    "training a recommender system gives us, as a by-product, a new way of\n",
    "measuring similarity between movies.\n",
    "\n",
    "First, let's think about what movie similarity might mean.  One\n",
    "intuition is that two movies A and B are similar if lots of people\n",
    "like both of them.  However, this criterion can be misleading.  Movies\n",
    "with high average ratings will look similar since lots of people will tend to\n",
    "like them.  So we will refine our criterion to be: \"If users tend to\n",
    "like movies A and B more than what is average for the movie and for\n",
    "the user, then A and B are similar\".\n",
    "\n",
    "### 4.2A)\n",
    "How can we tell if user $a$ liked movie $i$ more than average (for the movie and the user)? Recall that the predicted rating will be $ u^{(a)}\\cdot v^{(i)} + b_u^{(a)} + b_v^{(i)}$. Under what conditions will this predicted rating be higher than average for the movie and the user? Hint: Are the biases $b_u^{(a)}$ and/or $b_v^{(i)}$ relevant? Are the magnitudes of $u^{(a)}$ and/or $v^{(i)}$ relevant?\n",
    "\n",
    "We can generalize the above criterion to be \"If users tend to rate movies A and B similarly relative to the average, then A and B are similar\". Our previous criterion\n",
    "only discussed the case where ratings are high, but this new criterion should hold for low \n",
    "ratings as well, for example. So in general, if movies A and B are similar, then for\n",
    "every user $a$ we should expect the dot product/angle (based on the answer to the previous question)\n",
    "between $u^{(a)}$ and $v^{(A)}, v^{(B)}$ to be very close. This in turn implies that the two vectors\n",
    "$v^{(A)}, v^{(B)}$ are either very close together in space or very close in angle, again\n",
    "depending on the answer to the previous question. This motivates us to use <a\n",
    "href=\"https://en.wikipedia.org/wiki/Cosine_similarity\">cosine\n",
    "similarity</a> to compute similarities between movies, being the standard way\n",
    "to measure the type of proximity we are looking for. For the rest of\n",
    "this homework the similarity between movies $A$ and $B$ will be\n",
    "computed as $\\frac{v^{(A)} \\cdot v^{(B)}}{\\|v^{(A)}\\| \\|v^{(B)}\\|}$.\n",
    "**Note that the similarity can be positive or negative.**\n",
    "\n",
    "Write a piece of code which identifies the movies in the dataset with\n",
    "the highest similarity to a given movie, using the formula for cosine\n",
    "similarity defined above.\n",
    "\n",
    "### 4.2B)\n",
    "Find the 10 movies (in `movies_dict`) most similar to \"Star Wars: Episode IV - A New Hope (1977)\" (id 260).\n",
    "\n",
    "### 4.2C)\n",
    "Find the 10 movies (in `movies_dict`) most similar to \"Star Wars: Episode I - The Phantom Menace (1999)\" (id 2628)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "SIKrrrHuKMEt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1196, 1210, 1198, 1291, 2947, 2993, 592, 122886, 33493, 6104]\n",
      "['Star Wars: Episode V - The Empire Strikes Back (1980)', 'Star Wars: Episode VI - Return of the Jedi (1983)', 'Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)', 'Indiana Jones and the Last Crusade (1989)', 'Goldfinger (1964)', 'Thunderball (1965)', 'Batman (1989)', 'Star Wars: Episode VII - The Force Awakens (2015)', 'Star Wars: Episode III - Revenge of the Sith (2005)', 'Monty Python Live at the Hollywood Bowl (1982)']\n",
      "[5378, 33493, 6934, 6365, 59615, 7318, 52722, 153, 1210, 59501]\n",
      "['Star Wars: Episode II - Attack of the Clones (2002)', 'Star Wars: Episode III - Revenge of the Sith (2005)', '\"Matrix Revolutions, The (2003)\"', '\"Matrix Reloaded, The (2003)\"', 'Indiana Jones and the Kingdom of the Crystal Skull (2008)', '\"Passion of the Christ, The (2004)\"', 'Spider-Man 3 (2007)', 'Batman Forever (1995)', 'Star Wars: Episode VI - Return of the Jedi (1983)', '\"Chronicles of Narnia: Prince Caspian, The (2008)\"']\n"
     ]
    }
   ],
   "source": [
    "#Code to identify movies with highest similarity to a given movie\n",
    "def similarity(i, j, model):\n",
    "    u, b_u, v, b_v = model\n",
    "    return np.dot(v[i].T, v[j]) / (np.linalg.norm(v[i]) * np.linalg.norm(v[j]))\n",
    "\n",
    "def compute_similarity_scores(i, movies_dict):\n",
    "    similarity_scores = []\n",
    "    for j in movies_dict:\n",
    "        if i == j:\n",
    "            continue\n",
    "        similarity_score = similarity(i, j, model)\n",
    "        similarity_scores += [(j, similarity_score)]\n",
    "    return similarity_scores\n",
    "        \n",
    "movie_id = 260\n",
    "movie_similarity_scores = compute_similarity_scores(movie_id, movies_dict)\n",
    "movie_similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "top_10_similar_movies = list(map(lambda x: x[0], movie_similarity_scores[:10]))\n",
    "print(top_10_similar_movies)\n",
    "print(list(map(lambda x: movies_dict[x], top_10_similar_movies)))\n",
    "\n",
    "movie_id = 2628\n",
    "movie_similarity_scores = compute_similarity_scores(movie_id, movies_dict)\n",
    "movie_similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "top_10_similar_movies = list(map(lambda x: x[0], movie_similarity_scores[:10]))\n",
    "print(top_10_similar_movies)\n",
    "print(list(map(lambda x: movies_dict[x], top_10_similar_movies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DHxeY8EKPCu"
   },
   "source": [
    "Make sure that you also print the movie names (using `movies_dict`) so you can see that they\n",
    "make sense!  These results are fairly interesting; they would be quite\n",
    "clear to one who has seen the movies as the Star Wars prequels (Episodes\n",
    "I-III) and the originals (Episodes IV-VI) are very different movies!\n",
    "But it's interesting that our model can learn this difference.\n",
    "\n",
    "Now, we look at how similar movies within the same genre are. You can\n",
    "use `genres_dict` to help with this.\n",
    "\n",
    "### 4.2D)\n",
    "For calibration, compute the average similarity between all pairs\n",
    "of movies in `movies_dict`. **Remember not to compare a movie to itself.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "eiihalEzKqFQ"
   },
   "outputs": [],
   "source": [
    "#Code to compute average similarity between all pairs of movies\n",
    "def compute_average_similarity_of_all_pairs(movies_dict):\n",
    "    count = 0\n",
    "    similarity_score_sum = 0\n",
    "    all_movies = [i for i in movies_dict]\n",
    "    for i in range(len(all_movies)):\n",
    "        for j in range(i+1, len(all_movies)):\n",
    "            similarity_score_sum += similarity(all_movies[i], all_movies[j], model)\n",
    "            count += 1\n",
    "    return similarity_score_sum / count\n",
    "\n",
    "avg_similarity = compute_average_similarity_of_all_pairs(movies_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEy5Q8OdKs-o"
   },
   "source": [
    "Now compute the average similarities across all pairs of movies within\n",
    "each genre. Remember that a list of all possible genres is given by\n",
    "the `genres` list defined at the top of your code file.\n",
    "\n",
    "### 4.2E)\n",
    "Find the genre whose movies have the highest average similarity and the value of that average similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "TlIQ2camL-hI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Film-Noir', array([[0.43176651]]))\n"
     ]
    }
   ],
   "source": [
    "#Code to find the genre with the highest average similarity and its value\n",
    "def compute_genre_average_similarity_of_all_pairs(genre_to_movies):\n",
    "    genre_to_similarity_score = {}\n",
    "    for genre in genre_to_movies:\n",
    "        count = 0\n",
    "        similarity_score_sum = 0\n",
    "        movies_in_genre = genre_to_movies[genre]\n",
    "        for i in range(len(movies_in_genre)):\n",
    "            for j in range(i+1, len(movies_in_genre)):\n",
    "                similarity_score_sum += similarity(movies_in_genre[i], movies_in_genre[j], model)\n",
    "                count += 1\n",
    "        genre_to_similarity_score[genre] = similarity_score_sum / count\n",
    "    return genre_to_similarity_score\n",
    "    \n",
    "\n",
    "genre_to_movies = {genre : [] for genre in genres}\n",
    "for i in movies_dict:\n",
    "    movie_genres = genres_dict[i]\n",
    "    for genre in movie_genres:\n",
    "        genre_to_movies[genre] += [i]\n",
    "        \n",
    "genre_to_similarity_score = compute_genre_average_similarity_of_all_pairs(genre_to_movies)\n",
    "print(max(genre_to_similarity_score.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLb9V0ilMGHK"
   },
   "source": [
    "### 4.2F)\n",
    "Find the genre whose movies have the lowest average similarity and the value of that average similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "v59Td1uRMHKd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Crime', array([[0.05645273]]))\n"
     ]
    }
   ],
   "source": [
    "#Code to find the genre with the lowest average similarity and its value\n",
    "print(min(genre_to_similarity_score.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfjaV_VZMKyH"
   },
   "source": [
    "These results tell us that there is enormous variation in how similar\n",
    "the movies under most genres are.  But, note that the similarities are\n",
    "generally *positive* for movies in a genre.\n",
    "\n",
    "Next we look at the similarities across genres. More specifically, as we compute the similarity of different genres to the Comedy genre, we find the average similarity of pairs of movies, where one movie belongs to the Comedy genre and the other belongs to the genre being compared to. Note that movies can belong to multiple genres, so it's possible that both movies technically belong to the Comedy genre.\n",
    "\n",
    "### 4.2G)\n",
    "Which genre is most similar to Comedy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "M4QE_6zCMPIm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Children', array([[0.04526065]]))\n"
     ]
    }
   ],
   "source": [
    "#Code to find the genre most similar to Comedy\n",
    "def compute_genre_to_genre_average_similarity(target_genre, genre_to_movies):\n",
    "    genre_to_similarity_score = {}\n",
    "    movies_in_target_genre = genre_to_movies[target_genre]\n",
    "    for genre in genre_to_movies:\n",
    "        if genre == target_genre:\n",
    "            continue\n",
    "        count = 0\n",
    "        similarity_score_sum = 0\n",
    "        movies_in_genre = genre_to_movies[genre]\n",
    "        for i in range(len(movies_in_target_genre)):\n",
    "            for j in range(len(movies_in_genre)):\n",
    "                similarity_score_sum += similarity(movies_in_target_genre[i], movies_in_genre[j], model)\n",
    "                count += 1\n",
    "        genre_to_similarity_score[genre] = similarity_score_sum / count\n",
    "    return genre_to_similarity_score\n",
    "\n",
    "genre_to_movies = {genre : [] for genre in genres}\n",
    "for i in movies_dict:\n",
    "    movie_genres = genres_dict[i]\n",
    "    for genre in movie_genres:\n",
    "        genre_to_movies[genre] += [i]\n",
    "\n",
    "target_genre = 'Comedy'\n",
    "target_genre_to_similarity_score = compute_genre_to_genre_average_similarity(target_genre, genre_to_movies)\n",
    "print(max(target_genre_to_similarity_score.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8TzyMcnMWwx"
   },
   "source": [
    "### 4.2H)\n",
    "Which genre is least similar to Comedy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "7Wwt6iy0MaEf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('War', array([[-0.06082649]]))\n"
     ]
    }
   ],
   "source": [
    "#Code to find genre least similar to Comedy\n",
    "print(min(target_genre_to_similarity_score.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QN0eZ70JMcsB"
   },
   "source": [
    "Observe that this last similarity value is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Collaborative filtering and the SVD (OPTIONAL)\n",
    "As an exercise to explore the use of SVD for collaborative filtering, go back to the dataset with\n",
    "$$\n",
    "Y =\n",
    "\\begin{bmatrix}\n",
    "    4 & 3 & 1 \\\\\n",
    "    1 & 3 & -2 \\\\\n",
    "    5 & 2 & 3 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and write some python code to compute $U$ and $V^T$ using the SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y = \n",
      " [[ 4  3  1]\n",
      " [ 1  3 -2]\n",
      " [ 5  2  3]]\n",
      "U @ Sigma @ Vt = \n",
      " [[ 4.  3.  1.]\n",
      " [ 1.  3. -2.]\n",
      " [ 5.  2.  3.]]\n",
      "[[-0.6311939  -0.19072721]\n",
      " [-0.21351391 -0.88911878]\n",
      " [-0.74565815  0.41604197]] [[-0.80844891 -0.50325864 -0.30519027]\n",
      " [ 0.11435482 -0.64295988  0.75731471]]\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([[4, 3, 1], [1, 3, -2], [5, 2, 3]])\n",
    "\n",
    "# Compute the truncated SVD of Y\n",
    "U, s, Vt = np.linalg.svd(Y, full_matrices=False)\n",
    "k = 2\n",
    "U = U[:, :k]\n",
    "s = s[:k]\n",
    "Vt = Vt[:k, :]\n",
    "\n",
    "# Construct the diagonal matrix Sigma\n",
    "Sigma = np.diag(s)\n",
    "\n",
    "# Verify that Y = U @ Sigma @ Vt\n",
    "Y_reconstructed = U @ Sigma @ Vt\n",
    "print(\"Y = \\n\", Y)\n",
    "print(\"U @ Sigma @ Vt = \\n\", Y_reconstructed)\n",
    "print(U, Vt)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
