{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCZ5V-cO65Yr"
   },
   "source": [
    "# MIT 6.036 Spring 2019: Homework 9\n",
    "\n",
    "This colab notebook provides code and a framework for question 1 and 5 of the [the homework](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week9/week9_homework).  You can work out your solutions here, then submit your results back on the homework page when ready.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqYqLxGp7hZZ"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaNYfsS87tUi"
   },
   "source": [
    "First, download the code distribution for this homework that contains test cases and helper functions.\n",
    "\n",
    "Run the next code block to download and import the code for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "c7CRuXxj7ubB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: code_for_hw9*\n",
      "--2023-04-04 15:13:28--  https://introml_oll.odl.mit.edu/cat-soop/_static/6.036/homework/hw09/code_for_hw9.zip\n",
      "Resolving introml_oll.odl.mit.edu (introml_oll.odl.mit.edu)... 3.226.240.108\n",
      "Connecting to introml_oll.odl.mit.edu (introml_oll.odl.mit.edu)|3.226.240.108|:443... connected.\n",
      "WARNING: cannot verify introml_oll.odl.mit.edu's certificate, issued by ‘CN=InCommon RSA Server CA,OU=InCommon,O=Internet2,L=Ann Arbor,ST=MI,C=US’:\n",
      "  Unable to locally verify the issuer's authority.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8058 (7.9K) [application/zip]\n",
      "Saving to: ‘code_for_hw9.zip’\n",
      "\n",
      "code_for_hw9.zip    100%[===================>]   7.87K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-04-04 15:13:29 (549 MB/s) - ‘code_for_hw9.zip’ saved [8058/8058]\n",
      "\n",
      "Archive:  code_for_hw9.zip\n",
      "   creating: code_for_hw9/\n",
      "  inflating: code_for_hw9/util.py    \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/code_for_hw9/\n",
      "  inflating: __MACOSX/code_for_hw9/._util.py  \n",
      "  inflating: code_for_hw9/sm.py      \n",
      "  inflating: __MACOSX/code_for_hw9/._sm.py  \n",
      "  inflating: code_for_hw9/mdp.py     \n",
      "  inflating: __MACOSX/code_for_hw9/._mdp.py  \n",
      "  inflating: code_for_hw9/tests.py   \n",
      "  inflating: __MACOSX/code_for_hw9/._tests.py  \n",
      "  inflating: code_for_hw9/dist.py    \n",
      "  inflating: __MACOSX/code_for_hw9/._dist.py  \n",
      "  inflating: __MACOSX/._code_for_hw9  \n"
     ]
    }
   ],
   "source": [
    "!rm -rf code_for_hw9*\n",
    "!wget --no-check-certificate https://introml_oll.odl.mit.edu/cat-soop/_static/6.036/homework/hw09/code_for_hw9.zip\n",
    "!unzip code_for_hw9.zip\n",
    "!mv code_for_hw9/* .\n",
    "\n",
    "from dist import *\n",
    "from sm import *\n",
    "from util import *\n",
    "from mdp import *\n",
    "\n",
    "import mdp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhGY4b888N52"
   },
   "source": [
    "## 1) State Machines\n",
    "\n",
    "We will implement state machines as sub-classes of the `SM` class, which specifies the `start_state`, `transition_fn` and `output_fn`.\n",
    "\n",
    "```\n",
    "class SM:\n",
    "    start_state = None  # default start state\n",
    "    def transition_fn(self, s, i):\n",
    "        '''s:       the current state\n",
    "           i:       the given input\n",
    "           returns: the next state'''\n",
    "        raise NotImplementedError\n",
    "    def output_fn(self, s):\n",
    "        '''s:       the current state\n",
    "           returns: the corresponding output'''\n",
    "        raise NotImplementedError\n",
    "```\n",
    "\n",
    "An example of a sub-class is the `Accumulator` state machine, which adds up (accumulates) its input and outputs the sum. Convince yourself that the implementation works as expected before moving on.\n",
    "\n",
    "```\n",
    "class Accumulator(SM):\n",
    "    start_state = 0\n",
    "    def transition_fn(self, s, i):\n",
    "        return s + i\n",
    "    def output_fn(self, s):\n",
    "        return s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYF-u59B861-"
   },
   "source": [
    "### 1.1 Transduce\n",
    "Implement the `transduce` method for the `SM` class. It is given an input sequence (a list) and returns an output sequence (a list) of the outputs of the state machine on the input sequence. Assume `self.transition_fn` and `self.output_fn` are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Xy42nJa69D3i"
   },
   "outputs": [],
   "source": [
    "class SM:\n",
    "    start_state = None\n",
    "\n",
    "    def transduce(self, input_seq):\n",
    "        '''input_seq: a list of inputs to feed into SM\n",
    "           returns:   a list of outputs of SM'''\n",
    "        output_seq = []\n",
    "        s = self.start_state\n",
    "        for x in input_seq:\n",
    "            s = self.transition_fn(s, x)\n",
    "            output_seq += [self.output_fn(s)]\n",
    "        return output_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kzdkh0p8AGi_"
   },
   "source": [
    "Below is the `Accumulator` state machine implementation that you saw above as well as an unit test to help test your `SM` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "qmRnua5p_U9j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans s 0 s_t_1 -1\n",
      "out s_t -1\n",
      "trans s -1 s_t_1 2\n",
      "out s_t 1\n",
      "trans s 1 s_t_1 3\n",
      "out s_t 4\n",
      "trans s 4 s_t_1 -2\n",
      "out s_t 2\n",
      "trans s 2 s_t_1 5\n",
      "out s_t 7\n",
      "trans s 7 s_t_1 6\n",
      "out s_t 13\n",
      "res [-1, 1, 4, 2, 7, 13]\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "class Accumulator(SM):\n",
    "    start_state = 0\n",
    "\n",
    "    def transition_fn(self, s, i):\n",
    "        print('trans s', s, 's_t_1', i)\n",
    "        return s + i\n",
    "\n",
    "    def output_fn(self, s):\n",
    "        print('out s_t', s)\n",
    "        return s\n",
    "    \n",
    "def test_accumulator_sm():\n",
    "    res = Accumulator().transduce([-1, 2, 3, -2, 5, 6])\n",
    "    print('res', res)\n",
    "    assert(res == [-1, 1, 4, 2, 7, 13])\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Unit test\n",
    "test_accumulator_sm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-QW8TSk9T1E"
   },
   "source": [
    "### 1.2 Binary Addition\n",
    "Implement a `Binary_Addition` state machine that takes in a sequence of pairs of binary digits (0,1) representing two reversed binary numbers and returns a sequence of digits representing the reversed sum. For instance, to sum two binary numbers `100` and `011`, the input sequence will be `[(0, 1), (0, 1), (1, 0)]`. You will need to define `start_state`, `transition_fn` and `output_fn`. Note that when transduced, the input sequence may need to be extended with an extra (0,0) to output the final carry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "JKcWyGrZ9mEj"
   },
   "outputs": [],
   "source": [
    "class Binary_Addition(SM):\n",
    "    start_state = (0, 0) # Change\n",
    "\n",
    "    def transition_fn(self, s, x):\n",
    "        # Your code here\n",
    "        carry_bit = s[1]\n",
    "        a_bit, b_bit = x\n",
    "        res = (a_bit + b_bit + carry_bit) % 2\n",
    "        carry_bit = ((a_bit and carry_bit) or (b_bit and carry_bit) or (a_bit and b_bit))\n",
    "        return (res, carry_bit)\n",
    "        \n",
    "    def output_fn(self, s):\n",
    "        # Your code here\n",
    "        bit, carry = s\n",
    "        return bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "5hvOZXkcA0Au"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1]\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_binary_addition_sm():\n",
    "    res = Binary_Addition().transduce([(1, 1), (1, 0), (0, 0)])\n",
    "    print(res)\n",
    "    assert(res == [0, 0, 1])\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Unit test\n",
    "test_binary_addition_sm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtIAZJN79s0h"
   },
   "source": [
    "### 1.3 Reverser\n",
    "Implement a state machine that reverses a sequence. The input is a list of the form:\n",
    "\n",
    "```\n",
    " sequence1 + ['end'] + sequence2\n",
    " ```\n",
    " \n",
    "`+` refers to concatenation. `sequence1` is a list of strings, the `'end'` string indicates termination, and `sequence2` is arbitrary. The machine reverses `sequence1`: for each entry in the `sequence1`, the machine outputs `None`. For the `'end'` input and each entry in the second sequence, an item from the reversed `sequence1` is output, or `None` if no characters remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "VtsUESbg9wAS"
   },
   "outputs": [],
   "source": [
    "class Reverser(SM):\n",
    "    start_state = (False, [])\n",
    "\n",
    "    def transition_fn(self, s, x):\n",
    "        # Your code here\n",
    "        reverse, strings = s\n",
    "        if x == 'end':\n",
    "            return (True, strings)\n",
    "        elif reverse:\n",
    "            return (True, strings[1:])\n",
    "        else:\n",
    "            return (False, [x] + strings)\n",
    "\n",
    "    def output_fn(self, s):\n",
    "        # Your code here\n",
    "        reverse, strings = s\n",
    "        if reverse and len(strings) > 0:\n",
    "            return strings[0]\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "XchT3a-fA9oM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res [None, None, None, 'bar', ' ', 'foo', None, None, None]\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_reverser_sm():\n",
    "    res = Reverser().transduce(['foo', ' ', 'bar'] + ['end'] + list(range(5)))\n",
    "    print('res', res)\n",
    "    assert(res == [None, None, None, 'bar', ' ', 'foo', None, None, None])\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Unit test\n",
    "test_reverser_sm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmewFWqx_4ep"
   },
   "source": [
    "### 1.4 RNN\n",
    "An RNN has a transition function and an output function, each of which is defined in terms of weight matrices, offset vectors and activation functions, analogously to standard neural networks.\n",
    "\n",
    "* The inputs $x$ are $l\\times1$ vectors\n",
    "* The states $s$ are $m\\times1$ vectors\n",
    "* The outputs $y$ are $n\\times1$ vectors\n",
    "\n",
    "The behavior is defined as follows:\n",
    "$$\\begin{align*} s_{t} & = f_1(W^{ss} s_{{t-1}} + W^{sx} x_{t} + W^{ss}_0) \\\\ y_{t} & = f_2(W^o s_{t} + W^o_0) \\end{align*}$$\n",
    "\n",
    "where $f_1$ and $f_2$ are two activation functions, such as linear, softmax or tanh.\n",
    "\n",
    "\n",
    "Note that each input `i` below has dimension `l x 1`. Implement the corresponding state machine, where the weights are given in `__init__`. Make sure to set an appropriate `start_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "TcuRs5y0A4n-"
   },
   "outputs": [],
   "source": [
    "class RNN(SM):\n",
    "    def __init__(self, Wsx, Wss, Wo, Wss_0, Wo_0, f1, f2):\n",
    "        # Your code here\n",
    "        self.Wsx = Wsx\n",
    "        self.Wss = Wss\n",
    "        self.Wo = Wo\n",
    "        self.Wss_0 = Wss_0\n",
    "        self.Wo_0 = Wo_0\n",
    "        self.f1 = f1\n",
    "        self.f2 = f2\n",
    "        self.m, _ = Wss.shape\n",
    "        self.start_state = np.zeros((self.m, 1))\n",
    "\n",
    "    def transition_fn(self, s, i):\n",
    "        # Your code here\n",
    "        return self.f1(self.Wss @ s + self.Wsx @ i + self.Wss_0)\n",
    "\n",
    "    def output_fn(self, s):\n",
    "        # Your code here\n",
    "        return self.f2(self.Wo @ s + self.Wo_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "DcckX5R1JWII"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    v = np.exp(z)\n",
    "    return v / np.sum(v, axis = 0)\n",
    "\n",
    "def test_rnn():\n",
    "    Wsx1 = np.array([[0.1],\n",
    "                     [0.3],\n",
    "                     [0.5]])\n",
    "    Wss1 = np.array([[0.1,0.2,0.3],\n",
    "                     [0.4,0.5,0.6],\n",
    "                     [0.7,0.8,0.9]])\n",
    "    Wo1 = np.array([[0.1,0.2,0.3],\n",
    "                    [0.4,0.5,0.6]])\n",
    "    Wss1_0 = np.array([[0.01],\n",
    "                       [0.02],\n",
    "                       [0.03]])\n",
    "    Wo1_0 = np.array([[0.1],\n",
    "                      [0.2]])\n",
    "    in1 = [np.array([[0.1]]),\n",
    "           np.array([[0.3]]),\n",
    "           np.array([[0.5]])]\n",
    "    \n",
    "    rnn = RNN(Wsx1, Wss1, Wo1, Wss1_0, Wo1_0, np.tanh, softmax)\n",
    "    expected = np.array([[[0.4638293846951024], [0.5361706153048975]],\n",
    "                        [[0.4333239107898491], [0.566676089210151]],\n",
    "                        [[0.3821688606165438], [0.6178311393834561]]])\n",
    "\n",
    "    assert(np.allclose(expected, rnn.transduce(in1)))\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Unit test\n",
    "test_rnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gRiDImvBrTF"
   },
   "source": [
    "### 1.5 Accumulator Sign RNN\n",
    "Enter the parameter matrices and vectors for an instance of the `RNN` class such that the output is `1` if the cumulative sum of the inputs is positive, `-1` if the cumulative sum is negative and `0` if otherwise. Make sure that you scale the outputs so that the output activation values are very close to `1`, `0` and `-1`. Note that both the inputs and outputs are `1 x 1`.\n",
    "\n",
    "Hint: `np.tanh` may be useful. Remember to convert your Python lists to `np.array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "PhH9pv3GBvam"
   },
   "outputs": [],
   "source": [
    "Wsx = np.ones((1, 1))\n",
    "Wss = np.ones((1, 1))\n",
    "Wo = np.ones((1, 1))\n",
    "Wss_0 = np.zeros((1, 1))\n",
    "Wo_0 = np.zeros((1, 1))\n",
    "f1 = lambda x: x\n",
    "f2 = np.sign\n",
    "acc_sign = RNN(Wsx, Wss, Wo, Wss_0, Wo_0, f1, f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "78ug9-PLJk82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res [array([[-1.]]), array([[-1.]]), array([[-1.]]), array([[1.]]), array([[-1.]]), array([[0.]])]\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_acc_sign_rnn(acc_sign_rnn):\n",
    "    res = acc_sign_rnn.transduce(list(map(lambda x: np.array([[x]]), [-1, -2, 2, 3, -3, 1])))\n",
    "    print('res', res)\n",
    "    expected = np.array([[[-1.0]], [[-1.0]], [[-1.0]], [[1.0]], [[-1.0]], [[0.0]]])\n",
    "    assert(np.allclose(expected, res))\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Unit test\n",
    "test_acc_sign_rnn(acc_sign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J01wlpQRCKyo"
   },
   "source": [
    "### 1.6 Autoregression RNN\n",
    "\n",
    "Enter the parameter matrices and vectors for an instance of the `RNN` class such that it implements the following autoregressive model:\n",
    "$$y_t=y_{t-1} - 2y_{t-2} + 3y_{t-3}$$\n",
    "when $x_t = y_{t-1}$. Note that both the inputs and outputs are `1 x 1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "C744ijBCCOm5"
   },
   "outputs": [],
   "source": [
    "Wsx = np.array([[1, 0, 0]]).reshape((3, 1))\n",
    "Wss = np.array([[0, 0, 0], [1, 0, 0], [0, 1, 0]])\n",
    "Wo = np.array([[1, -2, 3]])\n",
    "Wss_0 = np.zeros((3, 1))\n",
    "Wo_0 = np.zeros((1, 1))\n",
    "f1 = lambda x: x\n",
    "f2 = lambda x: x\n",
    "auto = RNN(Wsx, Wss, Wo, Wss_0, Wo_0, f1, f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "id": "4RrDCow1J-M8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_auto_rnn(auto_rnn):\n",
    "    res = auto_rnn.transduce([np.array([[x]]) for x in range(-2,5)])\n",
    "    expected = np.array([[[-2.0]], [[3.0]], [[-4.0]], [[-2.0]], [[0.0]], [[2.0]], [[4.0]]])\n",
    "    assert(np.allclose(expected, res))\n",
    "    print(\"Test passed!\")\n",
    "    \n",
    "# Unit test\n",
    "test_auto_rnn(auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) MDP\n",
    "We consider the same tiny MDP as in the exercises, with states (0, 1, 2, 3) and actions ('b', 'c'). The reward function is:\n",
    "\n",
    "$$\n",
    "R(s,a) = \\begin{cases}\n",
    "        1, & \\text{if } s = 1 \\\\\n",
    "        2, & \\text{if } s = 3 \\\\\n",
    "        0, & \\text{otherwise}\n",
    "       \\end{cases}\n",
    "$$\n",
    "\n",
    "You get the reward associated with a state on the step when you exit that state. The transition function for each action is below, where $T[i,x,j]$ is the $P(s_{t+1} = j | a = x, s_t = i)$ (note rows correspond to the input states, and columns correspond to the output states).\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "T(s_t, \\text{'b'}, s_{t+1}) =\n",
    "    \\begin{bmatrix}\n",
    "        0.0 & 0.9 & 0.1 & 0.0 \\\\\n",
    "        0.9 & 0.1 & 0.0 & 0.0 \\\\\n",
    "        0.0 & 0.0 & 0.1 & 0.9 \\\\\n",
    "        0.9 & 0.0 & 0.0 & 0.1 \\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "T(s_t, \\text{'c'}, s_{t+1}) = \n",
    "    \\begin{bmatrix}\n",
    "        0.0 & 0.1 & 0.9 & 0.0 \\\\\n",
    "        0.9 & 0.1 & 0.0 & 0.0 \\\\\n",
    "        0.0 & 0.0 & 0.1 & 0.9 \\\\\n",
    "        0.9 & 0.0 & 0.0 & 0.1 \\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    " \n",
    "Note that the only effect of the action is to change the transition probability from state 0.\n",
    "\n",
    "# 2.1) Limited horizons\n",
    "Consider two policies: one that always takes action 'b' in state 0 and one that always takes action 'c'.\n",
    "\n",
    "## 2.1.A)\n",
    "Which policy is best if you're starting from state 0 with horizon 2?\n",
    "\n",
    "Horizon 2 values from taking action 'b':\n",
    "```\n",
    "[[0.9],\n",
    "[1.1],\n",
    "[1.8],\n",
    "[2.2]]\n",
    "```\n",
    "\n",
    "Horizon 2 values from taking action 'c':\n",
    "```\n",
    "[[0.1],\n",
    "[1.1],\n",
    "[1.8],\n",
    "[2.2]]\n",
    "```\n",
    "Since $0.9 \\gt 0.1$, taking action 'b' is the better policy.\n",
    "\n",
    "## 2.1.B)\n",
    "Which policy is best if you're starting from state 0 with horizon 3?\n",
    "\n",
    "Horizon 3 values from taking action 'b':\n",
    "```\n",
    "[[1.17],\n",
    "[1.92],\n",
    "[2.16],\n",
    "[3.03]]\n",
    "```\n",
    "\n",
    "Horizon 3 values from taking action 'c':\n",
    "```\n",
    "[[1.73],\n",
    "[1.2 ],\n",
    "[2.16],\n",
    "[2.31]]\n",
    "```\n",
    "Since $1.73 \\gt 1.17$, taking action 'c' is the better policy.\n",
    "\n",
    "## 2.1.C)\n",
    "What if we start in state 0 with horizon 5, take action 'c', land in state 2 with horizon 4, land in state 3 with horizon 3 (and get reward = 2!), and then land in state 0 with horizon 2? What action should we take now?\n",
    "\n",
    "Same as 2.1.A, action 'b'.\n",
    "\n",
    "# 2.2) At a discount\n",
    "Now, in the same tiny MDP, we are interested in the infinite-horizon discounted value, with discount factor 0.9. For a given policy, we can write down a set of linear equations characterizing these values, in the form:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    v_0 &= r_0 + c_{00}v_0 + c_{01}v_1 + c_{02}v_2 + c_{03}v_3 \\\\\n",
    "    v_1 &= r_1 + c_{10}v_0 + c_{11}v_1 + c_{12}v_2 + c_{13}v_3 \\\\\n",
    "    v_2 &= r_2 + c_{20}v_0 + c_{21}v_1 + c_{22}v_2 + c_{23}v_3 \\\\\n",
    "    v_3 &= r_3 + c_{30}v_0 + c_{31}v_1 + c_{32}v_2 + c_{33}v_3 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## 2.2.A)\n",
    "Enter the vector of $r_i$ values, in the order shown above. Enter the matrix as a list of 4 numbers:\n",
    "\n",
    "$[0, 1, 0, 2]$\n",
    "\n",
    "## 2.2.B)\n",
    "Enter the matrix of $c_{ij}$ values, in the order shown above, for the policy \"always take action 'c'\".\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    0.0 & 0.09 & 0.81 & 0.0 \\\\\n",
    "    0.81 & 0.09 & 0.0 & 0.0 \\\\\n",
    "    0.0 & 0.0 & 0.09 & 0.81 \\\\\n",
    "    0.81 & 0.0 & 0.0 & 0.09 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## 2.2.C)\n",
    "Solve for the value function under this policy by using numpy.linalg.solve to solve these equations. For example, if you have a set of linear equations of the form:\n",
    "\n",
    "```\n",
    "1 v0 + 2  v1 + 3  v2 = 4\n",
    "5 v0 + 6  v1 + 7  v2 = 8\n",
    "9 v0 + 10 v1 + 11 v2 = 12\n",
    "```\n",
    "Then you can solve as follows:\n",
    "```\n",
    "A = np.matrix([[1, 2, 3], [5, 6, 7], [9, 10, 11]])\n",
    "b = np.matrix([[4], [8], [12]])\n",
    "v = np.linalg.solve(A,b)  # A v = b\n",
    "```\n",
    "\n",
    "Since we know what the rewards are,$[0, 1, 0, 2]$, we will rearrange these equations and solve for $v$:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    v_0 &= r_0 + c_{00}v_0 + c_{01}v_1 + c_{02}v_2 + c_{03}v_3 \\\\\n",
    "    v_1 &= r_1 + c_{10}v_0 + c_{11}v_1 + c_{12}v_2 + c_{13}v_3 \\\\\n",
    "    v_2 &= r_2 + c_{20}v_0 + c_{21}v_1 + c_{22}v_2 + c_{23}v_3 \\\\\n",
    "    v_3 &= r_3 + c_{30}v_0 + c_{31}v_1 + c_{32}v_2 + c_{33}v_3 \\\\\n",
    "\\end{aligned}\n",
    "\\qquad\\Rightarrow\\qquad\n",
    "\\begin{aligned}\n",
    "    r_0 &= v_0 - c_{00}v_0 - c_{01}v_1 - c_{02}v_2 - c_{03}v_3 \\\\\n",
    "    r_1 &= v_1 - c_{10}v_0 - c_{11}v_1 - c_{12}v_2 - c_{13}v_3 \\\\\n",
    "    r_2 &= v_2 - c_{20}v_0 - c_{21}v_1 - c_{22}v_2 - c_{23}v_3 \\\\\n",
    "    r_3 &= v_3 - c_{30}v_0 - c_{31}v_1 - c_{32}v_2 - c_{33}v_3 \\\\\n",
    "\\end{aligned}\n",
    "\\qquad\\Rightarrow\\qquad\n",
    "\\begin{aligned}\n",
    "    r_0 &= v_0(1 - c_{00}) - c_{01}v_1 - c_{02}v_2 - c_{03}v_3 \\\\\n",
    "    r_1 &= - c_{10}v_0 + v_1(1 - c_{11}) - c_{12}v_2 - c_{13}v_3 \\\\\n",
    "    r_2 &= - c_{20}v_0 - c_{21}v_1 + v_2(1 - c_{22}) - c_{23}v_3 \\\\\n",
    "    r_3 &= - c_{30}v_0 - c_{31}v_1 - c_{32}v_2 + v_3(1 - c_{33}) \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.053, 6.487, 6.752, 7.586]])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_transition_matrix = np.array([[0.0, 0.9, 0.1, 0.0], [0.9, 0.1, 0.0, 0.0], [0.0, 0.0, 0.1, 0.9], [0.9, 0.0, 0.0, 0.1]])\n",
    "c_transition_matrix = np.array([[0.0, 0.1, 0.9, 0.0], [0.9, 0.1, 0.0, 0.0], [0.0, 0.0, 0.1, 0.9], [0.9, 0.0, 0.0, 0.1]])\n",
    "# print('b_transition_matrix', b_transition_matrix)\n",
    "# print('c_transition_matrix', c_transition_matrix)\n",
    "reward = np.array([[0, 1, 0, 2]]).reshape((4, 1))\n",
    "# horizon -> horizon values vector\n",
    "memo = {0 : np.zeros((4, 1))}\n",
    "\n",
    "def compute_horizon_values(horizon, transition_matrix, reward, memo):\n",
    "    for h in range(1, horizon+1):\n",
    "        v_h = reward + transition_matrix @ memo[h-1]\n",
    "        memo[h] = v_h\n",
    "    return memo\n",
    "\n",
    "#2.1.A)\n",
    "# print(compute_horizon_values(2, b_transition_matrix, reward, memo.copy()))\n",
    "# print(compute_horizon_values(2, c_transition_matrix, reward, memo.copy()))\n",
    "\n",
    "#2.1.B)\n",
    "# print(compute_horizon_values(3, b_transition_matrix, reward, memo.copy()))\n",
    "# print(compute_horizon_values(3, c_transition_matrix, reward, memo.copy()))\n",
    "\n",
    "#2.1.C)\n",
    "# print(compute_horizon_values(5, b_transition_matrix, reward, memo.copy()))\n",
    "# print(compute_horizon_values(5, c_transition_matrix, reward, memo.copy()))\n",
    "\n",
    "#2.2.C\n",
    "discount = 0.9\n",
    "A = np.eye(c_transition_matrix.shape[0]) - discount*c_transition_matrix\n",
    "# print(A)\n",
    "b = reward\n",
    "v = np.linalg.solve(A, b)\n",
    "np.round(v, 3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Tiny Q-value iteration\n",
    "__In the same example as above__, with an infinite horizon and a discount factor of 0.9, compute three iterations of value iteration. Don't assume a particular policy. Assume that:\n",
    "\n",
    "- All the value estimates start at 0 (meaning, at iteration 0, $Q(s,a)=0$ for all $s,a$ pairs), and\n",
    "- You operate synchronously (that is, on iteration $t$ of value iteration, you only use values that were computed on iteration $t−1$).\n",
    "We recommend you compute the Q-value iteration by hand to get a better understanding of the algorithm.\n",
    "For each iteration enter 8 numbers corresponding to:\n",
    "\n",
    "$$[Q(0,b),Q(0,c),Q(1,b),Q(1,c),Q(2,b),Q(2,c),Q(3,b),Q(3,c)]$$\n",
    "\n",
    "at that iteration, accurate to at least 3 decimal places.\n",
    "\n",
    "## 3.A) Iteration 1\n",
    "\n",
    "$[0, 0, 1, 1, 0, 0, 2, 2]$\n",
    "\n",
    "## 3.B) Iteration 2\n",
    "\n",
    "I got different answers than the online grader. I strongly believe the solutions of the grader are incorrect. Online grader solution at time of writing: $[0.81, 0.09, 1.09, 1.09, 1.62, 1.62, 2.18, 2.18]$. My solution $[0.81, 0.81 ,1.09, 1.09, 1.62, 1.62, 2.18, 2.18]$\n",
    "\n",
    "## 3.C) Iteration 3\n",
    "\n",
    "I got different answers than the online grader. I strongly believe the solutions of the grader are incorrect. Online grader solution at time of writing: $[1.0287, 1.4103, 1.7542, 1.7542, 1.9116, 1.9116, 2.8523, 2.8523]$. My solution $[1.4103, 1.4103, 1.7542, 1.7542, 1.9116, 1.9116, 2.8523, 2.8523]$\n",
    "\n",
    "## 3.D)\n",
    "After the third iteration, what action would you select in state 0?\n",
    "\n",
    "'c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 1: array([[0.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [2.]]), 2: array([[0.81],\n",
      "       [1.09],\n",
      "       [1.62],\n",
      "       [2.18]]), 3: array([[1.0287],\n",
      "       [1.7542],\n",
      "       [1.9116],\n",
      "       [2.8523]])}\n",
      "{0: array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 1: array([[0.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [2.]]), 2: array([[0.09],\n",
      "       [1.09],\n",
      "       [1.62],\n",
      "       [2.18]]), 3: array([[1.4103],\n",
      "       [1.171 ],\n",
      "       [1.9116],\n",
      "       [2.2691]])}\n",
      "{0: {'b': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'c': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])}, 1: {'b': array([[0.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [2.]]), 'c': array([[0.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [2.]])}, 2: {'b': array([[0.81],\n",
      "       [1.09],\n",
      "       [1.62],\n",
      "       [2.18]]), 'c': array([[0.81],\n",
      "       [1.09],\n",
      "       [1.62],\n",
      "       [2.18]])}, 3: {'b': array([[1.4103],\n",
      "       [1.7542],\n",
      "       [1.9116],\n",
      "       [2.8523]]), 'c': array([[1.4103],\n",
      "       [1.7542],\n",
      "       [1.9116],\n",
      "       [2.8523]])}}\n"
     ]
    }
   ],
   "source": [
    "b_transition_matrix = np.array([[0.0, 0.9, 0.1, 0.0], [0.9, 0.1, 0.0, 0.0], [0.0, 0.0, 0.1, 0.9], [0.9, 0.0, 0.0, 0.1]])\n",
    "c_transition_matrix = np.array([[0.0, 0.1, 0.9, 0.0], [0.9, 0.1, 0.0, 0.0], [0.0, 0.0, 0.1, 0.9], [0.9, 0.0, 0.0, 0.1]])\n",
    "reward = np.array([[0, 1, 0, 2]]).reshape((4, 1))\n",
    "discount = 0.9\n",
    "\n",
    "# print(discount*b_transition_matrix)\n",
    "# print(discount*c_transition_matrix)\n",
    "\n",
    "def value_iteration(horizon, b_transition_matrix, c_transition_matrix, reward, memo):\n",
    "    for h in range(1, horizon+1):\n",
    "        v_h_b = reward + (b_transition_matrix @ memo[h-1]['b'])\n",
    "        v_h_c = reward + (c_transition_matrix @ memo[h-1]['c'])\n",
    "        all_actions_v = np.concatenate((v_h_b, v_h_c), axis=1)\n",
    "        best_action_v = np.max(all_actions_v, axis=1, keepdims=True)\n",
    "        memo[h] = {}\n",
    "        memo[h]['b'] = best_action_v\n",
    "        memo[h]['c'] = best_action_v\n",
    "    return memo\n",
    "# 3.A-3.D\n",
    "print(compute_horizon_values(3, discount*b_transition_matrix, reward, memo.copy()))\n",
    "print(compute_horizon_values(3, discount*c_transition_matrix, reward, memo.copy()))\n",
    "# horizon -> action -> horizon value vectors\n",
    "memo = {0 : {'b' : np.zeros((4, 1)), 'c' : np.zeros((4, 1))}}\n",
    "print(value_iteration(3, discount*b_transition_matrix, discount*c_transition_matrix, reward, memo.copy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa8rXc0qDvkh"
   },
   "source": [
    "## 5) MDP Implementations\n",
    "\n",
    "We'll be using a couple of simple classes to represent MDPs and probability distributions.\n",
    "\n",
    "### 5.1 Working with MDPs\n",
    "\n",
    "Recall that given a $Q_\\pi$ for any policy $\\pi$, then $V_\\pi(s)$ = $\\max_a Q_\\pi(s, a)$.\n",
    "\n",
    "1. Write the `value` method, which takes a $Q$ function (an instance of `TabularQ`) and a state and returns the value `V` of an action that maximizes $Q$ function stored in `q`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "id": "i9bNukNug53m"
   },
   "outputs": [],
   "source": [
    "def value(q, s):\n",
    "    \"\"\" Return Q*(s,a) based on current Q\n",
    "\n",
    "    >>> q = TabularQ([0,1,2,3],['b','c'])\n",
    "    >>> q.set(0, 'b', 5)\n",
    "    >>> q.set(0, 'c', 10)\n",
    "    >>> q_star = value(q,0)\n",
    "    >>> q_star\n",
    "    10\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    return max(q.get(s, a) for a in q.actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "id": "aUaRY8RtOQv0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_value():\n",
    "    q = TabularQ([0,1,2,3], ['b','c'])\n",
    "    q.set(0, 'b', 5)\n",
    "    q.set(0, 'c', 10)\n",
    "    assert(value(q, 0) == 10)\n",
    "    print(\"Test passed!\")\n",
    "    \n",
    "test_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAkLsRdMhj5U"
   },
   "source": [
    "2. Write the `greedy` method, which takes a $Q$ function (an instance of `TabularQ`) and a state and returns the action `a` determined by the policy that acts greedily with respect to the current value of `q`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "id": "o-0-YCiVhrq6"
   },
   "outputs": [],
   "source": [
    "def greedy(q, s):\n",
    "    \"\"\" Return pi*(s) based on a greedy strategy.\n",
    "\n",
    "    >>> q = TabularQ([0,1,2,3],['b','c'])\n",
    "    >>> q.set(0, 'b', 5)\n",
    "    >>> q.set(0, 'c', 10)\n",
    "    >>> q.set(1, 'b', 2)\n",
    "    >>> greedy(q, 0)\n",
    "    'c'\n",
    "    >>> greedy(q, 1)\n",
    "    'b'\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    return argmax(q.actions, lambda a: q.get(s, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "id": "X5SlyiDuOb4n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_greedy():\n",
    "    q = TabularQ([0, 1, 2, 3],['b', 'c'])\n",
    "    q.set(0, 'b', 5)\n",
    "    q.set(0, 'c', 10)\n",
    "    q.set(1, 'b', 2)\n",
    "    assert(greedy(q, 0) == 'c')\n",
    "    assert(greedy(q, 1) == 'b')\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "test_greedy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EM4maWSahr-F"
   },
   "source": [
    "3. Write the `epsilon_greedy` method, which takes a state `s` and a parameter `epsilon`, and returns an action. With probability `1 - epsilon` it should select the greedy action and with probability `epsilon` it should select an action uniformly from the set of possible actions.\n",
    "\n",
    "    - You should use `random.random()` to generate a random number to test againts eps.\n",
    "    - You should use the `draw` method of `uniform_dist` to generate a random action.\n",
    "    - You can use the `greedy` function defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "id": "bTNz9DmwiEpJ"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(q, s, eps = 0.5):\n",
    "    \"\"\" Returns an action.\n",
    "\n",
    "    >>> q = TabularQ([0,1,2,3],['b','c'])\n",
    "    >>> q.set(0, 'b', 5)\n",
    "    >>> q.set(0, 'c', 10)\n",
    "    >>> q.set(1, 'b', 2)\n",
    "    >>> eps = 0.\n",
    "    >>> epsilon_greedy(q, 0, eps) #greedy\n",
    "    'c'\n",
    "    >>> epsilon_greedy(q, 1, eps) #greedy\n",
    "    'b'\n",
    "    \"\"\"\n",
    "    if random.random() < eps:  # True with prob eps, random action\n",
    "        # Your code here\n",
    "        return uniform_dist(q.actions).draw()\n",
    "    else:\n",
    "        # Your code here\n",
    "        return greedy(q, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "id": "jXjStECQOqiR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_epsilon_greedy():\n",
    "    q = TabularQ([0, 1, 2, 3],['b', 'c'])\n",
    "    q.set(0, 'b', 5)\n",
    "    q.set(0, 'c', 10)\n",
    "    q.set(1, 'b', 2)\n",
    "    eps = 0.0\n",
    "    assert(epsilon_greedy(q, 0, eps) == 'c')\n",
    "    assert(epsilon_greedy(q, 1, eps) == 'b')\n",
    "    print(\"Test passed!\")\n",
    "    \n",
    "test_epsilon_greedy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0FimoqLiJQ_"
   },
   "source": [
    "### 5.2 Implement Q-Value Iteration\n",
    "Provide the definition of the `value_iteration` function. It takes an MDP instance and a `TabularQ` instance. It should terminate when\n",
    "\n",
    "$$\\max_{(s, a)}\\left|Q_t(s, a) - Q_{t-1}(s, a)\\right| < \\epsilon$$\n",
    "\n",
    "that is, the biggest difference between the value functions on successive iterations is less than input parameter `eps`. This function should return the final `TabularQ` instance. It should do no more that `max_iters` iterations.\n",
    "\n",
    "* Make sure to copy the Q function between iterations, e.g. `new_q = q.copy()`.\n",
    "* The `q` parameter contains the initialization of the Q function.\n",
    "* The `value` function is already defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "id": "WvvczSHijES5"
   },
   "outputs": [],
   "source": [
    "def value_iteration(mdp, q, eps=0.01, max_iters=1000):\n",
    "    # Your code here\n",
    "    Q_old = q.copy()\n",
    "    for i in range(max_iters):\n",
    "        Q_new = Q_old.copy()\n",
    "        diff = 0.0\n",
    "        for state in mdp.states:\n",
    "            for action in mdp.actions:\n",
    "                transition_dist = mdp.transition_model(state, action)\n",
    "                expected_val = transition_dist.expectation(lambda s: value(Q_old, s))\n",
    "                Q_val = mdp.reward_fn(state, action) + mdp.discount_factor*expected_val\n",
    "                Q_new.set(state, action, Q_val)\n",
    "                diff = max(diff, abs(Q_old.get(state, action) - Q_new.get(state, action)))\n",
    "        if diff < eps:\n",
    "            return Q_new\n",
    "        Q_old = Q_new.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBYHRzz-_Q_-"
   },
   "source": [
    "Below is the implementation of the \"tiny\" MDP detailed in Problem 2 and Problem 5.3. We will be using it to test `value_iteration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "id": "pRO8Zf47_Qm0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=(0, 'b'), expected=5.072814297918393, got=5.072814297918393\n",
      "k=(0, 'c'), expected=5.262109602844769, got=5.262109602844769\n",
      "k=(1, 'b'), expected=5.6957634856549095, got=5.6957634856549095\n",
      "k=(1, 'c'), expected=5.6957634856549095, got=5.6957634856549095\n",
      "k=(2, 'b'), expected=5.962924188028282, got=5.962924188028282\n",
      "k=(2, 'c'), expected=5.962924188028282, got=5.962924188028282\n",
      "k=(3, 'b'), expected=6.794664584556008, got=6.794664584556008\n",
      "k=(3, 'c'), expected=6.794664584556008, got=6.794664584556008\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def tiny_reward(s, a):\n",
    "    # Reward function\n",
    "    if s == 1: return 1\n",
    "    elif s == 3: return 2\n",
    "    else: return 0\n",
    "\n",
    "def tiny_transition(s, a):\n",
    "    # Transition function\n",
    "    if s == 0:\n",
    "        if a == 'b':\n",
    "            return DDist({1 : 0.9, 2 : 0.1})\n",
    "        else:\n",
    "            return DDist({1 : 0.1, 2 : 0.9})\n",
    "    elif s == 1:\n",
    "        return DDist({1 : 0.1, 0 : 0.9})\n",
    "    elif s == 2:\n",
    "        return DDist({2 : 0.1, 3 : 0.9})\n",
    "    elif s == 3:\n",
    "        return DDist({3 : 0.1, 0 : 0.9})\n",
    "    \n",
    "def test_value_iteration():\n",
    "    tiny = MDP([0, 1, 2, 3], ['b', 'c'], tiny_transition, tiny_reward, 0.9)\n",
    "    q = TabularQ(tiny.states, tiny.actions)\n",
    "    qvi = value_iteration(tiny, q, eps=0.1, max_iters=100)\n",
    "    expected = dict([((2, 'b'), 5.962924188028282),\n",
    "                     ((1, 'c'), 5.6957634856549095),\n",
    "                     ((1, 'b'), 5.6957634856549095),\n",
    "                     ((0, 'b'), 5.072814297918393),\n",
    "                     ((0, 'c'), 5.262109602844769),\n",
    "                     ((3, 'b'), 6.794664584556008),\n",
    "                     ((3, 'c'), 6.794664584556008),\n",
    "                     ((2, 'c'), 5.962924188028282)])\n",
    "    for k in qvi.q:\n",
    "        print(\"k=%s, expected=%s, got=%s\" % (k, expected[k], qvi.q[k]))      \n",
    "        assert(abs(qvi.q[k] - expected[k]) < 1.0e-5)\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "test_value_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpxPhBqijE6e"
   },
   "source": [
    "### 5.3 Receding-horizon control and online search\n",
    "Value iteration depends on the dynamic programming principle to efficiently solve for a value function over all the states in the domain, which allows very fast determination of the optimal action for each state. In some cases, the state space is huge, but the set of states that are reachable in a reasonable number of steps from the current state is relatively small. In that case, it's actually more efficient to select actions by performing an \"expectimax\" tree search from the current state to find the best action, taking the action, seeing what state results, and performing another tree search.\n",
    "\n",
    "We will need to fix the horizon for the tree search; it is possible to choose a finite horizon that will guarantee bounded \"regret\" (optimal value minus value actually obtained) based on the discount factor, but for this problem we will just assume we are given horizon h.\n",
    "\n",
    "Consider a function $q_{em}(s,a,h)$ that computes the horizon-$h$ $Q$ value for state $s$ and action $a$ by using the definition of the finite-horizon Q function in the notes (__but including a discount factor__). This is sometimes called \"expectimax\" search, because you can think of it as building a search tree, in which some nodes are evaluated by taking an expectation (we take an expectation over possible outcomes of an action in a state) and some nodes are evaluated by taking a maximum (we take a maximum over the Q values for the possible actions in a state).\n",
    "\n",
    "Consider again our \"tiny\" MDP as in Section 2 with states (0, 1, 2, 3) and actions ('b', 'c'). Assume no discounting (i.e., discount factor = 1.0). The reward function is:\n",
    "$$\n",
    "R(s,a) = \\begin{cases}\n",
    "        1, & \\text{if } s = 1 \\\\\n",
    "        2, & \\text{if } s = 3 \\\\\n",
    "        0, & \\text{otherwise}\n",
    "       \\end{cases}\n",
    "$$\n",
    "\n",
    "You get the reward associated with a state on the step when you exit that state. The transition function for each action is below, where $T[i,x,j]$ is the $P(s_{t+1} = j | a = x, s_t = i)$ (note rows correspond to the input states, and columns correspond to the output states).\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "T(s_t, \\text{'b'}, s_{t+1}) =\n",
    "    \\begin{bmatrix}\n",
    "        0.0 & 0.9 & 0.1 & 0.0 \\\\\n",
    "        0.9 & 0.1 & 0.0 & 0.0 \\\\\n",
    "        0.0 & 0.0 & 0.1 & 0.9 \\\\\n",
    "        0.9 & 0.0 & 0.0 & 0.1 \\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "T(s_t, \\text{'c'}, s_{t+1}) = \n",
    "    \\begin{bmatrix}\n",
    "        0.0 & 0.1 & 0.9 & 0.0 \\\\\n",
    "        0.9 & 0.1 & 0.0 & 0.0 \\\\\n",
    "        0.0 & 0.0 & 0.1 & 0.9 \\\\\n",
    "        0.9 & 0.0 & 0.0 & 0.1 \\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    " \n",
    "Note that the only effect of the action is to change the transition probability from state 0.\n",
    "\n",
    "Write a procedure `q_em(mdp, s, a, h)` that computes the horizon-h Q value for state `s` and action `a` by using the definition of the finite-horizon Q function in the notes (but including a discount factor). \n",
    "\n",
    "This can be written as a relatively simple recursive procedure with a base case (what is the Q value when horizon is 0?) and a recursive case that computes the horizon `h` values assuming we can (recursively) get horizon `h-1` values.\n",
    "\n",
    "## 5.3.A) \n",
    "What is $q_{em}(0, \\text{'b'}, 0)$?\n",
    "\n",
    "0.0\n",
    "\n",
    "## 5.3.B) \n",
    "What is $q_{em}(0, \\text{'b'}, 1)$?\n",
    "\n",
    "0.0\n",
    "\n",
    "## 5.3.C) \n",
    "What is $q_{em}(0, \\text{'b'}, 2)$?\n",
    "\n",
    "0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "id": "M5qsQ-vVjco9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "#5.3.D\n",
    "def q_em(mdp, s, a, h):\n",
    "    # Your code here\n",
    "    Q_old = TabularQ(mdp.states, mdp.actions)\n",
    "    Q_new = Q_old.copy()\n",
    "    for horizon in range(1, h+1):\n",
    "        Q_new = Q_old.copy()\n",
    "        for state in mdp.states:\n",
    "            for action in mdp.actions:\n",
    "                transition_dist = mdp.transition_model(state, action)\n",
    "                expected_val = transition_dist.expectation(lambda s: value(Q_old, s))\n",
    "                Q_val = mdp.reward_fn(state, action) + mdp.discount_factor*expected_val\n",
    "                Q_new.set(state, action, Q_val)\n",
    "        Q_old = Q_new.copy()\n",
    "    return Q_new.get(s, a)\n",
    "\n",
    "tiny = MDP([0, 1, 2, 3], ['b', 'c'], tiny_transition, tiny_reward, 1.0)\n",
    "\n",
    "#5.3.A\n",
    "print(q_em(tiny, 0, 'b', 0))\n",
    "#5.3.B\n",
    "print(q_em(tiny, 0, 'b', 1))\n",
    "#5.3.C\n",
    "print(q_em(tiny, 0, 'b', 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pgEI65qLKKQ"
   },
   "source": [
    "We will be using the \"tiny\" MDP again to test `q_em`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "id": "9i3X0Q_v-3Vo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_q_em():\n",
    "    tiny = MDP([0, 1, 2, 3], ['b', 'c'], tiny_transition, tiny_reward, 0.9)\n",
    "    assert(np.allclose([q_em(tiny, 0, 'b', 1)], [0.0]))\n",
    "    assert(np.allclose([q_em(tiny, 0, 'b', 2)], [0.81]))\n",
    "    assert(np.allclose([q_em(tiny, 0, 'b', 3)], [1.0287000000000002]))\n",
    "    assert(np.allclose([q_em(tiny, 0, 'c', 3)], [1.4103]))\n",
    "    assert(np.allclose([q_em(tiny, 2, 'b', 3)], [1.9116000000000002]))\n",
    "    print(\"Tests passed!\")\n",
    "\n",
    "test_q_em()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
