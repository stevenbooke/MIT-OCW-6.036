{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xIaEwCD406A"
   },
   "source": [
    "# MIT 6.036 Spring 2019: Homework 7 #\n",
    "\n",
    "This colab notebook provides code and a framework for problem 2 of [the homework](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week7/week7_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
    "\n",
    "## <section>**Setup**</section>\n",
    "\n",
    "First, download the code distribution for this homework that contains test cases and helper functions.\n",
    "\n",
    "Run the next code block to download and import the code for this lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2YM-_zLf9Bp-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: code_for_hw7*\n",
      "--2023-03-29 15:44:54--  https://introml_oll.odl.mit.edu/6.036/static/homework/hw07/code_for_hw7.zip\n",
      "Resolving introml_oll.odl.mit.edu (introml_oll.odl.mit.edu)... 3.226.240.108\n",
      "Connecting to introml_oll.odl.mit.edu (introml_oll.odl.mit.edu)|3.226.240.108|:443... connected.\n",
      "WARNING: cannot verify introml_oll.odl.mit.edu's certificate, issued by ‘CN=InCommon RSA Server CA,OU=InCommon,O=Internet2,L=Ann Arbor,ST=MI,C=US’:\n",
      "  Unable to locally verify the issuer's authority.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://introml_oll.odl.mit.edu/cat-soop/_static/6.036/homework/hw07/code_for_hw7.zip [following]\n",
      "--2023-03-29 15:44:55--  https://introml_oll.odl.mit.edu/cat-soop/_static/6.036/homework/hw07/code_for_hw7.zip\n",
      "Reusing existing connection to introml_oll.odl.mit.edu:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7815 (7.6K) [application/zip]\n",
      "Saving to: ‘code_for_hw7.zip’\n",
      "\n",
      "code_for_hw7.zip    100%[===================>]   7.63K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-03-29 15:44:55 (2.43 GB/s) - ‘code_for_hw7.zip’ saved [7815/7815]\n",
      "\n",
      "Archive:  code_for_hw7.zip\n",
      "   creating: code_for_hw7/\n",
      "  inflating: code_for_hw7/code_for_hw7.py  \n",
      "  inflating: code_for_hw7/expected_results.py  \n",
      "  inflating: code_for_hw7/modules_disp.py  \n"
     ]
    }
   ],
   "source": [
    "!rm -rf code_for_hw7*\n",
    "!rm -rf mnist\n",
    "!wget --no-check-certificate https://introml_oll.odl.mit.edu/6.036/static/homework/hw07/code_for_hw7.zip\n",
    "!unzip code_for_hw7.zip\n",
    "!mv code_for_hw7/* .\n",
    "\n",
    "from code_for_hw7 import *\n",
    "import numpy as np\n",
    "import modules_disp as disp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework continues the exploration and implementation of neural networks as discussed in the notes.\n",
    "\n",
    "In particular, this homework considers neural networks with multiple layers. Each layer has multiple inputs and outputs, and can be broken down into two parts:\n",
    "\n",
    "- A __linear__ module that implements a linear transformation: $z_j = ( \\sum_{i=1} ^ {m} x_iW_{i,j} ) + W_{0j}$ specified by a weight matrix $W$ and a bias vector $W_0$. The output is $[z_1, \\ldots, z_n]^T$. \n",
    "- An __activation__ module that applies an activation function to the outputs of the linear module for some activation function $f$, such as Tanh or ReLU in the hidden layers or Softmax (see below) at the output layer. We write the output as: $[f(z_1), \\ldots, f(z_m)]$, although technically, for some activation functions such as softmax, each output will depend on all the $z_i$, not just one.\n",
    "\n",
    "We will use the following notation for quantities in a network:\n",
    "\n",
    "- Inputs to the network are $x_1, \\ldots, x_d$.\n",
    "- Number of layers is $L$\n",
    "- There are $m^l$ inputs to layer $l$\n",
    "- There are $n^l = m^{l+1}$ outouts from layer $l$\n",
    "- The weight matrix for layer $l$ is $W^l$, an $m^l \\times n^l$ matrix, and the bias vector (offset) is $W_0^l$ an $n^l \\times 1$ vector\n",
    "- The outputs of the linear module for layer $l$ are known as __pre-activation__ values and denoted $z^l$\n",
    "- The activation function at layer $l$ is $f^l(\\cdot)$ \n",
    "- Layer $l$ activations are $a^l = [f^l(z_1^l), \\ldots, f^l(z_{n^l}^l)]^T$\n",
    "- The output of the network is the values $a^L = [f^L(z_1^L), \\ldots, f^L(z_{n^L}^L)]^T$\n",
    "- Loss function $Loss(a,y)$ measures the loss of output values $a$ when the target is $y$\n",
    "\n",
    "Here is an illustrative picture:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://introml_oll.odl.mit.edu/cat-soop/_static/6.036/homework/hw07/nn_notation_new.png\" width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "# 1) Backpropagation\n",
    "The materials for week 6 and week 7 will be helpful here, including the week6 lecture and week7 lecture.\n",
    "\n",
    "We have seen in the lecture notes how to train multi-layer neural networks as classifiers using stochastic gradient descent (SGD). One of the key steps in the SGD method is the evaluation of the gradient of the loss function with respect to the model parameters. In this problem, you will derive the backpropagation method for a general $L$-layer neural network. We'll exploit the decomposition of the network into linear and activation modules that we introduced at the start of this homework. Remember that we've defined the shapes of the various quantities at the start of the homework.\n",
    "\n",
    "- Each linear module has a `forward` method that takes in a column vector of activations $A$ (from the previous layer) and returns a column vector $Z$ of pre-activations; it can also store its input or output vectors for use by other methods (e.g., for subsequent backpropagation).\n",
    "- Each activation module has a `forward` method that takes in a column vector of pre-activations $Z$ and returns a column vector $A$ of activations; it can also store its input or output vectors for use by other methods (e.g., for subsequent backpropagation).\n",
    "\n",
    "- Each linear module has a `backward` method that takes in a column vector $\\frac {\\partial Loss}{Z}$ and returns a column vector $\\frac {\\partial Loss}{A}$. This module also computes and stores $\\frac {\\partial Loss}{W}$ and $\\frac {\\partial Loss}{W_0}$, the gradients with respect to the weights.\n",
    "\n",
    "- Each activation module has a `backward` method that takes in a column vector $\\frac {\\partial Loss}{A}$ and returns a column vector $\\frac {\\partial Loss}{Z}$.\n",
    "\n",
    "The backpropagation algorithm will consist of:\n",
    "\n",
    "- Calling the `forward` method of each module in turn, feeding the output of one module as the input to the next; starting with the input values of the network. After this pass, we have a predicted value for the final network output.\n",
    "\n",
    "- Calling the `backward` method of each module in reverse order, using the returned value from one module as the input value of the previous one. The starting value for the backward method is $\\partial Loss(a^L, y) / \\partial a^L$, where $a^L$ is the activation of the final layer (computed during the forward pass) and $y$ is the desired output (the label).\n",
    "\n",
    "\n",
    "# 1.1) Linear Module\n",
    "The `forward` method, given $A$ from the previous layer, implements:\n",
    "\n",
    "$$ Z = W^T A + W_0 $$\n",
    "\n",
    "and stores the input $A$ to be used by the `backward` method.\n",
    "Recall that there are $n^l = m^{l+1}$ outputs from layer $l$. For layer $l$, $W$ is a $m^l \\times n^l$ matrix, $W_0$ is a $n^l \\times 1$ vector, and $A$ from the previous layer is a $n^{l-1} \\times 1$ (or $m^l \\times 1$) vector. Given these shapes, make sure that you understand why the forward equation has $W^T$ and not $W$.\n",
    "\n",
    "The following questions ask for a matrix expression involving any of `A`, `Z`, `dLdA`, `dLdZ`, `W` and `W_0`.\n",
    "\n",
    "__Enter your answers as Python expressions. You can use `transpose(x)` for transpose of an array, and `x@y` to indicate a matrix product of two arrays. Remember that `x*y` denotes component-wise multiplication.__\n",
    "\n",
    "The `backward` method, given $dLdZ = \\frac{\\partial Loss}{\\partial Z}$ (an $n^l \\times 1$ vector), returns $dLdA = \\frac{\\partial Loss}{\\partial A}$ (an $m^l \\times 1$ vector):\n",
    "\n",
    "$$ \\frac{\\partial Loss}{\\partial A} = \\frac{\\partial Z}{\\partial A} \\frac{\\partial Loss}{\\partial Z} $$\n",
    "\n",
    "## 1.1.A)\n",
    "\n",
    "$$ \\frac{\\partial Z}{\\partial A} (W^T A + W_0) = W $$\n",
    "\n",
    "$$ dLdA = \\frac{\\partial Loss}{\\partial A} = \\frac{\\partial Z}{\\partial A} \\frac{\\partial Loss}{\\partial Z} = W @ dLdZ $$\n",
    "\n",
    "\n",
    "The `backward method`, given $dLdZ = \\frac{\\partial Loss}{\\partial Z}$, also computes $dLdW$ (an $m^l \\times n^l$ matrix) and $dLdW0$ (an $n^l \\times 1$ vector), and stores them in the module instance.\n",
    "\n",
    "$$ dLdW = \\frac{\\partial Loss}{\\partial W} = \\frac{\\partial Z}{\\partial W} \\frac{\\partial Loss}{\\partial Z} $$\n",
    "\n",
    "\n",
    "## 1.1.B)\n",
    "\n",
    "$$ \\frac{\\partial Z}{\\partial W} (W^T A + W_0) = A $$\n",
    "\n",
    "$$ dLdW = \\frac{\\partial Loss}{\\partial W} = \\frac{\\partial Z}{\\partial W} \\frac{\\partial Loss}{\\partial Z} = A @ dLdZ^T $$\n",
    "\n",
    "\n",
    "and\n",
    "\n",
    "$$ dLdW0 = \\frac{\\partial Loss}{\\partial W_0} = \\frac{\\partial Loss}{\\partial Z} \\frac{\\partial Z}{\\partial W_0} $$\n",
    "\n",
    "## 1.1.C)\n",
    "\n",
    "$$ \\frac{\\partial Z}{\\partial W_0} (W^T A + W_0) = 1 $$\n",
    "\n",
    "$$ dLdW0 = \\frac{\\partial Loss}{\\partial W_0} = \\frac{\\partial Loss}{\\partial Z} \\frac{\\partial Z}{\\partial W_0} = dLdZ \\times 1 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2) Activation Module\n",
    "Activation modules don't have any weights and so they are simpler.\n",
    "\n",
    "The `forward` method for functions like _tanh_ or _sigmoid_, given $Z$, return the function on the vector, componentwise. _Softmax_ operates on the whole vector, as described earlier, and will need some special treatment.\n",
    "\n",
    "The `backward` method, given $dLdA = \\frac{\\partial Loss}{\\partial A}$ returns:\n",
    "\n",
    "$$ dLdZ = \\frac{\\partial Loss}{\\partial Z} = \\frac{\\partial Loss}{\\partial A} \\frac{\\partial A}{\\partial Z} $$\n",
    "\n",
    "In this case, $m^l = n^l$ and the quantities are column vectors of that size.\n",
    "\n",
    "For Softmax $=SM(Z)$ at the output layer and assuming that we are using $NLL$ as the $Loss(A,Y)$ function, we have seen that there is a simple form for $dLdZ = \\frac{\\partial Loss}{\\partial Z}$; namely, it is the prediction error $A−Y$. A similar result holds when using $NLL$ with a sigmoid output activation or a quadratic loss with a linear output activation. Note that for hinge loss with a linear activation, the form of dLdZ is different (see the lecture notes on the hinge loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFxhrJ5XDlvb"
   },
   "source": [
    "# 2) Implementing Neural Networks\n",
    "\n",
    "This homework considers neural networks with multiple layers. Each layer has multiple inputs and outputs, and can be broken down into two parts:\n",
    "\n",
    "<br>\n",
    "A linear module that implements a linear transformation:     $ z_j = (\\sum^{m}_{i=1} x_i W_{i,j}) + {W_0}_jz$\n",
    "\n",
    "specified by a weight matrix $W$ and a bias vector $W_0$. The output is $[z_1, \\ldots, z_n]^T$\n",
    "\n",
    "<br>\n",
    "An activation module that applies an activation function to the outputs of the linear module for some activation function $f$, such as Tanh or ReLU in the hidden layers or Softmax (see below) at the output layer. We write the output as: $[f(z_1), \\ldots, f(z_m)]^T$, although technically, for some activation functions such as softmax, each output will depend on all the $z_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjQgtwPHj08n"
   },
   "source": [
    "We'll use the modular implementation that we guided you through in the previous problem, which leads to clean code. The basic framework for SGD training is given below. We can construct a network and train it as follows:\n",
    "\n",
    "```\n",
    "# build a 3-layer network\n",
    "net = Sequential([Linear(2,3), Tanh(),\n",
    "                  Linear(3,3), Tanh(),\n",
    "    \t          Linear(3,2), SoftMax()])\n",
    "# train the network on data and labels\n",
    "net.sgd(X, Y)\n",
    "```\n",
    "Please fill in any unimplemented methods below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEwpgsbnho9K"
   },
   "source": [
    "## Linear Modules: ##\n",
    "Each linear module has a forward method that takes in a batch of activations A (from the previous layer) and returns a batch of pre-activations Z.\n",
    "\n",
    "Each linear module has a backward method that takes in dLdZ and returns dLdA. This module also computes and stores dLdW and dLdW0, the gradients with respect to the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "-VsYLAxCfy7U"
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, m, n):\n",
    "        self.m, self.n = (m, n)  # (in size, out size)\n",
    "        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
    "        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A   # (m x b)  Hint: make sure you understand what b stands for\n",
    "        # Your code (n x b)\n",
    "        return (self.W.T @ self.A) + self.W0 # n x m @ m x b = n x b\n",
    "\n",
    "    def backward(self, dLdZ):  # dLdZ is (n x b), uses stored self.A\n",
    "        # Your code: return dLdA (m x b)\n",
    "        dLdA = self.W @ dLdZ # m x n @ n x b = m x b\n",
    "        self.dLdW = self.A @ dLdZ.T # m x b @ b x m = m x m\n",
    "        self.dLdW0 = np.sum(dLdZ, axis=1).reshape((self.n, 1)) # n x 1\n",
    "        return dLdA # m x b\n",
    "\n",
    "    def sgd_step(self, lrate):  # Gradient descent step\n",
    "        # Your code\n",
    "        self.W -= lrate * self.dLdW\n",
    "        self.W0 -= lrate * np.sum(self.dLdW0, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqZ7_kZYr5s5"
   },
   "source": [
    " You are encouraged to make your own tests for each module. A unit test method and an example test case are given below for your reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "aY3yePY0r4eA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_forward: OK\n",
      "linear_backward: OK\n",
      "linear_sgd_step_W: OK\n",
      "linear_sgd_step_W0: OK\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# data\n",
    "X, Y = super_simple_separable()\n",
    "\n",
    "# module\n",
    "linear_1 = Linear(2, 3)\n",
    "\n",
    "#hyperparameters\n",
    "lrate = 0.005\n",
    "\n",
    "# test case\n",
    "# forward\n",
    "z_1 = linear_1.forward(X)\n",
    "exp_z_1 =  np.array([[10.41750064, 6.91122168, 20.73366505, 22.8912344],\n",
    "                     [7.16872235, 3.48998746, 10.46996239, 9.9982611],\n",
    "                     [-2.07105455, 0.69413716, 2.08241149, 4.84966811]])\n",
    "unit_test(\"linear_forward\", exp_z_1, z_1)\n",
    "\n",
    "# backward\n",
    "dL_dz1 = np.array([[1.69467553e-09, -1.33530535e-06, 0.00000000e+00, -0.00000000e+00],\n",
    "                                     [-5.24547376e-07, 5.82459519e-04, -3.84805202e-10, 1.47943038e-09],\n",
    "                                     [-3.47063705e-02, 2.55611604e-01, -1.83538094e-02, 1.11838432e-04]])\n",
    "exp_dLdX = np.array([[-2.40194628e-02, 1.77064845e-01, -1.27021626e-02, 7.74006953e-05],\n",
    "                                    [2.39827939e-02, -1.75870737e-01, 1.26832126e-02, -7.72828555e-05]])\n",
    "dLdX = linear_1.backward(dL_dz1)\n",
    "unit_test(\"linear_backward\", exp_dLdX, dLdX)\n",
    "\n",
    "# sgd step\n",
    "linear_1.sgd_step(lrate)\n",
    "exp_linear_1_W = np.array([[1.2473734,  0.28294514,  0.68940437],\n",
    "                           [1.58455079, 1.32055711, -0.69218045]]),\n",
    "unit_test(\"linear_sgd_step_W\",  exp_linear_1_W,  linear_1.W)\n",
    "\n",
    "exp_linear_1_W0 = np.array([[6.66805339e-09],\n",
    "                            [-2.90968033e-06],\n",
    "                            [-1.01331631e-03]]),\n",
    "unit_test(\"linear_sgd_step_W0\", exp_linear_1_W0, linear_1.W0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ETL01mPsBz4"
   },
   "source": [
    "The following datasets are defined for your use:\n",
    "*  `super_simple_separable_through_origin()`\n",
    "*  `super_simple_separable()`\n",
    "*  `xor()`\n",
    "*  `xor_more()`\n",
    "*  `hard()`\n",
    "\n",
    "Further, a plotting function is defined for your usage in modules_disp.py, and can be called in the colab notebook as `disp.plot_nn()`.\n",
    "\n",
    "```\n",
    "def plot_nn(X, Y, nn):\n",
    "    \"\"\" Plot output of nn vs. data \"\"\"\n",
    "    def predict(x):\n",
    "        return nn.modules[-1].class_fun(nn.forward(x))[0]\n",
    "    xmin, ymin = np.min(X, axis=1)-1\n",
    "    xmax, ymax = np.max(X, axis=1)+1\n",
    "    nax = plot_objective_2d(lambda x: predict(x), xmin, xmax, ymin, ymax)\n",
    "    plot_data(X, Y, nax)\n",
    "    plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4s70beWJh09h"
   },
   "source": [
    "## Activation functions: ##\n",
    "Each activation module has a forward method that takes in a batch of pre-activations Z and returns a batch of activations A.\n",
    "\n",
    "Each activation module has a backward method that takes in dLdA and returns dLdZ, with the exception of SoftMax, where we assume dLdZ is passed in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwaNAtLnhenT"
   },
   "source": [
    "### Tanh: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "ff6eD3dnftiR"
   },
   "outputs": [],
   "source": [
    "class Tanh(Module):            # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = np.tanh(Z)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # Uses stored self.A\n",
    "        # Your code: return dLdZ (n, b)\n",
    "        return dLdA * (1 - np.square(self.A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FW7ocKRhcgY"
   },
   "source": [
    "### ReLU: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "1fm2KsLUfqdp"
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):              # Layer activation\n",
    "    def forward(self, Z):\n",
    "        # Your code: (n, b)\n",
    "        self.A = np.where(Z > 0, Z, 0)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # uses stored self.A\n",
    "        # Your code: return dLdZ (n, b)\n",
    "        return dLdA * np.where(self.A > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKtXuTQ0hSNO"
   },
   "source": [
    "### SoftMax: ###\n",
    "For `SoftMax.class_fun()`, given the column vector of class probabilities for each point (computed by Softmax), return a vector of the classes (integers) with the highest probability for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "fqK-CJrnfn22"
   },
   "outputs": [],
   "source": [
    "class SoftMax(Module):           # Output activation\n",
    "    def forward(self, Z):\n",
    "        # Your code: (n, b)\n",
    "        self.A = np.apply_along_axis(lambda col: np.exp(col) / np.sum(np.exp(col)), axis=0, arr=Z)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdZ):    # Assume that dLdZ is passed in\n",
    "        return dLdZ\n",
    "\n",
    "    def class_fun(self, Ypred):  # Return class indices\n",
    "        # Your code: (1, b)\n",
    "        return np.apply_along_axis(lambda col: np.argmax(col), 0, self.A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZc7HnMSh4fn"
   },
   "source": [
    "## Loss Functions: ##\n",
    "Each loss module has a forward method that takes in a batch of predictions Ypred (from the previous layer) and labels Y and returns a scalar loss value.\n",
    "\n",
    "The NLL module has a backward method that returns dLdZ, the gradient with respect to the preactivation to SoftMax (note: not the activation!), since we are always pairing SoftMax activation with NLL loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4uy0pHVhNd8"
   },
   "source": [
    "### NLL: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "17Fb8mimflgb"
   },
   "outputs": [],
   "source": [
    "class NLL(Module):       # Loss\n",
    "    def forward(self, Ypred, Y):\n",
    "        self.Ypred = Ypred\n",
    "        self.Y = Y\n",
    "        # Your code\n",
    "        return -np.sum(Y * np.log(Ypred))\n",
    "\n",
    "    def backward(self):  # Use stored self.Ypred, self.Y\n",
    "        # Your code\n",
    "        return self.Ypred - self.Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1EffzDFkqMX"
   },
   "source": [
    "## Activation and Loss Test Cases: ##\n",
    "Run Test 1 and Test 2 below and compare your outputs with the expected outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "9DJFzpahkvcD",
    "outputId": "f37fe4f7-9d34-474f-cac3-2183396e7bed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_1.W: OK\n",
      "linear_1.W0: OK\n",
      "linear_2.W: OK\n",
      "linear_2.W0: OK\n",
      "z_1: OK\n",
      "a_1: OK\n",
      "z_2: OK\n",
      "a_2: OK\n",
      "loss: OK\n",
      "dloss: OK\n",
      "dL_dz2: OK\n",
      "dL_da1: OK\n",
      "dL_dz1: OK\n",
      "dL_dX: OK\n",
      "updated_linear_1.W: OK\n",
      "updated_linear_1.W0: OK\n",
      "updated_linear_2.W: OK\n",
      "updated_linear_2.W0: OK\n"
     ]
    }
   ],
   "source": [
    "# TEST 1: sgd_test for Tanh activation and SoftMax output\n",
    "np.random.seed(0)\n",
    "sgd_test(Sequential([Linear(2,3), Tanh(), Linear(3,2), SoftMax()], NLL()), test_1_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "Bd0dXg-Qk05_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_1.W: OK\n",
      "linear_1.W0: OK\n",
      "linear_2.W: OK\n",
      "linear_2.W0: OK\n",
      "z_1: OK\n",
      "a_1: OK\n",
      "z_2: OK\n",
      "a_2: OK\n",
      "loss: OK\n",
      "dloss: OK\n",
      "dL_dz2: OK\n",
      "dL_da1: OK\n",
      "dL_dz1: OK\n",
      "dL_dX: OK\n",
      "updated_linear_1.W: OK\n",
      "updated_linear_1.W0: OK\n",
      "updated_linear_2.W: OK\n",
      "updated_linear_2.W0: OK\n"
     ]
    }
   ],
   "source": [
    "# TEST 2: sgd_test for ReLU activation and SoftMax output\n",
    "np.random.seed(0)\n",
    "sgd_test(Sequential([Linear(2,3), ReLU(), Linear(3,2), SoftMax()], NLL()), test_2_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-l5JgBU2iBCZ"
   },
   "source": [
    "## Neural Network: ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXMGcdnXgiF3"
   },
   "source": [
    "Implement SGD. Randomly pick a data point Xt, Yt by using np.random.randint to choose a random index into the data. Compute the predicted output Ypred for Xt with the forward method. Compute the loss for Ypred relative to Yt. Use the backward method to compute the gradients. Use the sgd_step method to change the weights. Repeat.\n",
    "\n",
    "We will (later) be generalizing SGD to operate on a \"mini-batch\" of data points instead of a single point. You should strive for an implementation of the forward, backward, and `class_fun` methods that works with batches of data. Note that when $b$ is mentioned as part of the shape of a matrix in the code, this $b$ refers to the number of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "ejO15Vr7fhKB"
   },
   "outputs": [],
   "source": [
    " class Sequential:\n",
    "    def __init__(self, modules, loss):            # List of modules, loss module\n",
    "        self.modules = modules\n",
    "        self.loss = loss\n",
    "\n",
    "    def sgd(self, X, Y, iters=100, lrate=0.005):  # Train\n",
    "        D, N = X.shape\n",
    "        # Your code\n",
    "        for it in range(iters):\n",
    "            random_index = np.random.randint(N)\n",
    "            y = Y[:, random_index:random_index+1]\n",
    "            x = X[:, random_index:random_index+1]\n",
    "            y_pred = self.forward(x)\n",
    "            loss = self.loss.forward(y_pred, y)\n",
    "            err = self.loss.backward()\n",
    "            self.backward(err)\n",
    "            self.sgd_step(lrate)\n",
    "            self.print_accuracy(it, X, Y, loss)\n",
    "\n",
    "    def forward(self, Xt):                        # Compute Ypred\n",
    "        for m in self.modules: Xt = m.forward(Xt)\n",
    "        return Xt\n",
    "\n",
    "    def backward(self, delta):                    # Update dLdW and dLdW0\n",
    "        # Note reversed list of modules\n",
    "        for m in self.modules[::-1]: delta = m.backward(delta)\n",
    "\n",
    "    def sgd_step(self, lrate):                    # Gradient descent step\n",
    "        for m in self.modules: m.sgd_step(lrate)\n",
    "\n",
    "    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n",
    "        # Utility method to print accuracy on full dataset, should\n",
    "        # improve over time when doing SGD. Also prints current loss,\n",
    "        # which should decrease over time. Call this on each iteration\n",
    "        # of SGD!\n",
    "        if it % every == 1:\n",
    "            cf = self.modules[-1].class_fun\n",
    "            acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
    "            print('Iteration =', it, '\\tAcc =', acc, '\\tLoss =', cur_loss, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUojaXqphDjh"
   },
   "source": [
    "## Neural Network / SGD Test Cases: ##\n",
    "Use Test 3 and Test 4 to help you debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "wmupM8OScodw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1 \tAcc = 1.0 \tLoss = 0.8163501209126218\n",
      "Iteration = 251 \tAcc = 1.0 \tLoss = 0.10477735930943054\n",
      "Iteration = 501 \tAcc = 1.0 \tLoss = 0.3520196947953328\n",
      "Iteration = 751 \tAcc = 1.0 \tLoss = 0.5078020702186467\n",
      "Iteration = 1001 \tAcc = 1.0 \tLoss = 0.07333365070679529\n",
      "Iteration = 1251 \tAcc = 1.0 \tLoss = 0.02658564276835577\n",
      "Iteration = 1501 \tAcc = 1.0 \tLoss = 0.02471667672374407\n",
      "Iteration = 1751 \tAcc = 1.0 \tLoss = 1.9120151135596022\n",
      "Iteration = 2001 \tAcc = 1.0 \tLoss = 0.010556590138169696\n",
      "Iteration = 2251 \tAcc = 1.0 \tLoss = 0.012015435423861806\n",
      "Iteration = 2501 \tAcc = 1.0 \tLoss = 0.008657021958802552\n",
      "Iteration = 2751 \tAcc = 1.0 \tLoss = 0.008426119188991924\n",
      "Iteration = 3001 \tAcc = 1.0 \tLoss = 0.40637858317403797\n",
      "Iteration = 3251 \tAcc = 1.0 \tLoss = 0.0019107120086166122\n",
      "Iteration = 3501 \tAcc = 1.0 \tLoss = 0.04188944453555026\n",
      "Iteration = 3751 \tAcc = 1.0 \tLoss = 0.003208643240697625\n",
      "Iteration = 4001 \tAcc = 1.0 \tLoss = 0.0027827234467466353\n",
      "Iteration = 4251 \tAcc = 1.0 \tLoss = 0.006447831275421869\n",
      "Iteration = 4501 \tAcc = 1.0 \tLoss = 0.0051098971471632395\n",
      "Iteration = 4751 \tAcc = 1.0 \tLoss = 0.0006015141514427351\n",
      "Iteration = 5001 \tAcc = 1.0 \tLoss = 0.003323513730988572\n",
      "Iteration = 5251 \tAcc = 1.0 \tLoss = 0.002696401140358734\n",
      "Iteration = 5501 \tAcc = 1.0 \tLoss = 1.5733587445769748\n",
      "Iteration = 5751 \tAcc = 1.0 \tLoss = 0.0008904829688640953\n",
      "Iteration = 6001 \tAcc = 1.0 \tLoss = 0.28379989830219554\n",
      "Iteration = 6251 \tAcc = 1.0 \tLoss = 0.0002370717842358095\n",
      "Iteration = 6501 \tAcc = 1.0 \tLoss = 1.602687955170177\n",
      "Iteration = 6751 \tAcc = 1.0 \tLoss = 0.06348476041514994\n",
      "Iteration = 7001 \tAcc = 1.0 \tLoss = 0.03556668890317227\n",
      "Iteration = 7251 \tAcc = 1.0 \tLoss = 0.0004244692510122907\n",
      "Iteration = 7501 \tAcc = 1.0 \tLoss = 0.000566915875458982\n",
      "Iteration = 7751 \tAcc = 1.0 \tLoss = 0.0016284249399882587\n",
      "Iteration = 8001 \tAcc = 1.0 \tLoss = 1.902562543182756e-05\n",
      "Iteration = 8251 \tAcc = 1.0 \tLoss = 0.0013999772807491162\n",
      "Iteration = 8501 \tAcc = 1.0 \tLoss = 0.023600605117732352\n",
      "Iteration = 8751 \tAcc = 1.0 \tLoss = 0.020988890685233783\n",
      "Iteration = 9001 \tAcc = 1.0 \tLoss = 0.0009944086541207391\n",
      "Iteration = 9251 \tAcc = 1.0 \tLoss = 0.4033352052392007\n",
      "Iteration = 9501 \tAcc = 1.0 \tLoss = 0.01643917092926479\n",
      "Iteration = 9751 \tAcc = 1.0 \tLoss = 0.0194826055228657\n",
      "Iteration = 10001 \tAcc = 1.0 \tLoss = 0.0013002555581601027\n",
      "Iteration = 10251 \tAcc = 1.0 \tLoss = 0.0037168093584231112\n",
      "Iteration = 10501 \tAcc = 1.0 \tLoss = 0.051704508783197774\n",
      "Iteration = 10751 \tAcc = 1.0 \tLoss = 0.21650686261552726\n",
      "Iteration = 11001 \tAcc = 1.0 \tLoss = 0.0009353968413641944\n",
      "Iteration = 11251 \tAcc = 1.0 \tLoss = 0.1838543610478957\n",
      "Iteration = 11501 \tAcc = 1.0 \tLoss = 0.2578018880739478\n",
      "Iteration = 11751 \tAcc = 1.0 \tLoss = 3.5145681645907264e-05\n",
      "Iteration = 12001 \tAcc = 1.0 \tLoss = 0.04832054963963184\n",
      "Iteration = 12251 \tAcc = 1.0 \tLoss = 0.43721734033048615\n",
      "Iteration = 12501 \tAcc = 1.0 \tLoss = 3.819963156900902e-06\n",
      "Iteration = 12751 \tAcc = 1.0 \tLoss = 0.0001525639752614136\n",
      "Iteration = 13001 \tAcc = 1.0 \tLoss = 0.12798685244072427\n",
      "Iteration = 13251 \tAcc = 1.0 \tLoss = 0.020132657578398567\n",
      "Iteration = 13501 \tAcc = 1.0 \tLoss = 0.0003704585858940595\n",
      "Iteration = 13751 \tAcc = 1.0 \tLoss = 0.01977257394891925\n",
      "Iteration = 14001 \tAcc = 1.0 \tLoss = 1.1605467328190633e-05\n",
      "Iteration = 14251 \tAcc = 1.0 \tLoss = 0.16762794634099573\n",
      "Iteration = 14501 \tAcc = 1.0 \tLoss = 0.00032694370044933146\n",
      "Iteration = 14751 \tAcc = 1.0 \tLoss = 0.00036550841233230904\n",
      "Iteration = 15001 \tAcc = 1.0 \tLoss = 0.0002614001534531944\n",
      "Iteration = 15251 \tAcc = 1.0 \tLoss = 0.021407297238818783\n",
      "Iteration = 15501 \tAcc = 1.0 \tLoss = 7.51306911076795e-05\n",
      "Iteration = 15751 \tAcc = 1.0 \tLoss = 0.0894411699236106\n",
      "Iteration = 16001 \tAcc = 1.0 \tLoss = 0.0007504763950901106\n",
      "Iteration = 16251 \tAcc = 1.0 \tLoss = 0.10284490738726118\n",
      "Iteration = 16501 \tAcc = 1.0 \tLoss = 4.298613623249328e-06\n",
      "Iteration = 16751 \tAcc = 1.0 \tLoss = 1.291946070531986\n",
      "Iteration = 17001 \tAcc = 1.0 \tLoss = 2.50211840360188e-06\n",
      "Iteration = 17251 \tAcc = 1.0 \tLoss = 0.33407517951118637\n",
      "Iteration = 17501 \tAcc = 1.0 \tLoss = 0.0002118630156356675\n",
      "Iteration = 17751 \tAcc = 1.0 \tLoss = 0.00020569750935814878\n",
      "Iteration = 18001 \tAcc = 1.0 \tLoss = 0.00020029163864487923\n",
      "Iteration = 18251 \tAcc = 1.0 \tLoss = 2.4444760649052847e-05\n",
      "Iteration = 18501 \tAcc = 1.0 \tLoss = 2.778742282727566e-06\n",
      "Iteration = 18751 \tAcc = 1.0 \tLoss = 1.303889203904021e-07\n",
      "Iteration = 19001 \tAcc = 1.0 \tLoss = 0.00013892921656429846\n",
      "Iteration = 19251 \tAcc = 1.0 \tLoss = 0.09422176785103022\n",
      "Iteration = 19501 \tAcc = 1.0 \tLoss = 0.00011630084144658905\n",
      "Iteration = 19751 \tAcc = 1.0 \tLoss = 0.0047860355102345795\n",
      "Iteration = 20001 \tAcc = 1.0 \tLoss = 0.9370938733945094\n",
      "Iteration = 20251 \tAcc = 1.0 \tLoss = 0.10584146383866073\n",
      "Iteration = 20501 \tAcc = 1.0 \tLoss = 9.764856072535319e-05\n",
      "Iteration = 20751 \tAcc = 1.0 \tLoss = 0.00012020331694157445\n",
      "Iteration = 21001 \tAcc = 1.0 \tLoss = 0.000828009986534501\n",
      "Iteration = 21251 \tAcc = 1.0 \tLoss = 0.006506187099757169\n",
      "Iteration = 21501 \tAcc = 1.0 \tLoss = 4.2104380526559754e-05\n",
      "Iteration = 21751 \tAcc = 1.0 \tLoss = 0.00010465490697483777\n",
      "Iteration = 22001 \tAcc = 1.0 \tLoss = 3.713281471519605e-05\n",
      "Iteration = 22251 \tAcc = 1.0 \tLoss = 0.22262267109856007\n",
      "Iteration = 22501 \tAcc = 1.0 \tLoss = 7.061355965640761e-05\n",
      "Iteration = 22751 \tAcc = 1.0 \tLoss = 1.8868229490301634e-08\n",
      "Iteration = 23001 \tAcc = 1.0 \tLoss = 6.572622370549784e-05\n",
      "Iteration = 23251 \tAcc = 1.0 \tLoss = 0.36605450710160437\n",
      "Iteration = 23501 \tAcc = 1.0 \tLoss = 0.7850514102745723\n",
      "Iteration = 23751 \tAcc = 1.0 \tLoss = 1.1493024316632003e-09\n",
      "Iteration = 24001 \tAcc = 1.0 \tLoss = 0.08887984467325104\n",
      "Iteration = 24251 \tAcc = 1.0 \tLoss = 1.0022895896846751e-08\n",
      "Iteration = 24501 \tAcc = 1.0 \tLoss = 0.07104267499365331\n",
      "Iteration = 24751 \tAcc = 1.0 \tLoss = 0.05965370221537515\n",
      "Iteration = 25001 \tAcc = 1.0 \tLoss = 8.214733983649385e-06\n",
      "Iteration = 25251 \tAcc = 1.0 \tLoss = 0.031103845041374277\n",
      "Iteration = 25501 \tAcc = 1.0 \tLoss = 2.912636430932822e-06\n",
      "Iteration = 25751 \tAcc = 1.0 \tLoss = 4.782713236896818e-09\n",
      "Iteration = 26001 \tAcc = 1.0 \tLoss = 0.6937209232582288\n",
      "Iteration = 26251 \tAcc = 1.0 \tLoss = 0.013726748523643452\n",
      "Iteration = 26501 \tAcc = 1.0 \tLoss = 5.710283743100057e-05\n",
      "Iteration = 26751 \tAcc = 1.0 \tLoss = 6.0378400066182605e-05\n",
      "Iteration = 27001 \tAcc = 1.0 \tLoss = 0.002198021926355266\n",
      "Iteration = 27251 \tAcc = 1.0 \tLoss = 0.006365730424019206\n",
      "Iteration = 27501 \tAcc = 1.0 \tLoss = 0.6859455806964184\n",
      "Iteration = 27751 \tAcc = 1.0 \tLoss = 5.425935916514706e-05\n",
      "Iteration = 28001 \tAcc = 1.0 \tLoss = 3.999287538466397e-05\n",
      "Iteration = 28251 \tAcc = 1.0 \tLoss = 6.8444033521357535e-06\n",
      "Iteration = 28501 \tAcc = 1.0 \tLoss = 1.051912813779129e-06\n",
      "Iteration = 28751 \tAcc = 1.0 \tLoss = 0.003291734650309549\n",
      "Iteration = 29001 \tAcc = 1.0 \tLoss = 0.43024225829845614\n",
      "Iteration = 29251 \tAcc = 1.0 \tLoss = 4.372503500408951e-06\n",
      "Iteration = 29501 \tAcc = 1.0 \tLoss = 1.7817785551128628e-06\n",
      "Iteration = 29751 \tAcc = 1.0 \tLoss = 2.020428719693071e-06\n",
      "Iteration = 30001 \tAcc = 1.0 \tLoss = 0.4460975756749421\n",
      "Iteration = 30251 \tAcc = 1.0 \tLoss = 0.32999634244824266\n",
      "Iteration = 30501 \tAcc = 1.0 \tLoss = 6.866958552539728e-07\n",
      "Iteration = 30751 \tAcc = 1.0 \tLoss = 6.993380321732125e-10\n",
      "Iteration = 31001 \tAcc = 1.0 \tLoss = 0.04181892784859283\n",
      "Iteration = 31251 \tAcc = 1.0 \tLoss = 2.5122635569976227e-06\n",
      "Iteration = 31501 \tAcc = 1.0 \tLoss = 1.9059531730829084e-11\n",
      "Iteration = 31751 \tAcc = 1.0 \tLoss = 4.0116965191932794e-05\n",
      "Iteration = 32001 \tAcc = 1.0 \tLoss = 8.598757409771227e-09\n",
      "Iteration = 32251 \tAcc = 1.0 \tLoss = 8.971486767231688e-06\n",
      "Iteration = 32501 \tAcc = 1.0 \tLoss = 1.4664491843171366e-11\n",
      "Iteration = 32751 \tAcc = 1.0 \tLoss = 0.020227143299616647\n",
      "Iteration = 33001 \tAcc = 1.0 \tLoss = 3.0217128803702447e-10\n",
      "Iteration = 33251 \tAcc = 1.0 \tLoss = 1.2312599517126982e-06\n",
      "Iteration = 33501 \tAcc = 1.0 \tLoss = 3.198947939298157e-05\n",
      "Iteration = 33751 \tAcc = 1.0 \tLoss = 5.719257574335866e-07\n",
      "Iteration = 34001 \tAcc = 1.0 \tLoss = 2.9491418501080994e-05\n",
      "Iteration = 34251 \tAcc = 1.0 \tLoss = 7.412757890023191e-06\n",
      "Iteration = 34501 \tAcc = 1.0 \tLoss = 0.0002197508350075361\n",
      "Iteration = 34751 \tAcc = 1.0 \tLoss = 3.8058859034693666e-07\n",
      "Iteration = 35001 \tAcc = 1.0 \tLoss = 5.077999256212236e-09\n",
      "Iteration = 35251 \tAcc = 1.0 \tLoss = 0.0011234321536105585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 35501 \tAcc = 1.0 \tLoss = 1.5800616372101802e-10\n",
      "Iteration = 35751 \tAcc = 1.0 \tLoss = 3.930078484878314e-12\n",
      "Iteration = 36001 \tAcc = 1.0 \tLoss = 0.02539612656221157\n",
      "Iteration = 36251 \tAcc = 1.0 \tLoss = 0.001681090984722847\n",
      "Iteration = 36501 \tAcc = 1.0 \tLoss = 4.211366601802077e-05\n",
      "Iteration = 36751 \tAcc = 1.0 \tLoss = 0.0013700582912151622\n",
      "Iteration = 37001 \tAcc = 1.0 \tLoss = 0.008248420594466968\n",
      "Iteration = 37251 \tAcc = 1.0 \tLoss = 3.581292960648546e-07\n",
      "Iteration = 37501 \tAcc = 1.0 \tLoss = 0.0015455309223089968\n",
      "Iteration = 37751 \tAcc = 1.0 \tLoss = 0.0010187469485047082\n",
      "Iteration = 38001 \tAcc = 1.0 \tLoss = 0.43626755964324887\n",
      "Iteration = 38251 \tAcc = 1.0 \tLoss = 0.0007356998088186914\n",
      "Iteration = 38501 \tAcc = 1.0 \tLoss = 9.5652673711212e-06\n",
      "Iteration = 38751 \tAcc = 1.0 \tLoss = 3.9729441959302904e-11\n",
      "Iteration = 39001 \tAcc = 1.0 \tLoss = 1.5657415376502986e-09\n",
      "Iteration = 39251 \tAcc = 1.0 \tLoss = 0.0021560085411305033\n",
      "Iteration = 39501 \tAcc = 1.0 \tLoss = 0.0003030627939025226\n",
      "Iteration = 39751 \tAcc = 1.0 \tLoss = 0.0014113418192739575\n",
      "Iteration = 40001 \tAcc = 1.0 \tLoss = 0.0015012958849374171\n",
      "Iteration = 40251 \tAcc = 1.0 \tLoss = 1.946165451035606e-11\n",
      "Iteration = 40501 \tAcc = 1.0 \tLoss = 0.001813305740016457\n",
      "Iteration = 40751 \tAcc = 1.0 \tLoss = 4.0239998765308026e-05\n",
      "Iteration = 41001 \tAcc = 1.0 \tLoss = 0.8066443293882424\n",
      "Iteration = 41251 \tAcc = 1.0 \tLoss = 4.139709098764842e-05\n",
      "Iteration = 41501 \tAcc = 1.0 \tLoss = 0.01291038331312912\n",
      "Iteration = 41751 \tAcc = 1.0 \tLoss = 0.6089258117517169\n",
      "Iteration = 42001 \tAcc = 1.0 \tLoss = 1.2239818935363735e-05\n",
      "Iteration = 42251 \tAcc = 1.0 \tLoss = 9.89801491028658e-06\n",
      "Iteration = 42501 \tAcc = 1.0 \tLoss = 0.0007115358024604936\n",
      "Iteration = 42751 \tAcc = 1.0 \tLoss = 0.36618465135386485\n",
      "Iteration = 43001 \tAcc = 1.0 \tLoss = 2.300382107023589e-13\n",
      "Iteration = 43251 \tAcc = 1.0 \tLoss = 1.2626421027817004e-09\n",
      "Iteration = 43501 \tAcc = 1.0 \tLoss = 0.0002542366541648453\n",
      "Iteration = 43751 \tAcc = 1.0 \tLoss = 0.014231187215775938\n",
      "Iteration = 44001 \tAcc = 1.0 \tLoss = 7.260858581048788e-14\n",
      "Iteration = 44251 \tAcc = 1.0 \tLoss = 5.677902592554095e-12\n",
      "Iteration = 44501 \tAcc = 1.0 \tLoss = 4.9960036108133294e-14\n",
      "Iteration = 44751 \tAcc = 1.0 \tLoss = 1.8308907534158024e-07\n",
      "Iteration = 45001 \tAcc = 1.0 \tLoss = 0.014306832845748983\n",
      "Iteration = 45251 \tAcc = 1.0 \tLoss = 4.2409327169395884e-05\n",
      "Iteration = 45501 \tAcc = 1.0 \tLoss = 0.0010050898375814621\n",
      "Iteration = 45751 \tAcc = 1.0 \tLoss = 3.173133117757271e-08\n",
      "Iteration = 46001 \tAcc = 1.0 \tLoss = 0.0061128991710091484\n",
      "Iteration = 46251 \tAcc = 1.0 \tLoss = 0.003764026787334936\n",
      "Iteration = 46501 \tAcc = 1.0 \tLoss = 0.1854864550202219\n",
      "Iteration = 46751 \tAcc = 1.0 \tLoss = 1.0782852248462173e-07\n",
      "Iteration = 47001 \tAcc = 1.0 \tLoss = 5.183529162800039e-10\n",
      "Iteration = 47251 \tAcc = 1.0 \tLoss = 0.3564072924662354\n",
      "Iteration = 47501 \tAcc = 1.0 \tLoss = 9.314376390416964e-07\n",
      "Iteration = 47751 \tAcc = 1.0 \tLoss = 0.007514261797095945\n",
      "Iteration = 48001 \tAcc = 1.0 \tLoss = 4.31489557246605e-05\n",
      "Iteration = 48251 \tAcc = 1.0 \tLoss = 0.007958062125005173\n",
      "Iteration = 48501 \tAcc = 1.0 \tLoss = 4.6900223105734504e-05\n",
      "Iteration = 48751 \tAcc = 1.0 \tLoss = 0.0035765914129995313\n",
      "Iteration = 49001 \tAcc = 1.0 \tLoss = 1.5737144921773973e-10\n",
      "Iteration = 49251 \tAcc = 1.0 \tLoss = 3.993827446779499e-05\n",
      "Iteration = 49501 \tAcc = 1.0 \tLoss = 2.849359833150329e-08\n",
      "Iteration = 49751 \tAcc = 1.0 \tLoss = 4.697587080020128e-06\n",
      "Iteration = 50001 \tAcc = 1.0 \tLoss = 3.9950660906714015e-05\n",
      "Iteration = 50251 \tAcc = 1.0 \tLoss = 0.7229072481221679\n",
      "Iteration = 50501 \tAcc = 1.0 \tLoss = 1.6358026044840436e-12\n",
      "Iteration = 50751 \tAcc = 1.0 \tLoss = 0.013827696896923336\n",
      "Iteration = 51001 \tAcc = 1.0 \tLoss = 0.24554175918068907\n",
      "Iteration = 51251 \tAcc = 1.0 \tLoss = 1.3593570713519656e-12\n",
      "Iteration = 51501 \tAcc = 1.0 \tLoss = 0.0009242725736580308\n",
      "Iteration = 51751 \tAcc = 1.0 \tLoss = 0.002156266929228515\n",
      "Iteration = 52001 \tAcc = 1.0 \tLoss = 3.189873843128527e-07\n",
      "Iteration = 52251 \tAcc = 1.0 \tLoss = 0.008261314017202356\n",
      "Iteration = 52501 \tAcc = 1.0 \tLoss = 2.665747194831931e-05\n",
      "Iteration = 52751 \tAcc = 1.0 \tLoss = 0.0022469262767861138\n",
      "Iteration = 53001 \tAcc = 1.0 \tLoss = 0.0023488552236422015\n",
      "Iteration = 53251 \tAcc = 1.0 \tLoss = 0.454108368496936\n",
      "Iteration = 53501 \tAcc = 1.0 \tLoss = 0.007785556969309309\n",
      "Iteration = 53751 \tAcc = 1.0 \tLoss = 1.4863375580248395e-07\n",
      "Iteration = 54001 \tAcc = 1.0 \tLoss = 0.0013721354636654747\n",
      "Iteration = 54251 \tAcc = 1.0 \tLoss = 0.0009308598725705525\n",
      "Iteration = 54501 \tAcc = 1.0 \tLoss = 1.042885092967262e-06\n",
      "Iteration = 54751 \tAcc = 1.0 \tLoss = 0.004338064061742972\n",
      "Iteration = 55001 \tAcc = 1.0 \tLoss = 0.12452119562990438\n",
      "Iteration = 55251 \tAcc = 1.0 \tLoss = 2.2204460492503154e-15\n",
      "Iteration = 55501 \tAcc = 1.0 \tLoss = 0.0019473145611205093\n",
      "Iteration = 55751 \tAcc = 1.0 \tLoss = 2.0252029143794506e-05\n",
      "Iteration = 56001 \tAcc = 1.0 \tLoss = 1.0688117058637562e-10\n",
      "Iteration = 56251 \tAcc = 1.0 \tLoss = 3.90909526970594e-13\n",
      "Iteration = 56501 \tAcc = 1.0 \tLoss = 0.00180345973947054\n",
      "Iteration = 56751 \tAcc = 1.0 \tLoss = 1.1772448515786992e-05\n",
      "Iteration = 57001 \tAcc = 1.0 \tLoss = 2.1094237467877998e-15\n",
      "Iteration = 57251 \tAcc = 1.0 \tLoss = 7.75854579592652e-08\n",
      "Iteration = 57501 \tAcc = 1.0 \tLoss = 6.823963616630864e-11\n",
      "Iteration = 57751 \tAcc = 1.0 \tLoss = 1.0544011666030708e-05\n",
      "Iteration = 58001 \tAcc = 1.0 \tLoss = 6.65173471880916e-11\n",
      "Iteration = 58251 \tAcc = 1.0 \tLoss = 0.004751373339524853\n",
      "Iteration = 58501 \tAcc = 1.0 \tLoss = 1.9047752722666735e-06\n",
      "Iteration = 58751 \tAcc = 1.0 \tLoss = 2.177147351290169e-13\n",
      "Iteration = 59001 \tAcc = 1.0 \tLoss = 4.850253532258039e-11\n",
      "Iteration = 59251 \tAcc = 1.0 \tLoss = 0.0029838803609362435\n",
      "Iteration = 59501 \tAcc = 1.0 \tLoss = 0.0014905574804381866\n",
      "Iteration = 59751 \tAcc = 1.0 \tLoss = 0.00517841086925916\n",
      "Iteration = 60001 \tAcc = 1.0 \tLoss = 0.0016644264524147436\n",
      "Iteration = 60251 \tAcc = 1.0 \tLoss = 2.236044450099894e-06\n",
      "Iteration = 60501 \tAcc = 1.0 \tLoss = 1.1066952556137242e-07\n",
      "Iteration = 60751 \tAcc = 1.0 \tLoss = 0.0013027705741328727\n",
      "Iteration = 61001 \tAcc = 1.0 \tLoss = 0.0010409359537286028\n",
      "Iteration = 61251 \tAcc = 1.0 \tLoss = 9.993653149230015e-08\n",
      "Iteration = 61501 \tAcc = 1.0 \tLoss = 0.0098960866501955\n",
      "Iteration = 61751 \tAcc = 1.0 \tLoss = 7.039884406839752e-06\n",
      "Iteration = 62001 \tAcc = 1.0 \tLoss = 0.0014695697197052238\n",
      "Iteration = 62251 \tAcc = 1.0 \tLoss = 4.440892098500627e-16\n",
      "Iteration = 62501 \tAcc = 1.0 \tLoss = 0.0003827587068795779\n",
      "Iteration = 62751 \tAcc = 1.0 \tLoss = 4.861456200307951e-08\n",
      "Iteration = 63001 \tAcc = 1.0 \tLoss = 1.1309440257651746e-07\n",
      "Iteration = 63251 \tAcc = 1.0 \tLoss = 0.0046292932837122495\n",
      "Iteration = 63501 \tAcc = 1.0 \tLoss = 7.509104449382902e-12\n",
      "Iteration = 63751 \tAcc = 1.0 \tLoss = 0.0006277268133317588\n",
      "Iteration = 64001 \tAcc = 1.0 \tLoss = 0.008162399398192228\n",
      "Iteration = 64251 \tAcc = 1.0 \tLoss = 6.422973264384545e-12\n",
      "Iteration = 64501 \tAcc = 1.0 \tLoss = 3.561761216112411e-08\n",
      "Iteration = 64751 \tAcc = 1.0 \tLoss = 5.51780843238718e-14\n",
      "Iteration = 65001 \tAcc = 1.0 \tLoss = 0.0009249051014270705\n",
      "Iteration = 65251 \tAcc = 1.0 \tLoss = 6.875550276679318e-06\n",
      "Iteration = 65501 \tAcc = 1.0 \tLoss = 0.004324894790230053\n",
      "Iteration = 65751 \tAcc = 1.0 \tLoss = 6.026290577683508e-12\n",
      "Iteration = 66001 \tAcc = 1.0 \tLoss = 5.561950240597211e-07\n",
      "Iteration = 66251 \tAcc = 1.0 \tLoss = 1.350267584644476e-08\n",
      "Iteration = 66501 \tAcc = 1.0 \tLoss = 4.6719295099360345e-12\n",
      "Iteration = 66751 \tAcc = 1.0 \tLoss = 6.095536308752169e-06\n",
      "Iteration = 67001 \tAcc = 1.0 \tLoss = 0.0012593515043154014\n",
      "Iteration = 67251 \tAcc = 1.0 \tLoss = 6.306947988307784e-06\n",
      "Iteration = 67501 \tAcc = 1.0 \tLoss = 1.1102230246251565e-16\n",
      "Iteration = 67751 \tAcc = 1.0 \tLoss = 2.220446049250313e-16\n",
      "Iteration = 68001 \tAcc = 1.0 \tLoss = 4.4695771547080436e-07\n",
      "Iteration = 68251 \tAcc = 1.0 \tLoss = 0.007302217994855475\n",
      "Iteration = 68501 \tAcc = 1.0 \tLoss = 0.0005997285178261575\n",
      "Iteration = 68751 \tAcc = 1.0 \tLoss = 0.001980494361242168\n",
      "Iteration = 69001 \tAcc = 1.0 \tLoss = 5.583842125321862e-06\n",
      "Iteration = 69251 \tAcc = 1.0 \tLoss = 2.588237132056056e-07\n",
      "Iteration = 69501 \tAcc = 1.0 \tLoss = 0.27892350264317656\n",
      "Iteration = 69751 \tAcc = 1.0 \tLoss = 0.0032840539195771556\n",
      "Iteration = 70001 \tAcc = 1.0 \tLoss = 2.4868995751603816e-14\n",
      "Iteration = 70251 \tAcc = 1.0 \tLoss = 0.0011258814571703667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 70501 \tAcc = 1.0 \tLoss = 1.1102230246251565e-16\n",
      "Iteration = 70751 \tAcc = 1.0 \tLoss = 0.0010251835510355091\n",
      "Iteration = 71001 \tAcc = 1.0 \tLoss = 2.4646951146678778e-14\n",
      "Iteration = 71251 \tAcc = 1.0 \tLoss = 8.654299499293005e-12\n",
      "Iteration = 71501 \tAcc = 1.0 \tLoss = 1.6542323066914968e-14\n",
      "Iteration = 71751 \tAcc = 1.0 \tLoss = 0.002370929170439777\n",
      "Iteration = 72001 \tAcc = 1.0 \tLoss = 8.106444407917651e-10\n",
      "Iteration = 72251 \tAcc = 1.0 \tLoss = 4.527736155161061e-06\n",
      "Iteration = 72501 \tAcc = 1.0 \tLoss = 0.0034166220697241048\n",
      "Iteration = 72751 \tAcc = 1.0 \tLoss = 0.19071599638142903\n",
      "Iteration = 73001 \tAcc = 1.0 \tLoss = 0.09590698676329354\n",
      "Iteration = 73251 \tAcc = 1.0 \tLoss = 0.0009029406695642633\n",
      "Iteration = 73501 \tAcc = 1.0 \tLoss = 0.09918234011652279\n",
      "Iteration = 73751 \tAcc = 1.0 \tLoss = 1.3766765505352036e-14\n",
      "Iteration = 74001 \tAcc = 1.0 \tLoss = 0.103176611750752\n",
      "Iteration = 74251 \tAcc = 1.0 \tLoss = 4.42424381020031e-06\n",
      "Iteration = 74501 \tAcc = 1.0 \tLoss = 0.001353761413516696\n",
      "Iteration = 74751 \tAcc = 1.0 \tLoss = 6.971266899512201e-10\n",
      "Iteration = 75001 \tAcc = 1.0 \tLoss = 2.481357288855613e-07\n",
      "Iteration = 75251 \tAcc = 1.0 \tLoss = 3.3359992958076328e-06\n",
      "Iteration = 75501 \tAcc = 1.0 \tLoss = 0.0012890115695565754\n",
      "Iteration = 75751 \tAcc = 1.0 \tLoss = 0.0010021609683224955\n",
      "Iteration = 76001 \tAcc = 1.0 \tLoss = 1.0794698468436223e-12\n",
      "Iteration = 76251 \tAcc = 1.0 \tLoss = 3.0116466947703683e-06\n",
      "Iteration = 76501 \tAcc = 1.0 \tLoss = 4.801391532798269e-06\n",
      "Iteration = 76751 \tAcc = 1.0 \tLoss = 0.10154263341345134\n",
      "Iteration = 77001 \tAcc = 1.0 \tLoss = 0.0008779337810874623\n",
      "Iteration = 77251 \tAcc = 1.0 \tLoss = 0.0925030622222877\n",
      "Iteration = 77501 \tAcc = 1.0 \tLoss = 3.51751960892607e-12\n",
      "Iteration = 77751 \tAcc = 1.0 \tLoss = 0.0014453304366853874\n",
      "Iteration = 78001 \tAcc = 1.0 \tLoss = 1.537698058494497e-08\n",
      "Iteration = 78251 \tAcc = 1.0 \tLoss = 0.00022864387222386347\n",
      "Iteration = 78501 \tAcc = 1.0 \tLoss = 0.0008045971198480724\n",
      "Iteration = 78751 \tAcc = 1.0 \tLoss = 0.005819183612008976\n",
      "Iteration = 79001 \tAcc = 1.0 \tLoss = 8.854923275875818e-08\n",
      "Iteration = 79251 \tAcc = 1.0 \tLoss = 0.002708183510087148\n",
      "Iteration = 79501 \tAcc = 1.0 \tLoss = 5.440092820663282e-15\n",
      "Iteration = 79751 \tAcc = 1.0 \tLoss = 5.1070259132757335e-15\n",
      "Iteration = 80001 \tAcc = 1.0 \tLoss = 0.005441250999549882\n",
      "Iteration = 80251 \tAcc = 1.0 \tLoss = 3.9968028886505714e-15\n",
      "Iteration = 80501 \tAcc = 1.0 \tLoss = 2.70950956747629e-06\n",
      "Iteration = 80751 \tAcc = 1.0 \tLoss = 0.005143179536972194\n",
      "Iteration = 81001 \tAcc = 1.0 \tLoss = 0.001362780759549411\n",
      "Iteration = 81251 \tAcc = 1.0 \tLoss = 4.067034906842552e-06\n",
      "Iteration = 81501 \tAcc = 1.0 \tLoss = 3.2667135668524836e-10\n",
      "Iteration = 81751 \tAcc = 1.0 \tLoss = 2.594136156901343e-06\n",
      "Iteration = 82001 \tAcc = 1.0 \tLoss = 0.0006849329873448129\n",
      "Iteration = 82251 \tAcc = 1.0 \tLoss = 0.0010832946146492961\n",
      "Iteration = 82501 \tAcc = 1.0 \tLoss = 5.987432771805262e-13\n",
      "Iteration = 82751 \tAcc = 1.0 \tLoss = 0.002069960998907038\n",
      "Iteration = 83001 \tAcc = 1.0 \tLoss = 6.55808740646295e-13\n",
      "Iteration = 83251 \tAcc = 1.0 \tLoss = 0.00047734781393312173\n",
      "Iteration = 83501 \tAcc = 1.0 \tLoss = 4.037938073779876e-06\n",
      "Iteration = 83751 \tAcc = 1.0 \tLoss = 5.129625811343674e-08\n",
      "Iteration = 84001 \tAcc = 1.0 \tLoss = 2.2724934182491562e-09\n",
      "Iteration = 84251 \tAcc = 1.0 \tLoss = 2.7191116029145775e-10\n",
      "Iteration = 84501 \tAcc = 1.0 \tLoss = 1.1197973809026353e-07\n",
      "Iteration = 84751 \tAcc = 1.0 \tLoss = 0.004952140466232936\n",
      "Iteration = 85001 \tAcc = 1.0 \tLoss = 0.00048380345165363787\n",
      "Iteration = 85251 \tAcc = 1.0 \tLoss = 0.0006204743528660781\n",
      "Iteration = 85501 \tAcc = 1.0 \tLoss = 4.0360998943001165e-06\n",
      "Iteration = 85751 \tAcc = 1.0 \tLoss = 2.5535129566378632e-15\n",
      "Iteration = 86001 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 86251 \tAcc = 1.0 \tLoss = 1.2639889135365396e-12\n",
      "Iteration = 86501 \tAcc = 1.0 \tLoss = 2.3314683517128315e-15\n",
      "Iteration = 86751 \tAcc = 1.0 \tLoss = 0.0004666834605145546\n",
      "Iteration = 87001 \tAcc = 1.0 \tLoss = 2.5535129566378632e-15\n",
      "Iteration = 87251 \tAcc = 1.0 \tLoss = 1.991466105118022e-09\n",
      "Iteration = 87501 \tAcc = 1.0 \tLoss = 3.7333836508706943e-06\n",
      "Iteration = 87751 \tAcc = 1.0 \tLoss = 0.0007001569430026693\n",
      "Iteration = 88001 \tAcc = 1.0 \tLoss = 0.0005434373985481813\n",
      "Iteration = 88251 \tAcc = 1.0 \tLoss = 1.0621503676594513e-12\n",
      "Iteration = 88501 \tAcc = 1.0 \tLoss = 9.406919687653376e-13\n",
      "Iteration = 88751 \tAcc = 1.0 \tLoss = 1.4937755488163757e-09\n",
      "Iteration = 89001 \tAcc = 1.0 \tLoss = 2.54883457816102e-06\n",
      "Iteration = 89251 \tAcc = 1.0 \tLoss = 0.0012786824787569184\n",
      "Iteration = 89501 \tAcc = 1.0 \tLoss = 0.0004987300839008301\n",
      "Iteration = 89751 \tAcc = 1.0 \tLoss = 2.652153748275904e-06\n",
      "Iteration = 90001 \tAcc = 1.0 \tLoss = 2.3578157232236705e-09\n",
      "Iteration = 90251 \tAcc = 1.0 \tLoss = 9.893197372439663e-13\n",
      "Iteration = 90501 \tAcc = 1.0 \tLoss = 0.0021047387783626583\n",
      "Iteration = 90751 \tAcc = 1.0 \tLoss = 3.370637102762543e-13\n",
      "Iteration = 91001 \tAcc = 1.0 \tLoss = 1.457104105102783e-09\n",
      "Iteration = 91251 \tAcc = 1.0 \tLoss = 0.13895728493697562\n",
      "Iteration = 91501 \tAcc = 1.0 \tLoss = 2.685470627259306e-09\n",
      "Iteration = 91751 \tAcc = 1.0 \tLoss = 2.837060338044621e-08\n",
      "Iteration = 92001 \tAcc = 1.0 \tLoss = 0.0018137161381534613\n",
      "Iteration = 92251 \tAcc = 1.0 \tLoss = 0.0016867546813186713\n",
      "Iteration = 92501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 92751 \tAcc = 1.0 \tLoss = 1.6653345369377362e-15\n",
      "Iteration = 93001 \tAcc = 1.0 \tLoss = 0.001200175911849465\n",
      "Iteration = 93251 \tAcc = 1.0 \tLoss = 1.221245327087673e-15\n",
      "Iteration = 93501 \tAcc = 1.0 \tLoss = -0.0\n",
      "Iteration = 93751 \tAcc = 1.0 \tLoss = 3.500238677073903e-06\n",
      "Iteration = 94001 \tAcc = 1.0 \tLoss = 3.854651344660781e-06\n",
      "Iteration = 94251 \tAcc = 1.0 \tLoss = 2.494138720071072e-06\n",
      "Iteration = 94501 \tAcc = 1.0 \tLoss = 5.948574965943358e-13\n",
      "Iteration = 94751 \tAcc = 1.0 \tLoss = 1.5895773598927908e-09\n",
      "Iteration = 95001 \tAcc = 1.0 \tLoss = 0.0010405483567352343\n",
      "Iteration = 95251 \tAcc = 1.0 \tLoss = 3.135236741810163e-09\n",
      "Iteration = 95501 \tAcc = 1.0 \tLoss = 0.09676040285218299\n",
      "Iteration = 95751 \tAcc = 1.0 \tLoss = 2.2435351379655627e-09\n",
      "Iteration = 96001 \tAcc = 1.0 \tLoss = 3.3246301082119584e-06\n",
      "Iteration = 96251 \tAcc = 1.0 \tLoss = 0.0017265081373818638\n",
      "Iteration = 96501 \tAcc = 1.0 \tLoss = 2.2723878259694422e-08\n",
      "Iteration = 96751 \tAcc = 1.0 \tLoss = 0.06265590270929473\n",
      "Iteration = 97001 \tAcc = 1.0 \tLoss = 0.00041110063700115475\n",
      "Iteration = 97251 \tAcc = 1.0 \tLoss = 2.181718994871212e-06\n",
      "Iteration = 97501 \tAcc = 1.0 \tLoss = 0.0010491215680360534\n",
      "Iteration = 97751 \tAcc = 1.0 \tLoss = 1.3731504916366959e-08\n",
      "Iteration = 98001 \tAcc = 1.0 \tLoss = 0.003291073885935593\n",
      "Iteration = 98251 \tAcc = 1.0 \tLoss = 0.000502985130094633\n",
      "Iteration = 98501 \tAcc = 1.0 \tLoss = 0.00046890738224997827\n",
      "Iteration = 98751 \tAcc = 1.0 \tLoss = 0.0010887329272436176\n",
      "Iteration = 99001 \tAcc = 1.0 \tLoss = 0.00039561820532255733\n",
      "Iteration = 99251 \tAcc = 1.0 \tLoss = 2.5580052866778616e-06\n",
      "Iteration = 99501 \tAcc = 1.0 \tLoss = 1.2030468915715331e-08\n",
      "Iteration = 99751 \tAcc = 1.0 \tLoss = 0.0004021913461515541\n",
      "-3.46493986 -3.41956036 3.39710997 2.0597278\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Sequential at 0x11d9f1790>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST 3: you should achieve 100% accuracy on the hard dataset (note that we provided plotting code)\n",
    "X, Y = hard()\n",
    "nn = Sequential([Linear(2, 10), ReLU(), Linear(10, 10), ReLU(), Linear(10,2), SoftMax()], NLL())\n",
    "disp.classify(X, Y, nn, it=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "MaWfgC7Qe3ar"
   },
   "outputs": [],
   "source": [
    "# TEST 4: try calling these methods that train with a simple dataset\n",
    "def nn_tanh_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), Tanh(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
    "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
    "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
    "\n",
    "\n",
    "def nn_relu_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=2, lrate=0.005)\n",
    "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
    "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
    "\n",
    "\n",
    "def nn_pred_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
    "    Ypred = nn.forward(X)\n",
    "    return nn.modules[-1].class_fun(Ypred).tolist(), [nn.loss.forward(Ypred, Y)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "_dx-zM2y3R0z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n",
       "  [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n",
       "  [-8.47337764291184e-12, 2.6227368810847106e-09, 0.00017353185263155828]],\n",
       " [[0.544808855557535, -0.08366117689965663],\n",
       "  [-0.06331837550937104, 0.24078409926389266],\n",
       "  [0.08677202043839037, 0.8360167748667923],\n",
       "  [-0.0037249480614718, 0.0037249480614718]]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_tanh_test()\n",
    "\n",
    "# Expected output:\n",
    "# '''\n",
    "# [[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n",
    "#   [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n",
    "#   [-8.47337764291184e-12, 2.6227368810847106e-09, 0.00017353185263155828]],\n",
    "#  [[0.544808855557535, -0.08366117689965663],\n",
    "#   [-0.06331837550937104, 0.24078409926389266],\n",
    "#   [0.08677202043839037, 0.8360167748667923],\n",
    "#   [-0.0037249480614718, 0.0037249480614718]]]\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "WmYT9IWk3TQL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1 \tAcc = 1.0 \tLoss = 0.1491149875248131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n",
       "  [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n",
       "  [-0.0027754917572235106, 0.001212351486908601, -0.0005239629389906042]],\n",
       " [[0.501769700845158, -0.040622022187279644],\n",
       "  [-0.09260786974986723, 0.27007359350438886],\n",
       "  [0.08364438851530624, 0.8391444067898763],\n",
       "  [-0.004252310922204504, 0.004252310922204505]]]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_relu_test()\n",
    "\n",
    "# Expected output:\n",
    "# '''\n",
    "# [[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n",
    "#   [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n",
    "#   [-0.0027754917572235106, 0.001212351486908601, -0.0005239629389906042]],\n",
    "#  [[0.501769700845158, -0.040622022187279644],\n",
    "#   [-0.09260786974986723, 0.27007359350438886],\n",
    "#   [0.08364438851530624, 0.8391444067898763],\n",
    "#   [-0.004252310922204504, 0.004252310922204505]]]\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "uo_woDFh3a2v",
    "outputId": "dc8b59c0-3ae0-447c-ac5c-4a25d010fc46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 0], [8.565750618357672])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_pred_test()\n",
    "\n",
    "# Expected output:\n",
    "# '''\n",
    "# ([0, 0, 0, 0], [8.56575061835767])\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
