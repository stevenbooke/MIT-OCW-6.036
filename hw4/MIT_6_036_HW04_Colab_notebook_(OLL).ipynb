{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIT 6.036 Spring 2019: Homework 4\n",
    "This homework does not include provided Python code. Instead, we\n",
    "encourage you to write your own code to help you answer some of these\n",
    "problems, and/or test and debug the code components we do ask for.\n",
    "Some of the problems below are simple enough that hand calculation\n",
    "should be possible; your hand solutions can serve as test cases for\n",
    "your code.  You may also find that including utilities written in\n",
    "previous labs (like a `sd` or signed distance function) will be\n",
    "helpful, as you build up additional functions and utilities for\n",
    "calculation of margins, different loss functions, gradients, and other\n",
    "functions needed for margin maximization and gradient descent.\n",
    "\n",
    "## 1) Margin\n",
    "When we train a classifier, it is desirable for the classifier to have a large margin with regard to the points in our data set, in the hope that this will make the classifier more robust to any new points we might see.\n",
    "\n",
    "We have previously defined the margin of a single example (a single data point) with respect to a separator, but that does not directly indicate whether a separator will perform well on a large data set. Thus, we would like to find a score function $S$ for a separator  $(\\theta,\\theta_0)$ such that maximizing $S$ leads to a better separator.\n",
    "\n",
    "Marge Inovera suggests that because big margins are good, we should maximize the sum of the margins. So, she defines:\n",
    "\n",
    "$$ S_{sum}(\\theta, \\theta_0) = \\sum_{i} \\gamma(x^{(i)}, y^{(i)}, \\theta, \\theta_0) $$\n",
    "\n",
    "Minnie Malle suggests that it would be better to just worry about the points closest to the margin, and defines:\n",
    "\n",
    "$$ S_{min}(\\theta, \\theta_0) = \\min_{i} \\gamma(x^{(i)}, y^{(i)}, \\theta, \\theta_0) $$\n",
    "\n",
    "Maxim Argent suggests:\n",
    "\n",
    "$$ S_{max}(\\theta, \\theta_0) = \\max_{i} \\gamma(x^{(i)}, y^{(i)}, \\theta, \\theta_0) $$\n",
    "\n",
    "Recall that the margin of a given point is defined as:\n",
    "\n",
    "$$ \\gamma(x, y, \\theta, \\theta_0) = \\frac {y(\\theta \\cdot x + \\theta_0)}{\\lVert \\theta \\rVert} $$\n",
    "\n",
    "Consider the following data, and two potential separators (red and blue).\n",
    "\n",
    "```\n",
    "data = np.array([[1, 2, 1, 2, 10, 10.3, 10.5, 10.7],\n",
    "                 [1, 1, 2, 2,  2,  2,  2, 2]])\n",
    "labels = np.array([[-1, -1, 1, 1, 1, 1, 1, 1]])\n",
    "blue_th = np.array([[0, 1]]).T\n",
    "blue_th0 = -1.5\n",
    "red_th = np.array([[1, 0]]).T\n",
    "red_th0 = -2.5\n",
    "```\n",
    "\n",
    "The situation is illustrated in the figure below.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "## 1A) \n",
    "What are the values of each score $ S_{sum}(\\theta, \\theta_0), \\ S_{min}(\\theta, \\theta_0), \\ S_{max}(\\theta, \\theta_0) $ on the red separator? Enter a Python list of three numbers.\n",
    "\n",
    "$[31.5, -1.5, 8.2]$\n",
    "\n",
    "## 1B) \n",
    "What are the values of each score $ S_{sum}(\\theta, \\theta_0), \\ S_{min}(\\theta, \\theta_0), \\ S_{max}(\\theta, \\theta_0) $ on the blue separator? Enter a Python list of three numbers.\n",
    "\n",
    "$[4.0, 0.5, 0.5]$\n",
    "\n",
    "## 1C) \n",
    "Which of these separators maximizes $ S_{sum} $?\n",
    "\n",
    "Red.\n",
    "\n",
    "## 1D) \n",
    "Which of these separators maximizes $ S_{min} $?\n",
    "\n",
    "Blue.\n",
    "\n",
    "## 1E) \n",
    "Which of these separators maximizes $ S_{max} $?\n",
    "\n",
    "Red.\n",
    "\n",
    "## 1F)\n",
    "Which score function should we prefer if our goal is to find a separator that generalizes better to new data?\n",
    "\n",
    "$S_{min}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31.5, -1.5, 8.2]\n",
      "[4.0, 0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sd(x, theta, theta_0):\n",
    "    return (theta.T@x + theta_0) / np.linalg.norm(theta)\n",
    "\n",
    "def margin(x, y, theta, theta_0):\n",
    "    return y*sd(x, theta, theta_0)\n",
    "\n",
    "def sum_of_margins(x, y, theta, theta_0):\n",
    "    return np.sum(margin(x, y, theta, theta_0))\n",
    "    \n",
    "def min_of_margins(x, y, theta, theta_0):\n",
    "    return np.min(margin(x, y, theta, theta_0))\n",
    "    \n",
    "def max_of_margins(x, y, theta, theta_0):\n",
    "    return np.max(margin(x, y, theta, theta_0))\n",
    "\n",
    "data = np.array([[1, 2, 1, 2, 10, 10.3, 10.5, 10.7],\n",
    "                 [1, 1, 2, 2,  2,  2,  2, 2]])\n",
    "labels = np.array([[-1, -1, 1, 1, 1, 1, 1, 1]])\n",
    "blue_th = np.array([[0, 1]]).T\n",
    "blue_th0 = -1.5\n",
    "red_th = np.array([[1, 0]]).T\n",
    "red_th0 = -2.5\n",
    "\n",
    "# 1A\n",
    "score_functions = [sum_of_margins, min_of_margins, max_of_margins]\n",
    "scores = []\n",
    "for score_function in score_functions:\n",
    "    scores += [score_function(data, labels, red_th, red_th0)]\n",
    "print(scores)\n",
    "\n",
    "# 1B\n",
    "scores = []\n",
    "for score_function in score_functions:\n",
    "    scores += [score_function(data, labels, blue_th, blue_th0)]\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) What a loss\n",
    "Based on the previous part, we've decided to try to find a linear separator $(\\theta,\\theta_0)$ that __maximizes__ the __minimum margin__ (the distance between the separator and the points that come closest to it.) We define the margin of a data set $(X,Y)$, with respect to a separator as:\n",
    "\n",
    "$$ \\gamma(X, Y, \\theta, \\theta_0) = \\min_{i} \\gamma(x^{(i)}, y^{(i)}, \\theta, \\theta_0) $$\n",
    "\n",
    "As discussed in the [notes](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week4/margin_maximization/3), an approach to this problem is to specify a value $\\gamma_{ref}$ for the margin of the data set, and then seek to find a linear separator that maximizes $\\gamma_{ref}$.\n",
    "\n",
    "## 2A)\n",
    "We can think about a (not necessarily maximal) margin $\\gamma_{ref}$ for the data set as a value such that:\n",
    "\n",
    "For every point $x^{(i)}, y^{(i)}$, we have $\\gamma(x^{(i)}, y^{(i)}, \\theta, \\theta_0) \\geq \\gamma_{ref}$\n",
    "\n",
    "## 2B)\n",
    "Suppose for our data set we find that the maximum $\\gamma_{ref}$ across all linear separators is 0. Is our data linearly separable?\n",
    "\n",
    "No.\n",
    "\n",
    "\n",
    "## 2C)\n",
    "For this subproblem, assume that $\\gamma_{ref} \\gt 0$ (i.e., the data is linearly separable). Note that in this case, the Perceptron algorithm is guaranteed to find a separator that correctly classifies all of the data points. What is the __largest minimum margin__ guaranteed by running the Perceptron algorithm on a data set that has a maximum margin equal to $\\gamma_{ref} \\gt 0$?\n",
    "\n",
    "Some $\\epsilon$ where $\\epsilon \\gt 0$\n",
    "\n",
    "Now we want to improve on the (infinitesimally small) guaranteed margin of the Perceptron algorithm. We saw in the lecture that a powerful way of designing learning algorithms is to [describe them as optimization problems](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week4/margin_maximization/1), then use relatively general-purpose optimization strategies to solve them.\n",
    "\n",
    "A typical form of the optimization problem is to minimize an objective that has the form:\n",
    "\n",
    "$$ J(\\theta, \\theta_0) = \\frac{1}{n} \\sum_{i=1}^{n} L(x^{(i)}, y^{(i)}, \\theta, \\theta_0) + \\lambda R(\\theta, \\theta_0) $$\n",
    "\n",
    "where $L$ is a per-point loss function that characterizes how much error was made by the hypothesis $(\\theta, \\theta_0)$ on the point, and $R$ is a regularizer that describes some prior knowledge or general preference over hypotheses.\n",
    "\n",
    "We first consider the objective of finding a maximum-margin separator using the format above, using the so-called \"zero-infinity\" loss, $L_{0, \\infty}$:\n",
    "\n",
    "$$ L_{0, \\infty}(\\gamma(x^{(i)}, y^{(i)}, \\theta, \\theta_0), \\gamma_{ref}) = \n",
    "\\begin{cases}\n",
    "    \\infty, & \\text{if } \\gamma(x, y, \\theta, \\theta_0) \\lt \\gamma_{ref} \\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases} $$\n",
    "\n",
    "## 2D)\n",
    "For a linearly separable data set, positive $\\lambda$ and positive $R$ given nonzero $\\theta$ what is true about the __minimal__ value of $J_{0,\\infty}$?\n",
    "\n",
    "It is always finite and positive.\n",
    "\n",
    "## 2E)\n",
    "For a __non__ linearly separable data set, and positive $\\lambda$ and $\\gamma_{ref}$, what is true about the __minimal__ value of $J_{0,\\infty}$?\n",
    "\n",
    "It is infinite.\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Simply inseparable\n",
    "We would prefer a loss function that helps steer optimization toward a solution, in the case when the data is linearly separable. Furthermore, in real data sets it is relatively rare that the data is linearly separable, so our algorithm should be able to handle this case also and still work toward an optimal, though imperfect, linear separator. Instead of using $(0, \\infty)$ loss, we should design a loss function that will let us \"relax\" the constraint that all of the points have margin bigger than $\\gamma_{ref}$, while still encouraging large margins.\n",
    "\n",
    "The hinge loss is one such more relaxed loss function; we will define it in a way that makes a connection to the problem we are facing:\n",
    "\n",
    "$$ L_{h}(\\frac{\\gamma(x, y, \\theta, \\theta_0)}{\\gamma_{ref}}) =\n",
    "\\begin{cases}\n",
    "    1 - \\frac{\\gamma(x, y, \\theta, \\theta_0)}{\\gamma_{ref}}, & \\text{if } \\gamma(x, y, \\theta, \\theta_0) \\lt \\gamma_{ref} \\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases} $$\n",
    "\n",
    "When the margin of the point is greater than or equal to $\\gamma_{ref}$, we are happy and the loss is 0; while when the margin is less than $\\gamma_{ref}$, we have a positive loss that increases the further away the margin is from $\\gamma_{ref}$.\n",
    "\n",
    "## 3A) \n",
    "Given this definition, if $\\gamma_{ref}$ is positive what can we say about $L_{h}(\\frac{\\gamma(x, y, \\theta, \\theta_0)}{\\gamma_{ref}})$, no matter what finite values $\\theta$ and $\\theta_0$ take on?\n",
    "\n",
    "It is always >= 0.\n",
    "\n",
    "Here is a separator and three points. The dotted lines represent the margins determined by $\\gamma_{ref}$.\n",
    "\n",
    "```\n",
    "data = np.array([[1.1, 1, 4],[3.1, 1, 2]])\n",
    "labels = np.array([[1, -1, -1]])\n",
    "th = np.array([[1, 1]]).T\n",
    "th0 = -4\n",
    "```\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "## 3B)\n",
    "What is $ L_{h}(\\frac{\\gamma(x, y, \\theta, \\theta_0)}{\\gamma_{ref}})$ for each point, where $\\gamma_{ref} = \\frac{\\sqrt{2}}{2}$? Enter the values in the same order as the respective points are listed in data. You can do this computationally or by hand. Enter the three hinge loss values in order as a Python list of three numbers:\n",
    "\n",
    "$[0.8, 0.,  3. ]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8 0.  3. ]]\n"
     ]
    }
   ],
   "source": [
    "# 3B\n",
    "def hinge_losses(x, y, theta, theta_0, margin_ref):\n",
    "    loss = np.vectorize(lambda x: 1 - (x/margin_ref) if x < margin_ref else 0)\n",
    "    return loss(margin(x, y, theta, theta_0))\n",
    "\n",
    "data = np.array([[1.1, 1, 4],[3.1, 1, 2]])\n",
    "labels = np.array([[1, -1, -1]])\n",
    "th = np.array([[1, 1]]).T\n",
    "th0 = -4\n",
    "margin_ref = (2**0.5)/2\n",
    "\n",
    "print(hinge_losses(data, labels, th, th0, margin_ref))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) It hinges on the loss\n",
    "Putting hinge loss and regularization together, we can look at regularized average hinge loss:\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{i=1}^{n} L_{h}(\\frac{\\gamma(x^{(i)}, y^{(i)}, \\theta, \\theta_0}{\\gamma_{ref}}) + \\lambda \\frac{1}{\\gamma_{ref}^2} $$\n",
    "\n",
    "We only need to minimize this over two parameters $\\theta, \\theta_0$, since the third parameter $\\gamma_{ref}$ can be expressed as $\\frac{1}{\\lVert \\theta \\rVert}$, as they both represent the distance from the decision boundary to the margin boundary. Plugging in $\\gamma_{ref} = \\frac{1}{\\lVert \\theta \\rVert}$ and also expanding $\\gamma$, we arrive at the SVM (support vector machine) objective:\n",
    "\n",
    "$$ J(\\theta, \\theta_0) = \\frac{1}{n} \\sum_{i=1}^{n} L_{h}(y^{(i)}(\\theta^Tx^{(i)} + \\theta_0)) +\\lambda \\lVert \\theta \\rVert ^2$$\n",
    "\n",
    "## 4A)\n",
    "If the data is linearly separable and we use the SVM objective, if we now let $\\lambda = 0$ and find the minimizing values of $\\theta, \\theta_0$, what will happen?\n",
    "\n",
    "The minimal objective value will be 0.\n",
    "\n",
    "## 4B) \n",
    "Consider the following plots of separators. They are for $\\lambda$ values of 0 and 0.001. Match each $\\lambda$ to a plot. Enter a Python list with the values of $\\lambda$ for the two graphs below.\n",
    "\n",
    "$[0.001, 0]$\n",
    "\n",
    "<div style=\"display:flex;\"><img src=\"https://introml_oll.odl.mit.edu/cat-soop/_static/6.036/homework/hw04/lamb_A.png\" style=\"width:50%;\"><img src=\"https://introml_oll.odl.mit.edu/cat-soop/_static/6.036/homework/hw04/lamb_B.png\" style=\"width:50%;\"></div>\n",
    "\n",
    "## 4C)\n",
    "Consider the following three plots of separators. They are for $\\lambda$ values of 0, 0.001, and 0.03. Match to the plot. Enter a Python list with the values of $\\lambda$ for the three graphs below.\n",
    "\n",
    "$[0.03, 0, 0.001]$\n",
    "\n",
    "<div style=\"display:flex;\"><img src=\"https://introml_oll.odl.mit.edu/cat-soop/_static/6.036/homework/hw04/bada_A.png\" style=\"width:33%;\"><img src=\"https://introml_oll.odl.mit.edu/cat-soop/_static/6.036/homework/hw04/bada_B.png\" style=\"width:33%;\"><img src=\"https://introml_oll.odl.mit.edu/cat-soop/_static/6.036/homework/hw04/bada_C.png\" style=\"width:33%;\"></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.409602165190182\n",
      "2.567739315055543\n",
      "1.4204927238739404\n",
      "4.525230920153827\n",
      "4.131878940912039\n"
     ]
    }
   ],
   "source": [
    "# 4B\n",
    "print(np.linalg.norm(np.array([[-0.0737901], [2.40847205]])))\n",
    "print(np.linalg.norm(np.array([[-0.23069578], [2.55735501]])))\n",
    "\n",
    "# 4C\n",
    "print(np.linalg.norm(np.array([[0.01280916], [-1.42043497]])))\n",
    "print(np.linalg.norm(np.array([[0.45589866], [-4.50220738]])))\n",
    "print(np.linalg.norm(np.array([[0.04828952], [-4.13159675]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Linear Support Vector Machines\n",
    "The training objective for the Support Vector Machine (with slack) can be seen as optimizing a balance between the average hinge loss over the examples and a regularization term that tries to keep $\\theta$ small (or equivalently, increase the margin). This balance is set by the regularization parameter $\\lambda$. Here we only consider the case without the offset parameter $\\theta_0$  (setting it to zero) and rewrite the training objective as an average so that it is given by\n",
    "\n",
    "$$\n",
    " \\left[  \\frac{1}{n} \\sum_{i=1}^{n} L_{h}(y^{(i)}\\theta \\cdot x^{(i)}) \\right] + \\frac{\\lambda}{2}\\lVert \\theta \\rVert ^2 = \\frac{1}{n} \\sum_{i=1}^{n} \\left[ L_{h}(y^{(i)}\\theta \\cdot x^{(i)}) + \\frac{\\lambda}{2}\\lVert \\theta \\rVert ^2 \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "where $ L_{h}(y(\\theta \\cdot x)) = \\max\\left\\{0,1-y(\\theta \\cdot x)\\right\\} $ is the hinge loss. (Note that we will also sometimes write the hinge loss as $ L_{h}(v) = \\max\\left\\{0,1-v\\right\\} $. Now we can minimize the above overall objective function with the Pegasos algorithm that iteratively selects a training point at random and applies a gradient descent update rule based on the corresponding term inside the brackets on the right hand side.\n",
    "\n",
    "In this problem we will optimize the training objective using a single training example, so that we can gain a better understanding of how the regularization parameter, $\\lambda$, affects the result. To this end, we refer to the single training example as the feature vector and label pair, $(x, y)$. We will then try to find a $\\theta$ that minimizes\n",
    "\n",
    "$$ J_{\\lambda}^1(\\theta) \\equiv  L_{h}(y(\\theta \\cdot x)) + \\frac{\\lambda}{2}\\lVert \\theta \\rVert ^2 $$\n",
    "\n",
    "In the next subparts, we will try to show that the $\\theta$ minimizing $J_{\\lambda}^1$, denoted $\\hat{\\theta}$ is necessarily of the form \n",
    "\n",
    "$$\\hat{\\theta} = \\eta yx$$ \n",
    "\n",
    "for some real $\\eta \\gt 0$\n",
    "\n",
    "In the expressions below, you can use `lambda` to stand for $\\lambda$, `x` to stand for $x$, `transpose(x)` for transpose of an array, `norm(x)` for the length (norm) of a vector, `x@y` to indicate a matrix product of two arrays, and `x*y` for elementwise (or scalar) multiply.\n",
    "\n",
    "## 5A) \n",
    "Consider first the case where the loss is positive: $L_{h}(y(\\theta \\cdot x)) \\gt 0$. We can minimize $J_{\\lambda}^1$ with respect to $\\theta$ by computing a formula for its gradient with respect to $\\theta$, and then solving for the $\\theta$ for which the gradient is equal to 0. Let us denote that value as $\\hat{\\theta}$. Enter and expression for $\\hat{\\theta}$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\hat{\\theta} &= \\frac{\\partial J_{\\lambda}^1}{\\partial \\theta} \\\\\n",
    "    \\hat{\\theta} &= \\frac{\\partial }{\\partial \\theta} (1-y(\\theta \\cdot x)) + \\frac{\\partial}{\\partial \\theta} \\frac{\\lambda}{2} \\lVert \\theta \\rVert ^2 \\\\\n",
    "    \\hat{\\theta} &= -yx + 2 \\frac{\\lambda}{2} \\theta \\\\\n",
    "    \\hat{\\theta} &= -yx + 2 \\lambda \\theta\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## 5B) \n",
    "Now find the smallest (in the norm sense) $\\hat{\\theta}$ for which $L_{h}(y(\\theta \\cdot x)) = 0$\n",
    "Note: Be careful -- you cannot simply divide by a vector! Enter your answer as a Python expression.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    0 &= 1-y(\\theta \\cdot x) \\\\\n",
    "    1 &= y(\\theta \\cdot x) \\\\\n",
    "    \\frac{1}{y} &= \\theta \\cdot x \\\\\n",
    "    y \\cdot x &= \\theta (x \\cdot x) \\\\\n",
    "    y \\cdot x &= \\theta \\lVert x \\rVert ^2 \\\\\n",
    "    \\theta &= \\frac{xy}{\\lVert x \\rVert ^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "## 5C) \n",
    "Let $\\hat{\\theta} = \\hat{\\theta}(\\lambda)$ be the minimizer of $J_{\\lambda}^1$. Is it possible to pick a value for $\\lambda$ so that the training example $(x, y)$ wil be misclassified by $\\hat{\\theta}(\\lambda)$? To answer this question, recall that a point is misclassified when $y(\\theta \\cdot x) \\leq 0$. Use your result from part 5A where you found the $\\hat{\\theta}$ that minimizes $J_{\\lambda}^1$ to write an expression for $y(\\hat{\\theta} \\cdot x)$ in terms $x$, $y$ and $\\lambda$.\n",
    "\n",
    "$$ \\frac{y^2}{\\lambda}\\lVert x \\rVert ^2 $$\n",
    "\n",
    "## 5D)\n",
    "Under what conditions is $y(\\hat{\\theta} \\cdot x) \\leq 0$? Select all that are true.\n",
    "\n",
    "$\\lambda = \\infty, x = 0$\n",
    "\n",
    "## 5E) \n",
    "You will notice that if $y(\\hat{\\theta} \\cdot x) \\leq 0$, then $\\hat{\\theta}$ will misclassify $x$. The above result shows that our optimal classifier $\\hat{\\theta}$ won't misclassify (except in edge cases); however, we might still be concerned about correctly classified points that are \"too close\" to the separator, and thereby increase our regularized loss function.\n",
    "\n",
    "Suppose we have a linear classifier described by $\\theta$. We say a correctly classified datapoint $\\hat{x}, \\hat{y}$ is on the margin boundary of the classifier if \n",
    "\n",
    "$$\\hat{y}(\\theta \\cdot \\hat{x}) = 1$$\n",
    "\n",
    "When a classifier is determined by minimizing a regularized loss function with a single training example, like $J_{\\lambda}^1$ above, too much regularization can result in a classifier that puts a correctly classified training point _inside_ the margin, and thus incur hinge loss. That is, if we have a single training example $(x,y)$ and regularize with a $\\lambda$ that is too large, we may discover that $y(\\hat{\\theta} \\cdot x) \\lt 1$. Fortunately, for this single training example case, we can ensure that $\\lambda$ is not too large.\n",
    "Write an expression for the maximum value of $\\lambda$, in terms of $x$ and $y$, that ensures that the $(x,y)$ example is NOT inside the margin:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    1 & \\le \\frac{y^2 \\lVert x \\rVert ^2}{\\lambda} \\\\\n",
    "    \\lambda & \\le y^2 \\lVert x \\rVert ^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "So, where are we? We now have a good objective function, the SVM objective, that will strive to correctly classify data points, but also seek to maximize the margin (minimize the norm of $\\theta$), given a judicious choice of $\\lambda$. This objective function can be used in either batch optimization (calculating average losses across the whole data set), or on a data point by data point basis. This gives us powerful flexibility in optimizing (minimizing) this objective function using gradient descent, which we will consider next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUS51a8m5rEI"
   },
   "source": [
    "## 6) Implementing gradient descent\n",
    "In this section we will implement generic versions of gradient descent and apply these to the SVM objective.\n",
    "\n",
    "<b>Note: </b> If you need a refresher on gradient descent,\n",
    "you may want to reference\n",
    "<a href=\"https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week4/gradient_descent/2\">this week's notes</a>.\n",
    "\n",
    "### 6.1) Implementing Gradient Descent\n",
    "We want to find the $x$ that minimizes the value of the *objective\n",
    "function* $f(x)$, for an arbitrary scalar function $f$.  The function\n",
    "$f$ will be implemented as a Python function of one argument, that\n",
    "will be a numpy column vector.  For efficiency, we will work with\n",
    "Python functions that return not just the value of $f$ at $f(x)$ but\n",
    "also return the gradient vector at $x$, that is, $\\nabla_x f(x)$.\n",
    "\n",
    "We will now implement a generic gradient descent function, `gd`, that\n",
    "has the following input arguments:\n",
    "\n",
    "* `f`: a function whose input is an `x`, a column vector, and\n",
    "  returns a scalar.\n",
    "* `df`: a function whose input is an `x`, a column vector, and\n",
    "  returns a column vector representing the gradient of `f` at `x`.\n",
    "* `x0`: an initial value of $x$, `x0`, which is a column vector.\n",
    "* `step_size_fn`: a function that is given the iteration index (an\n",
    "  integer) and returns a step size.\n",
    "* `max_iter`: the number of iterations to perform\n",
    "\n",
    "Our function `gd` returns a tuple:\n",
    "\n",
    "* `x`: the value at the final step\n",
    "* `fs`: the list of values of `f` found during all the iterations (including `f(x0)`)\n",
    "* `xs`: the list of values of `x` found during all the iterations (including `x0`)\n",
    "\n",
    "**Hint:** This is a short function!\n",
    "\n",
    "**Hint 2:** If you do `temp_x = x` where `x` is a vector\n",
    "(numpy array), then `temp_x` is just another name for the same vector\n",
    "as `x` and changing an entry in one will change an entry in the other.\n",
    "You should either use `x.copy()` or remember to change entries back after modification.\n",
    "\n",
    "Some utilities you may find useful are included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fYOF0HS34YOj"
   },
   "outputs": [],
   "source": [
    "def rv(value_list):\n",
    "    return np.array([value_list])\n",
    "\n",
    "def cv(value_list):\n",
    "    return np.transpose(rv(value_list))\n",
    "\n",
    "def f1(x):\n",
    "    return float((2 * x + 3)**2)\n",
    "\n",
    "def df1(x):\n",
    "    return 2 * 2 * (2 * x + 3)\n",
    "\n",
    "def f2(v):\n",
    "    x = float(v[0]); y = float(v[1])\n",
    "    return (x - 2.) * (x - 3.) * (x + 3.) * (x + 1.) + (x + y -1)**2\n",
    "\n",
    "def df2(v):\n",
    "    x = float(v[0]); y = float(v[1])\n",
    "    return cv([(-3. + x) * (-2. + x) * (1. + x) + \\\n",
    "               (-3. + x) * (-2. + x) * (3. + x) + \\\n",
    "               (-3. + x) * (1. + x) * (3. + x) + \\\n",
    "               (-2. + x) * (1. + x) * (3. + x) + \\\n",
    "               2 * (-1. + x + y),\n",
    "               2 * (-1. + x + y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s03NFuxG6kvt"
   },
   "source": [
    "The main function to implement is `gd`, defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mNsLE3bg6jt9"
   },
   "outputs": [],
   "source": [
    "def gd(f, df, x0, step_size_fn, max_iter):\n",
    "    x = x0\n",
    "    fs = []\n",
    "    xs = []\n",
    "    for i in range(max_iter):\n",
    "        f_val = f(x)\n",
    "        gradient_at_x = df(x)\n",
    "        fs += [f_val]\n",
    "        xs += [x]\n",
    "        x = x - step_size_fn(i)*gradient_at_x\n",
    "    return (x, fs, xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXu60n-H5_Hz"
   },
   "source": [
    "To evaluate results, we also use a simple `package_ans` function,\n",
    "which checks the final `x`, as well as the first and last values in\n",
    "`fs`, `xs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GJcClaqN4nE6"
   },
   "outputs": [],
   "source": [
    "def package_ans(gd_vals):\n",
    "    x, fs, xs = gd_vals\n",
    "    return [x.tolist(), [fs[0], fs[-1]], [xs[0].tolist(), xs[-1].tolist()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aN_XbacQ6Rue"
   },
   "source": [
    "The test cases are provided below, but you should feel free (and are encouraged!) to write more of your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jq0OJLEf6Dan"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1.5]], [9.0, 0.0], [[[0.0]], [[-1.5]]]]\n",
      "[[[-2.2058239041648853], [3.205823890926977]], [19.0, -20.96723961134875], [[[0.0], [0.0]], [[-2.2058239041540233], [3.2058238906570375]]]]\n"
     ]
    }
   ],
   "source": [
    "# Test case 1\n",
    "ans=package_ans(gd(f1, df1, cv([0.]), lambda i: 0.1, 1000))\n",
    "print(ans)\n",
    "\n",
    "# Test case 2\n",
    "ans=package_ans(gd(f2, df2, cv([0., 0.]), lambda i: 0.01, 1000))\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbuSt5hY645k"
   },
   "source": [
    "### 6.2) Numerical Gradient\n",
    "Getting the analytic gradient correct for complicated functions is\n",
    "tricky.  A very handy method of verifying the analytic gradient or\n",
    "even substituting for it is to estimate the gradient at a point by\n",
    "means of *finite differences*.\n",
    "\n",
    "Assume that we are given a function $f(x)$ that takes a column vector\n",
    "as its argument and returns a scalar value.  In gradient descent, we\n",
    "will want to estimate the gradient of $f$ at a particular $x_0.$\n",
    "\n",
    "The $i^{th}$ component of $\\nabla_x f(x_0)$ can be estimated as\n",
    "$$\\frac{f(x_0+\\delta^{i}) - f(x_0-\\delta^{i})}{2\\delta}$$\n",
    "where $\\delta^{i}$ is a column vector whose $i^{th}$ coordinate is\n",
    "$\\delta$, a small constant such as 0.001, and whose other components\n",
    "are zero.\n",
    "Note that adding or subtracting $\\delta^{i}$ is the same as\n",
    "incrementing or decrementing the $i^{th}$ component of $x_0$ by\n",
    "$\\delta$, leaving the other components of $x_0$ unchanged.  Using\n",
    "these results, we can estimate the $i^{th}$ component of the gradient.\n",
    "\n",
    "For example, if $x_0 = (1,1,\\dots,1)^T$ and $\\delta = 0.01$,\n",
    "we may approximate the first component of $\\nabla_x f(x_0)$ as\n",
    "$$\\frac{f((1,1,1,\\dots)^T+(0.01,0,0,\\dots)^T) - f((1,1,1,\\dots)^T-(0.01,0,0,\\dots)^T)}{2\\cdot 0.01}.$$\n",
    "(We add the transpose so that these are column vectors.)\n",
    "**This process should be done for each dimension independently,\n",
    "and together the results of each computation are compiled to give the\n",
    "estimated gradient, which is $d$ dimensional.**\n",
    "\n",
    "Implement this as a function `num_grad` that takes as arguments the\n",
    "objective function `f` and a value of `delta`, and returns a new\n",
    "**function** that takes an `x` (a column vector of parameters) and\n",
    "returns a gradient column vector.\n",
    "\n",
    "**Note:** As in the previous part, make sure you do not modify your input vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WPVwGZ-l6XvW"
   },
   "outputs": [],
   "source": [
    "def num_grad(f, delta=0.001):\n",
    "    def df(x):\n",
    "        def estimate_gradient(column_partial):\n",
    "            d = len(column_partial)\n",
    "            column_partial = column_partial.reshape((d, 1))\n",
    "            return (f(x+column_partial) - f(x-column_partial)) / (2*delta)\n",
    "        d, _ = x.shape\n",
    "        deltas = np.eye(d)*delta\n",
    "        return np.apply_along_axis(estimate_gradient, axis=0, arr=deltas).reshape((d, 1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kElTR0bL7cbG"
   },
   "source": [
    "The test cases are shown below; these use the functions defined in the previous exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3D7BHu4S7Z8D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[11.999999999998678]], [[0.0]])\n",
      "([[12.799999999999478]], [[0.1]])\n",
      "([[6.99999899999959], [-2.000000000000668]], [[0.0], [0.0]])\n",
      "([[4.7739994000011166], [-2.000000000000668]], [[0.1], [-0.1]])\n"
     ]
    }
   ],
   "source": [
    "x = cv([0.])\n",
    "ans=(num_grad(f1)(x).tolist(), x.tolist())\n",
    "print(ans)\n",
    "\n",
    "x = cv([0.1])\n",
    "ans=(num_grad(f1)(x).tolist(), x.tolist())\n",
    "print(ans)\n",
    "\n",
    "x = cv([0., 0.])\n",
    "ans=(num_grad(f2)(x).tolist(), x.tolist())\n",
    "print(ans)\n",
    "\n",
    "x = cv([0.1, -0.1])\n",
    "ans=(num_grad(f2)(x).tolist(), x.tolist())\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WASaSsYu75sG"
   },
   "source": [
    "A faster (one function evaluation per entry), though sometimes less\n",
    "accurate, estimate is to use:\n",
    "$$\\frac{f(x_0+\\delta^{i}) - f(x_0)}{\\delta}$$\n",
    "for the $i^{th}$ component of $\\nabla_x f(x_0).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E31sdqyG78jD"
   },
   "source": [
    "### 6.3) Using the Numerical Gradient\n",
    "Recall that our generic gradient descent function takes both a function\n",
    "`f` that returns the value of our function at a given point, and `df`,\n",
    "a function that returns a gradient at a given point.  Write a function\n",
    "`minimize` that takes only a function `f` and uses this function and\n",
    "numerical gradient descent to return the local minimum.  We have\n",
    "provided you with our implementations of `num_grad` and `gd`, so you\n",
    "should not redefine them in the code box below.\n",
    "You may use the default of `delta=0.001` for `num_grad`.\n",
    "\n",
    "**Hint:** Your definition of `minimize` should call `num_grad` exactly\n",
    "once, to return a function that is called many times.\n",
    "You should return the same outputs as `gd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "CStwqDem76Bx"
   },
   "outputs": [],
   "source": [
    "def minimize(f, x0, step_size_fn, max_iter):\n",
    "    df = num_grad(f)\n",
    "    return gd(f, df, x0, step_size_fn, max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gl0FTby8EQq"
   },
   "source": [
    "The test cases are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "UxBLWJFm8DnV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1.5]], [9.0, 0.0], [[[0.0]], [[-1.5]]]]\n",
      "[[[-2.2058237062057517], [3.205823692967833]], [19.0, -20.967239611347775], [[[0.0], [0.0]], [[-2.2058237061948627], [3.205823692697898]]]]\n"
     ]
    }
   ],
   "source": [
    "ans = package_ans(minimize(f1, cv([0.]), lambda i: 0.1, 1000))\n",
    "print(ans)\n",
    "\n",
    "ans = package_ans(minimize(f2, cv([0., 0.]), lambda i: 0.01, 1000))\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BH-1e98V8LtM"
   },
   "source": [
    "## 7) Applying gradient descent to SVM objective\n",
    "\n",
    "Now that we've implemented gradient descent in the general case, let's go back to hinge loss and the SVM objective. Our goal in this section will be to derive and implement appropriate gradient calculations that we can use with gd for optimization of the SVM objective. In the derivations below, we'll consider linear classifiers _with_ offset, i.e., $\\theta, \\theta_0$.\n",
    "\n",
    "Recall that hinge loss is defined as:\n",
    "\n",
    "$$ L_{h}(v) =\n",
    "\\begin{cases}\n",
    "    1-v, & \\text{if } v \\lt 1 \\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases} $$\n",
    "\n",
    "This is usually implemented as:\n",
    "\n",
    "$$ \\it{hinge(v)} = \\max\\left\\{0,1-v\\right\\}$$\n",
    "\n",
    "The hinge loss function, in the context of our problem, takes in the distance from a point to the separator as $x$. This loss function helps us penalize a model for leaving points within a distance to our separator:\n",
    "\n",
    "$$ L_{h}(y(\\theta \\cdot x + \\theta_0)) =\n",
    "\\begin{cases}\n",
    "    1 - y(\\theta \\cdot x + \\theta_0), & \\text{if } y(\\theta \\cdot x + \\theta_0) \\lt 1 \\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases} $$\n",
    "\n",
    "The SVM objective function incorporates the mean of the hinge loss over all points and introduces a regularization term to this equation to make sure that the magnitude of $\\theta$ stays small (and keeps the margin large). Note that we have used $\\lambda$ instead of $\\frac{\\lambda}{2}$ for simplicity (and without loss of generality).\n",
    "\n",
    "$$\n",
    " J(\\theta, \\theta_0) = \\left[ \\frac{1}{n} \\sum_{i=1}^{n} L_{h}(y^{(i)} (\\theta \\cdot x^{(i)} + \\theta_0)) \\right] + \\lambda \\lVert \\theta \\rVert ^2\n",
    "$$\n",
    "\n",
    "We're interested in applying our gradient descent procedure to this function in order to find the 'best' separator for our data, where 'best' is measured by the lowest possible SVM objective.\n",
    "\n",
    "\n",
    "**Note:** In this section,\n",
    "you will code many individual functions, each of which depends on previous ones.\n",
    "We **strongly recommend** that you test each of the components on your own to debug.\n",
    "\n",
    "### 7.1) Calculating the SVM objective\n",
    "\n",
    "Implement the single-argument hinge function, which computes $L_h$,\n",
    "and use that to implement hinge loss for a data point and separator.\n",
    "Using the latter function, implement the SVM objective.\n",
    "Note that these functions should work for matrix/vector arguments,\n",
    "so that we can compute the objective for a whole dataset with one call.\n",
    "<pre> x is d x n, y is 1 x n, th is d x 1, th0 is 1 x 1, lam is a scalar </pre>\n",
    "\n",
    "Hint: Look at `np.where` for implementing `hinge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "F_6E78BF8e2W"
   },
   "outputs": [],
   "source": [
    "def hinge(v):\n",
    "    return np.where(v < 1, 1-v, 0)\n",
    "\n",
    "# x is dxn, y is 1xn, th is dx1, th0 is 1x1\n",
    "def hinge_loss(x, y, th, th0):\n",
    "    return hinge(y * ((th.T @ x) + th0))\n",
    "\n",
    "# x is dxn, y is 1xn, th is dx1, th0 is 1x1, lam is a scalar\n",
    "def svm_obj(x, y, th, th0, lam):\n",
    "    return np.mean(hinge_loss(x, y, th, th0)) + lam * np.linalg.norm(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "muPDf4etYsoy"
   },
   "outputs": [],
   "source": [
    "# add your tests here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QY1NJEOP8jCC"
   },
   "source": [
    "In the test cases for this problem, we'll use the following\n",
    "`super_simple_separable` test dataset and test separator for some of\n",
    "the tests.  A couple of the test cases are also shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "POFvK7zW8iYK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]]\n",
      "0.12517346719850858\n",
      "[[0. 0. 0. 0.]]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def super_simple_separable():\n",
    "    X = np.array([[2, 3, 9, 12],\n",
    "                  [5, 2, 6, 5]])\n",
    "    y = np.array([[1, -1, 1, -1]])\n",
    "    return X, y\n",
    "\n",
    "sep_e_separator = np.array([[-0.40338351], [1.1849563]]), np.array([[-2.26910091]])\n",
    "\n",
    "# Test case 1\n",
    "x_1, y_1 = super_simple_separable()\n",
    "th1, th1_0 = sep_e_separator\n",
    "ans = svm_obj(x_1, y_1, th1, th1_0, .1)\n",
    "print(ans)\n",
    "\n",
    "# Test case 2\n",
    "ans = svm_obj(x_1, y_1, th1, th1_0, 0.0)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjBB0R4u8tF1"
   },
   "source": [
    "### 7.2) Calculating the SVM gradient\n",
    "\n",
    "Define a function `svm_obj_grad` that returns the gradient of the SVM\n",
    "objective function with respect to $\\theta$ and $\\theta_0$ in a single\n",
    "column vector.  The last component of the gradient vector should be\n",
    "the partial derivative with respect to $\\theta_0$.  Look at\n",
    "`np.vstack` as a simple way of stacking two matrices/vectors\n",
    "vertically.  We have broken it down into pieces that mimic steps in\n",
    "the chain rule; this leads to code that is a bit inefficient but\n",
    "easier to write and debug.  We can worry about efficiency later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "lAtDiGVK8vnt"
   },
   "outputs": [],
   "source": [
    "# Returns the gradient of hinge(v) with respect to v.\n",
    "def d_hinge(v):\n",
    "    return np.where(v < 1, -1, 0)\n",
    "\n",
    "# Returns the gradient of hinge_loss(x, y, th, th0) with respect to th\n",
    "def d_hinge_loss_th(x, y, th, th0):\n",
    "    return y * x * d_hinge(y * ((th.T @ x) + th0))\n",
    "\n",
    "# Returns the gradient of hinge_loss(x, y, th, th0) with respect to th0\n",
    "def d_hinge_loss_th0(x, y, th, th0):\n",
    "    return y * d_hinge(y * ((th.T @ x) + th0))\n",
    "\n",
    "# Returns the gradient of svm_obj(x, y, th, th0) with respect to th\n",
    "def d_svm_obj_th(x, y, th, th0, lam):\n",
    "    return np.mean(d_hinge_loss_th(x, y, th, th0), axis=1, keepdims=True) + 2*lam*th\n",
    "\n",
    "# Returns the gradient of svm_obj(x, y, th, th0) with respect to th0\n",
    "def d_svm_obj_th0(x, y, th, th0, lam):\n",
    "    return np.mean(d_hinge_loss_th0(x, y, th, th0), axis=1, keepdims=True)\n",
    "\n",
    "# Returns the full gradient as a single vector\n",
    "def svm_obj_grad(X, y, th, th0, lam):\n",
    "    th_gradient = d_svm_obj_th(X, y, th, th0, lam)\n",
    "    th0_gradient = d_svm_obj_th0(X, y, th, th0, lam)\n",
    "    return np.concatenate((th_gradient, th0_gradient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "OPjFac-nY1Z4"
   },
   "outputs": [],
   "source": [
    "# add your tests here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDP6H_2P80vm"
   },
   "source": [
    "Some test cases that may be of use are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "xNuF6-c38yji"
   },
   "outputs": [],
   "source": [
    "X1 = np.array([[1, 2, 3, 9, 10]])\n",
    "y1 = np.array([[1, 1, 1, -1, -1]])\n",
    "th1, th10 = np.array([[-0.31202807]]), np.array([[1.834     ]])\n",
    "X2 = np.array([[2, 3, 9, 12],\n",
    "               [5, 2, 6, 5]])\n",
    "y2 = np.array([[1, -1, 1, -1]])\n",
    "th2, th20=np.array([[ -3.,  15.]]).T, np.array([[ 2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "tZ9Q6k935tLY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, -1]]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_hinge(np.array([[ 71.]])).tolist()\n",
    "d_hinge(np.array([[ -23.]])).tolist()\n",
    "d_hinge(np.array([[ 71, -23.]])).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "5fcjaWEo5uWo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0, 1]]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_hinge_loss_th(X2[:,0:1], y2[:,0:1], th2, th20).tolist()\n",
    "d_hinge_loss_th(X2, y2, th2, th20).tolist()\n",
    "d_hinge_loss_th0(X2[:,0:1], y2[:,0:1], th2, th20).tolist()\n",
    "d_hinge_loss_th0(X2, y2, th2, th20).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "FkjrEb5y5xBU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5]]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_svm_obj_th(X2[:,0:1], y2[:,0:1], th2, th20, 0.01).tolist()\n",
    "d_svm_obj_th(X2, y2, th2, th20, 0.01).tolist()\n",
    "d_svm_obj_th0(X2[:,0:1], y2[:,0:1], th2, th20, 0.01).tolist()\n",
    "d_svm_obj_th0(X2, y2, th2, th20, 0.01).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "BB7axWab58Lp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.06], [0.3], [0.0]]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_obj_grad(X2, y2, th2, th20, 0.01).tolist()\n",
    "svm_obj_grad(X2[:,0:1], y2[:,0:1], th2, th20, 0.01).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vf6OFEU89pC"
   },
   "source": [
    "### 7.3) Batch SVM minimize\n",
    "\n",
    "Putting it all together, use the functions you built earlier to write\n",
    "a gradient descent minimizer for the SVM objective.  You do not need\n",
    "to paste in your previous definitions; you can just call the ones\n",
    "defined by the staff.  You will need to call `gd`, which is already\n",
    "defined for you as well; your function `batch_svm_min` should return\n",
    "the values that `gd` does.\n",
    "\n",
    "* Initialize all the separator parameters to zero,\n",
    "* use the step size function provided below, and\n",
    "* specify 10 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "dIqWIYnq8_Nb"
   },
   "outputs": [],
   "source": [
    "def batch_svm_min(data, labels, lam):\n",
    "    def svm_min_step_size_fn(i):\n",
    "       return 2/(i+1)**0.5\n",
    "    max_iter = 10\n",
    "    d, n = data.shape\n",
    "    x0 = np.zeros((d+1, 1))\n",
    "    f = lambda th_th0: svm_obj(data, labels, th_th0[:-1, :], th_th0[-1:, :], lam)\n",
    "    df = lambda th_th0: svm_obj_grad(data, labels, th_th0[:-1, :], th_th0[-1:, :], lam)\n",
    "    return gd(f, df, x0, svm_min_step_size_fn, max_iter)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JH4xd7C-9BIm"
   },
   "source": [
    "Test cases are shown below, where an additional separable test\n",
    "data set has been specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "HgOC_i879Acd"
   },
   "outputs": [],
   "source": [
    "def separable_medium():\n",
    "    X = np.array([[2, -1, 1, 1],\n",
    "                  [-2, 2, 2, -1]])\n",
    "    y = np.array([[1, -1, 1, -1]])\n",
    "    return X, y\n",
    "sep_m_separator = np.array([[ 2.69231855], [ 0.67624906]]), np.array([[-3.02402521]])\n",
    "\n",
    "x_1, y_1 = super_simple_separable()\n",
    "ans = package_ans(batch_svm_min(x_1, y_1, 0.0001))\n",
    "\n",
    "x_1, y_1 = separable_medium()\n",
    "ans = package_ans(batch_svm_min(x_1, y_1, 0.0001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tFslLvo5X9w"
   },
   "source": [
    "### 7.4) Numerical SVM objective (Optional)\n",
    "\n",
    "Recall from the previous question that we were able to closely approximate gradients\n",
    "with numerical estimates.\n",
    "We may apply the same technique to optimize the SVM objective.\n",
    "\n",
    "Using your definition of `minimize` and `num_grad` from the previous problem,\n",
    "implement a function that optimizes the SVM objective through numeric approximations.\n",
    "\n",
    "How well does this function perform, compared to the analytical result?\n",
    "Consider both accuracy and runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLt1LwN05b3k"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
